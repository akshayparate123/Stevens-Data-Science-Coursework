{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6cd68c",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: Akshay Parate\n",
    "#### Stevens ID: 20023008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc6720",
   "metadata": {},
   "source": [
    "## Part A: Multi-Layer Perceptron (MLP) (50 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
    "3. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
    "4. Implement the MLP with Tensorflow and compare to your implementation.\n",
    "5. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267ae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may not run this cell after the first installation\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c983c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5d8a2",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data to text and labels\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3884dae",
   "metadata": {},
   "source": [
    "#### Download NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1db93f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to a2-data\\nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk_path = os.path.join('a2-data', 'nltk')\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.data.path.append(nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d9535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    sys.stdout.write(str_ + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9ee164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c0b2d",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "\n",
    "- Load sentences and labels\n",
    "- Transform string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d93854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\" Load sentences and labels from the specified path\n",
    "    Args:\n",
    "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
    "        sentences: the raw text list of all sentences\n",
    "    Returns:\n",
    "        labels: the label list of all sentences\n",
    "    \"\"\"\n",
    "    sentences, labels = [], []\n",
    "    # Start your code here (load text and label from files)\n",
    "    file = open(data_path, \"r\", encoding='utf-8')  \n",
    "    fileData = file.readlines()\n",
    "    file.close()\n",
    "    for data in fileData:\n",
    "        splitData = data.split(\"\t\")\n",
    "        sentences.append(splitData[1].split(\"\\n\")[0])\n",
    "        labels.append(splitData[0])\n",
    "    # End\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56034dbe-a4ef-445f-abe6-89f51bb7c0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85bb60a1-a1e5-4330-aad7-7e9385f8a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataDf = pd.DataFrame()\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "dataDf[\"Sentences\"] = sentences\n",
    "dataDf[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e1278ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
      "Number of sentences and labels: (19536,) (19536,)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('a2-data', 'books.txt')\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "label_map = {}\n",
    "for label in sorted(list(set(labels))):\n",
    "    label_map[label] = len(label_map)\n",
    "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
    "sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "print('Label map:', label_map)\n",
    "print('Number of sentences and labels:', sentences.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a9d7d",
   "metadata": {},
   "source": [
    "#### Split the data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e009ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(sentences: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
    "    Args:\n",
    "        sentences: A numpy array containing all sentences\n",
    "        labels: A number array containing label ids\n",
    "        test_ratio: A float number to calculate the number of test data\n",
    "\n",
    "    Returns:\n",
    "        train_sentences: A numpy array containing all training sentences\n",
    "        train_labels: A number array containing all training label ids\n",
    "        test_sentences: A numpy array containing all test sentences\n",
    "        test_labels: A number array containing all test label ids\n",
    "    \"\"\"\n",
    "    \n",
    "    assert 0 < test_ratio < 1\n",
    "    assert len(sentences) == len(labels)\n",
    "\n",
    "    train_index, test_index = [], []\n",
    "    # Start your code here (split the index for training and test)\n",
    "    n= int(round(len(sentences) * test_ratio,0))\n",
    "    rand_list=list(range(0, len(sentences)))\n",
    "    shuffled_rand_list = random.sample(rand_list, len(rand_list))\n",
    "    train_index = shuffled_rand_list[n:]\n",
    "    test_index = shuffled_rand_list[:n]\n",
    "    # End\n",
    "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
    "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
    "    return train_sentences, train_labels, test_sentences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cb00f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 14066\n",
      "Validation data length: 1563\n",
      "Test data length: 3907\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "(train_sentences, train_labels,\n",
    "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
    "(train_sentences, train_labels,\n",
    "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
    "\n",
    "print('Training data length:', len(train_sentences))\n",
    "print('Validation data length:', len(valid_sentences))\n",
    "print('Test data length:', len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1d47c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(labels: np.ndarray, label_map: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: The labels of a dataset \n",
    "        label_map: The mapping from label to label id\n",
    "    Returns:\n",
    "        label_count: The mapping from label to its count\n",
    "    \"\"\"\n",
    "    label_count = {key: 0 for key in label_map.keys()}\n",
    "    # Start your code here (count the number of each label)\n",
    "    uniqueVal = label_map.values()\n",
    "    for u,k in zip(uniqueVal,list(label_count.keys())):\n",
    "        x = [i for i in labels if i==u]\n",
    "        label_count[k] = len(x)\n",
    "    # End\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e37c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'Arthur Conan Doyle': 1833, 'Fyodor Dostoyevsky': 4229, 'Jane Austen': 8004}\n",
      "Validation: {'Arthur Conan Doyle': 200, 'Fyodor Dostoyevsky': 487, 'Jane Austen': 876}\n",
      "Test: {'Arthur Conan Doyle': 505, 'Fyodor Dostoyevsky': 1228, 'Jane Austen': 2174}\n"
     ]
    }
   ],
   "source": [
    "print('Training:', count_label(train_labels, label_map))\n",
    "print('Validation:', count_label(valid_labels, label_map))\n",
    "print('Test:', count_label(test_labels, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0e0b5",
   "metadata": {},
   "source": [
    "#### Dataset statistics\n",
    "Fill this table with the statistics you just printed (double click this cell to edit)\n",
    "\n",
    "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
    "|:--------------:|--------------------|--------------------|-------------|-------|\n",
    "|  **Training**  |      1865          |     4325           |    7876     | 14066 |\n",
    "| **Validation** |       188          |      491           |     884     |  1563 |\n",
    "|    **Test**    |       485          |     1128           |    2294     |  3907 |\n",
    "|    **Total**   |      2538          |     5944           |   11054     | 19536 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e93b7",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fb98fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        sentence = url_pattern.sub(\"\", sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'\\d', '', sentence)\n",
    "        # End\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec028d49",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6d7e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238aadd7",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84e5e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['below', 'all', 'were', 'am', 'itself', 'not', 'once', 'his', 'himself', 'y']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "print(list(stopwords_set)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fa11482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" Tokenize a sentence into tokens (words)\n",
    "    Args:\n",
    "        sentence: clean sentence\n",
    "    Returns:\n",
    "        tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    # Start your code here\n",
    "    #     Step 1. Split sentence into words\n",
    "    splitWords = sentence.split(\" \")\n",
    "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
    "    stemmedWords = [stemmer.stem(word) for word in splitWords]\n",
    "    #     Step 3. Remove stop words using the defined stopwords_set\n",
    "    words = [w for w in stemmedWords if not w in stopwords_set]\n",
    "    # End\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb0ba0",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce65f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "tokens = tokenize(clean_sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{tokens}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa5afa",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction\n",
    "\n",
    "TF-IDF:\n",
    "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
    "\n",
    "- $t$: A term\n",
    "- $d$: A document. Here, we regard a sentence as a document\n",
    "- $f_{t, d}$: Number of term $t$ in $d$\n",
    "- $N$: Number of document\n",
    "- $n_t$: Number of document containing $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7239b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TfIdfEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.token2index = {}\n",
    "        self.df = defaultdict(int)\n",
    "        self.num_doc = 0\n",
    "        self.processor = Preprocessor()\n",
    "\n",
    "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
    "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
    "            In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
    "                2. Construct the document frequency map to tokens (self.df).\n",
    "                3. Construct the token to index map based on the frequency.\n",
    "                   The token with a higher frequency has the smaller index\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            token_num\n",
    "        \"\"\"\n",
    "        self.num_doc = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == len(sentences) - 1:\n",
    "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
    "            # Start your code here (step 1 & 2)\n",
    "            clean_sentence = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentence)\n",
    "            unique_tokens = set(tokens)\n",
    "            for token in unique_tokens:\n",
    "                self.vocab[token] += 1\n",
    "                self.df[token] += 1\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        # Start your code here (Step 3)\n",
    "        sorted_tokens = sorted(self.vocab.keys(), key=lambda x: self.vocab[x], reverse=True) #sorting tokens in descending order based on the frequency.\n",
    "        self.token2index = {token: idx for idx, token in enumerate(sorted_tokens)} #Maps each token to its index in the sorted list.\n",
    "        # End\n",
    "        token_num = len(self.token2index) \n",
    "        print('The number of distinct tokens:', token_num)\n",
    "        return token_num\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
    "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            features: A (n x token_num) matrix, where n is the number of sentences\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        features = np.zeros((n, len(self.token2index)))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
    "            # Start your code (calculate TF-IDF)\n",
    "            clean_sentence = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentence)\n",
    "            for token in tokens:\n",
    "                if token in self.token2index:\n",
    "                    features[i, self.token2index[token]] += 1 / self.df[token]\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238e87e",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77d41a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 1316\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[0.02777778 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.03703704 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "encoder = TfIdfEncoder()\n",
    "encoder.fit(train_sentences[:100])\n",
    "features = encoder.encode(train_sentences[:10])\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b343de5",
   "metadata": {},
   "source": [
    "#### Encode training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d731b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 14066 / 14066\n",
      "The number of distinct tokens: 16600\n",
      "Encoding with TF-IDF encoder: 14066 / 14066\n",
      "Encoding with TF-IDF encoder: 1563 / 1563\n",
      "Encoding with TF-IDF encoder: 3907 / 3907\n",
      "The size of training set: (14066, 16600) (14066, 3)\n",
      "The size of validation set: (1563, 16600) (1563, 3)\n",
      "The size of test set: (3907, 16600) (3907, 3)\n"
     ]
    }
   ],
   "source": [
    "num_class = 3\n",
    "\n",
    "encoder = TfIdfEncoder()\n",
    "vocab_size = encoder.fit(train_sentences)\n",
    "\n",
    "x_train = encoder.encode(train_sentences)\n",
    "x_valid = encoder.encode(valid_sentences)\n",
    "x_test = encoder.encode(test_sentences)\n",
    "\n",
    "y_train = np.zeros((len(train_labels), num_class))\n",
    "y_valid = np.zeros((len(valid_labels), num_class))\n",
    "y_test = np.zeros((len(test_labels), num_class))\n",
    "#One hot encoding - The code uses NumPy indexing to assign 1 to the appropriate column for each sample in the training, validation, and test sets.\n",
    "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
    "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
    "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
    "\n",
    "print('The size of training set:', x_train.shape, y_train.shape)\n",
    "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
    "print('The size of test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ffa24",
   "metadata": {},
   "source": [
    "## 2. MLP (20 Points)\n",
    "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
    "\n",
    "The objective function of LR for multi-class classification:\n",
    "\n",
    "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
    "\n",
    "- $z_1 = w_1x$\n",
    "- $h_1 = activation(z_1)$\n",
    "- $z_2 = w_2 h_1$\n",
    "- $\\hat{y} = softmax(z_2)$\n",
    "\n",
    "- $n$: Number of samples\n",
    "- $d$: Dimension of $\\mathbf{w}$\n",
    "- Here, you can use `sigmoid` as the activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16af1ff",
   "metadata": {},
   "source": [
    "### 2.1 MLP Model (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bdfa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\" The softmax activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
    "    Returns:\n",
    "        output: Softmax value of the specified dimension in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    x = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # End\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" The sigmoid activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "    Returns:\n",
    "        output: Sigmoid value of each entry in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    # End\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb8d4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.w1 = np.random.randn(feature_dim, hidden_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.w2 = np.random.randn(hidden_dim, num_class)\n",
    "        self.b2 = np.zeros((1, num_class))\n",
    "        # End\n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, x: np.ndarray, return_hiddens) -> np.ndarray:\n",
    "        \"\"\" Forward process of logistic regression\n",
    "            Calculate y_hat using x\n",
    "        Args:\n",
    "            x: Input data\n",
    "            return_hiddens: If true the function will return h1 for gradient calculation\n",
    "        Returns:\n",
    "            y_hat: Output\n",
    "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
    "        \"\"\"\n",
    "        y_hat = 0\n",
    "        h1 = 0, 0\n",
    "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
    "        # Start your code here (calculate y_hat of MLP using x)\n",
    "        z1 = np.dot(x, w1) + b1\n",
    "        h1 = sigmoid(z1)\n",
    "        z2 = np.dot(h1,w2) + b2\n",
    "        y_hat = sigmoid(z2)\n",
    "        # End\n",
    "        if return_hiddens:\n",
    "            return y_hat, h1\n",
    "        else:\n",
    "            return y_hat\n",
    "\n",
    "    def backward(self,\n",
    "                 x: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
    "        \"\"\" Backward process of logistic regression\n",
    "            Calculate the gradient of w and b\n",
    "        Args:\n",
    "            x: Input data\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "            h1: Hidden output of the hidden layer\n",
    "        Returns:\n",
    "            dw1: Gradient of w1\n",
    "            db1: Gradient of b1\n",
    "            dw2: Gradient of w2\n",
    "            db2: Gradient of b2\n",
    "        \"\"\"\n",
    "        w1, w2 = self.w1, self.w2\n",
    "        dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
    "        n = len(x)\n",
    "        # Start your code here (calculate the gradient of w and b)\n",
    "        dz2 = y_hat - y\n",
    "        dw2 = np.transpose(h1) @ dz2\n",
    "        db2 = np.sum(dz2,axis = 0,keepdims = True)\n",
    "        dz1 = dz2 @ np.transpose(w2) * (h1 * (1-h1))\n",
    "        dw1 = np.transpose(x) @ dz1\n",
    "        db1 = np.sum(dz1,axis = 0,keepdims = True)\n",
    "        # End\n",
    "        return dw1, db1, dw2, db2\n",
    "\n",
    "    def categorical_cross_entropy_loss(self,\n",
    "                                       y_hat: np.ndarray,\n",
    "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "        \"\"\" Calculate the binary cross-entropy loss\n",
    "        Args:\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            loss: BCE loss\n",
    "        \"\"\"\n",
    "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
    "        loss = 0\n",
    "        n = y.shape[0]\n",
    "        # Start your code here (Calculate the binary cross-entropy)\n",
    "#         print(y.shape)\n",
    "        loss = -1/n * np.sum(y * np.log(y_hat)) + self.lambda_ * (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))\n",
    "        # End\n",
    "        return loss\n",
    "\n",
    "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "\n",
    "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict the label using the output y_hat\n",
    "        Args:\n",
    "            y_hat: Model output\n",
    "        Returns:\n",
    "            pred: Prediction\n",
    "        \"\"\"\n",
    "        pred = np.zeros_like(y_hat)\n",
    "        index = np.argmax(y_hat, axis=-1)\n",
    "        pred[np.arange(len(y_hat)), index] = 1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40ab16de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\AppData\\Local\\Temp\\ipykernel_8\\2069942790.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #:  50\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "lossList = []\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "# uncomment this line\n",
    "for i in range(0,100):\n",
    "    if(i%50 == 0):\n",
    "        print(\"Epoch #: \",i)\n",
    "    y_hat,h1 = model_mbgd.forward(x_train,True)\n",
    "    prob = softmax(y_hat)\n",
    "    loss = model_mbgd.categorical_cross_entropy_loss(prob, y_train)\n",
    "    lossList.append(loss)\n",
    "    dw1, db1, dw2, db2 = model_mbgd.backward(x_train,y_hat,y_train,h1)\n",
    "    # print(dw1.shape, db1.shape, dw2.shape, db2.shape)\n",
    "    # print(model_mbgd.w1.shape, model_mbgd.b1.shape, model_mbgd.w2.shape, model_mbgd.b2.shape)\n",
    "    model_mbgd.gradient_descent(dw1, db1, dw2, db2,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8499231",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation Metrics\n",
    "\n",
    "Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c18e32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
    "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
    "    Args:\n",
    "        y_pred: Prediction\n",
    "        y_true: Ground-truth\n",
    "    Returns:\n",
    "        accuracy: float number. The accuracy for the whole dataset\n",
    "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    # Start your code here\n",
    "    correct_predictions = 0\n",
    "    for i in range(0,len(y_true)):\n",
    "        if all(y_pred[i] == y_true[i]):\n",
    "            correct_predictions = correct_predictions + 1\n",
    "        else:\n",
    "            pass\n",
    "    total_examples = len(y_true)\n",
    "    accuracy = correct_predictions / total_examples\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f69d4af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD: (0.5564371640644996, 0.5564371640644996, 0.5564371640644996, 0.5564371640644996)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\AppData\\Local\\Temp\\ipykernel_8\\2069942790.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test,False)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "# model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864559b1",
   "metadata": {},
   "source": [
    "### 2.3 AdaGrad (5 points)\n",
    "\n",
    "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
    "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d123002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, init_lr, model):\n",
    "        self.init_lr = init_lr\n",
    "        self.model = model\n",
    "        \n",
    "        self.accumulative_dw1 = 0\n",
    "        self.accumulative_db1 = 0\n",
    "        self.accumulative_dw2 = 0\n",
    "        self.accumulative_db2 = 0\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
    "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
    "            2. Calculate the new gradient with the accumulative gradient\n",
    "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
    "        \n",
    "        Do not return anything\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1\n",
    "        self.accumulative_dw1 += dw1 ** 2\n",
    "        self.accumulative_db1 += db1 ** 2\n",
    "        self.accumulative_dw2 += dw2 ** 2\n",
    "        self.accumulative_db2 += db2 ** 2\n",
    "        # Step 2\n",
    "        new_dw1 = dw1 / (np.sqrt(self.accumulative_dw1) + self.eps)\n",
    "        new_db1 = db1 / (np.sqrt(self.accumulative_db1) + self.eps)\n",
    "        new_dw2 = dw2 / (np.sqrt(self.accumulative_dw2) + self.eps)\n",
    "        new_db2 = db2 / (np.sqrt(self.accumulative_db2) + self.eps)\n",
    "        # Step 3\n",
    "        self.model.gradient_descent(new_dw1, new_db1, new_dw2, new_db2, self.init_lr)\n",
    "        # End\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26ba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40336a46",
   "metadata": {},
   "source": [
    "### 2.4 Mini-batch Gradient Descent (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb2e3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train_mbgd(model,\n",
    "               x_train: np.ndarray,\n",
    "               y_train: np.ndarray,\n",
    "               x_valid: np.ndarray,\n",
    "               y_valid: np.ndarray,\n",
    "               lr: float,\n",
    "               num_epoch: int,\n",
    "               batch_size: int,\n",
    "               print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        batch_size: Number of training samples in a batch\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of training and validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    # IMPORTANT: YOU SHOULD USE THIS OPTIMIZER TO UPDATE THE MODEL\n",
    "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    n_batch = train_num_samples // batch_size\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        # Start your code here (training)\n",
    "        #     Step 1. Model forward\n",
    "        y_hat,h1 = model.forward(x_train,True)\n",
    "        #     Step 2. Calculate loss\n",
    "        prob = softmax(y_hat)\n",
    "        epoch_loss = model.categorical_cross_entropy_loss(prob, y_train)\n",
    "        #     Step 3. Model backward\n",
    "        dw1, db1, dw2, db2 = model.backward(x_train,y_hat,y_train,h1)\n",
    "        #     Step 4. Optimization with Adagrad\n",
    "        optimizer.update(dw1, db1, dw2, db2)\n",
    "        # End\n",
    "\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        # Start your code here (validation)\n",
    "        #     Step 1. Predict\n",
    "        y_hat = model.forward(x_valid,False)\n",
    "        pred = model.predict(y_hat)\n",
    "        #     Step 2. Calculate loss\n",
    "        prob = softmax(y_hat)\n",
    "        valid_loss = model.categorical_cross_entropy_loss(prob, y_valid)\n",
    "        valid_history[\"loss\"].append(loss)\n",
    "        #     Step 3. Calculate metrics\n",
    "        accuracy, precision, recall, f1 = get_metrics(pred,y_valid)\n",
    "        valid_history[\"f1\"].append(f1)\n",
    "        valid_history[\"recall\"].append(recall)\n",
    "        valid_history[\"precision\"].append(precision)\n",
    "        valid_history[\"accuracy\"].append(accuracy)\n",
    "#         End\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbec04",
   "metadata": {},
   "source": [
    "Run Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adfd49fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1000: train_loss: 0.0001 - valid_loss: 1.2240 - valid_accuracy: 0.2636 - valid_precision: 0.2636 - valid_recall: 0.2636 - valid_f1: 0.2636\n",
      "Epoch 11 / 1000: train_loss: 0.0001 - valid_loss: 1.2243 - valid_accuracy: 0.3052 - valid_precision: 0.3052 - valid_recall: 0.3052 - valid_f1: 0.3052\n",
      "Epoch 21 / 1000: train_loss: 0.0001 - valid_loss: 1.1909 - valid_accuracy: 0.3346 - valid_precision: 0.3346 - valid_recall: 0.3346 - valid_f1: 0.3346\n",
      "Epoch 31 / 1000: train_loss: 0.0001 - valid_loss: 1.1401 - valid_accuracy: 0.3730 - valid_precision: 0.3730 - valid_recall: 0.3730 - valid_f1: 0.3730\n",
      "Epoch 41 / 1000: train_loss: 0.0001 - valid_loss: 1.0879 - valid_accuracy: 0.4280 - valid_precision: 0.4280 - valid_recall: 0.4280 - valid_f1: 0.4280\n",
      "Epoch 51 / 1000: train_loss: 0.0001 - valid_loss: 1.0486 - valid_accuracy: 0.5138 - valid_precision: 0.5138 - valid_recall: 0.5138 - valid_f1: 0.5138\n",
      "Epoch 61 / 1000: train_loss: 0.0001 - valid_loss: 1.0226 - valid_accuracy: 0.6059 - valid_precision: 0.6059 - valid_recall: 0.6059 - valid_f1: 0.6059\n",
      "Epoch 71 / 1000: train_loss: 0.0001 - valid_loss: 1.0053 - valid_accuracy: 0.6564 - valid_precision: 0.6564 - valid_recall: 0.6564 - valid_f1: 0.6564\n",
      "Epoch 81 / 1000: train_loss: 0.0001 - valid_loss: 0.9932 - valid_accuracy: 0.6545 - valid_precision: 0.6545 - valid_recall: 0.6545 - valid_f1: 0.6545\n",
      "Epoch 91 / 1000: train_loss: 0.0001 - valid_loss: 0.9841 - valid_accuracy: 0.6532 - valid_precision: 0.6532 - valid_recall: 0.6532 - valid_f1: 0.6532\n",
      "Epoch 101 / 1000: train_loss: 0.0001 - valid_loss: 0.9769 - valid_accuracy: 0.6622 - valid_precision: 0.6622 - valid_recall: 0.6622 - valid_f1: 0.6622\n",
      "Epoch 111 / 1000: train_loss: 0.0001 - valid_loss: 0.9710 - valid_accuracy: 0.6641 - valid_precision: 0.6641 - valid_recall: 0.6641 - valid_f1: 0.6641\n",
      "Epoch 121 / 1000: train_loss: 0.0001 - valid_loss: 0.9659 - valid_accuracy: 0.6667 - valid_precision: 0.6667 - valid_recall: 0.6667 - valid_f1: 0.6667\n",
      "Epoch 131 / 1000: train_loss: 0.0001 - valid_loss: 0.9615 - valid_accuracy: 0.6699 - valid_precision: 0.6699 - valid_recall: 0.6699 - valid_f1: 0.6699\n",
      "Epoch 141 / 1000: train_loss: 0.0001 - valid_loss: 0.9575 - valid_accuracy: 0.6731 - valid_precision: 0.6731 - valid_recall: 0.6731 - valid_f1: 0.6731\n",
      "Epoch 151 / 1000: train_loss: 0.0001 - valid_loss: 0.9540 - valid_accuracy: 0.6769 - valid_precision: 0.6769 - valid_recall: 0.6769 - valid_f1: 0.6769\n",
      "Epoch 161 / 1000: train_loss: 0.0001 - valid_loss: 0.9508 - valid_accuracy: 0.6795 - valid_precision: 0.6795 - valid_recall: 0.6795 - valid_f1: 0.6795\n",
      "Epoch 171 / 1000: train_loss: 0.0001 - valid_loss: 0.9478 - valid_accuracy: 0.6827 - valid_precision: 0.6827 - valid_recall: 0.6827 - valid_f1: 0.6827\n",
      "Epoch 181 / 1000: train_loss: 0.0001 - valid_loss: 0.9451 - valid_accuracy: 0.6852 - valid_precision: 0.6852 - valid_recall: 0.6852 - valid_f1: 0.6852\n",
      "Epoch 191 / 1000: train_loss: 0.0001 - valid_loss: 0.9426 - valid_accuracy: 0.6878 - valid_precision: 0.6878 - valid_recall: 0.6878 - valid_f1: 0.6878\n",
      "Epoch 201 / 1000: train_loss: 0.0001 - valid_loss: 0.9402 - valid_accuracy: 0.6903 - valid_precision: 0.6903 - valid_recall: 0.6903 - valid_f1: 0.6903\n",
      "Epoch 211 / 1000: train_loss: 0.0001 - valid_loss: 0.9380 - valid_accuracy: 0.6948 - valid_precision: 0.6948 - valid_recall: 0.6948 - valid_f1: 0.6948\n",
      "Epoch 221 / 1000: train_loss: 0.0001 - valid_loss: 0.9358 - valid_accuracy: 0.6942 - valid_precision: 0.6942 - valid_recall: 0.6942 - valid_f1: 0.6942\n",
      "Epoch 231 / 1000: train_loss: 0.0001 - valid_loss: 0.9339 - valid_accuracy: 0.6967 - valid_precision: 0.6967 - valid_recall: 0.6967 - valid_f1: 0.6967\n",
      "Epoch 241 / 1000: train_loss: 0.0001 - valid_loss: 0.9320 - valid_accuracy: 0.6980 - valid_precision: 0.6980 - valid_recall: 0.6980 - valid_f1: 0.6980\n",
      "Epoch 251 / 1000: train_loss: 0.0001 - valid_loss: 0.9302 - valid_accuracy: 0.6999 - valid_precision: 0.6999 - valid_recall: 0.6999 - valid_f1: 0.6999\n",
      "Epoch 261 / 1000: train_loss: 0.0001 - valid_loss: 0.9284 - valid_accuracy: 0.7012 - valid_precision: 0.7012 - valid_recall: 0.7012 - valid_f1: 0.7012\n",
      "Epoch 271 / 1000: train_loss: 0.0001 - valid_loss: 0.9268 - valid_accuracy: 0.7025 - valid_precision: 0.7025 - valid_recall: 0.7025 - valid_f1: 0.7025\n",
      "Epoch 281 / 1000: train_loss: 0.0001 - valid_loss: 0.9252 - valid_accuracy: 0.7031 - valid_precision: 0.7031 - valid_recall: 0.7031 - valid_f1: 0.7031\n",
      "Epoch 291 / 1000: train_loss: 0.0001 - valid_loss: 0.9237 - valid_accuracy: 0.7038 - valid_precision: 0.7038 - valid_recall: 0.7038 - valid_f1: 0.7038\n",
      "Epoch 301 / 1000: train_loss: 0.0001 - valid_loss: 0.9222 - valid_accuracy: 0.7044 - valid_precision: 0.7044 - valid_recall: 0.7044 - valid_f1: 0.7044\n",
      "Epoch 311 / 1000: train_loss: 0.0001 - valid_loss: 0.9208 - valid_accuracy: 0.7057 - valid_precision: 0.7057 - valid_recall: 0.7057 - valid_f1: 0.7057\n",
      "Epoch 321 / 1000: train_loss: 0.0001 - valid_loss: 0.9195 - valid_accuracy: 0.7070 - valid_precision: 0.7070 - valid_recall: 0.7070 - valid_f1: 0.7070\n",
      "Epoch 331 / 1000: train_loss: 0.0001 - valid_loss: 0.9182 - valid_accuracy: 0.7070 - valid_precision: 0.7070 - valid_recall: 0.7070 - valid_f1: 0.7070\n",
      "Epoch 341 / 1000: train_loss: 0.0001 - valid_loss: 0.9169 - valid_accuracy: 0.7083 - valid_precision: 0.7083 - valid_recall: 0.7083 - valid_f1: 0.7083\n",
      "Epoch 351 / 1000: train_loss: 0.0001 - valid_loss: 0.9157 - valid_accuracy: 0.7089 - valid_precision: 0.7089 - valid_recall: 0.7089 - valid_f1: 0.7089\n",
      "Epoch 361 / 1000: train_loss: 0.0001 - valid_loss: 0.9145 - valid_accuracy: 0.7115 - valid_precision: 0.7115 - valid_recall: 0.7115 - valid_f1: 0.7115\n",
      "Epoch 371 / 1000: train_loss: 0.0001 - valid_loss: 0.9133 - valid_accuracy: 0.7127 - valid_precision: 0.7127 - valid_recall: 0.7127 - valid_f1: 0.7127\n",
      "Epoch 381 / 1000: train_loss: 0.0001 - valid_loss: 0.9122 - valid_accuracy: 0.7159 - valid_precision: 0.7159 - valid_recall: 0.7159 - valid_f1: 0.7159\n",
      "Epoch 391 / 1000: train_loss: 0.0001 - valid_loss: 0.9111 - valid_accuracy: 0.7166 - valid_precision: 0.7166 - valid_recall: 0.7166 - valid_f1: 0.7166\n",
      "Epoch 401 / 1000: train_loss: 0.0001 - valid_loss: 0.9101 - valid_accuracy: 0.7185 - valid_precision: 0.7185 - valid_recall: 0.7185 - valid_f1: 0.7185\n",
      "Epoch 411 / 1000: train_loss: 0.0001 - valid_loss: 0.9090 - valid_accuracy: 0.7185 - valid_precision: 0.7185 - valid_recall: 0.7185 - valid_f1: 0.7185\n",
      "Epoch 421 / 1000: train_loss: 0.0001 - valid_loss: 0.9080 - valid_accuracy: 0.7210 - valid_precision: 0.7210 - valid_recall: 0.7210 - valid_f1: 0.7210\n",
      "Epoch 431 / 1000: train_loss: 0.0001 - valid_loss: 0.9071 - valid_accuracy: 0.7210 - valid_precision: 0.7210 - valid_recall: 0.7210 - valid_f1: 0.7210\n",
      "Epoch 441 / 1000: train_loss: 0.0001 - valid_loss: 0.9061 - valid_accuracy: 0.7230 - valid_precision: 0.7230 - valid_recall: 0.7230 - valid_f1: 0.7230\n",
      "Epoch 451 / 1000: train_loss: 0.0001 - valid_loss: 0.9052 - valid_accuracy: 0.7236 - valid_precision: 0.7236 - valid_recall: 0.7236 - valid_f1: 0.7236\n",
      "Epoch 461 / 1000: train_loss: 0.0001 - valid_loss: 0.9043 - valid_accuracy: 0.7242 - valid_precision: 0.7242 - valid_recall: 0.7242 - valid_f1: 0.7242\n",
      "Epoch 471 / 1000: train_loss: 0.0001 - valid_loss: 0.9034 - valid_accuracy: 0.7262 - valid_precision: 0.7262 - valid_recall: 0.7262 - valid_f1: 0.7262\n",
      "Epoch 481 / 1000: train_loss: 0.0001 - valid_loss: 0.9025 - valid_accuracy: 0.7274 - valid_precision: 0.7274 - valid_recall: 0.7274 - valid_f1: 0.7274\n",
      "Epoch 491 / 1000: train_loss: 0.0001 - valid_loss: 0.9017 - valid_accuracy: 0.7294 - valid_precision: 0.7294 - valid_recall: 0.7294 - valid_f1: 0.7294\n",
      "Epoch 501 / 1000: train_loss: 0.0001 - valid_loss: 0.9009 - valid_accuracy: 0.7300 - valid_precision: 0.7300 - valid_recall: 0.7300 - valid_f1: 0.7300\n",
      "Epoch 511 / 1000: train_loss: 0.0001 - valid_loss: 0.9000 - valid_accuracy: 0.7306 - valid_precision: 0.7306 - valid_recall: 0.7306 - valid_f1: 0.7306\n",
      "Epoch 521 / 1000: train_loss: 0.0001 - valid_loss: 0.8993 - valid_accuracy: 0.7326 - valid_precision: 0.7326 - valid_recall: 0.7326 - valid_f1: 0.7326\n",
      "Epoch 531 / 1000: train_loss: 0.0001 - valid_loss: 0.8985 - valid_accuracy: 0.7338 - valid_precision: 0.7338 - valid_recall: 0.7338 - valid_f1: 0.7338\n",
      "Epoch 541 / 1000: train_loss: 0.0001 - valid_loss: 0.8977 - valid_accuracy: 0.7345 - valid_precision: 0.7345 - valid_recall: 0.7345 - valid_f1: 0.7345\n",
      "Epoch 551 / 1000: train_loss: 0.0001 - valid_loss: 0.8970 - valid_accuracy: 0.7345 - valid_precision: 0.7345 - valid_recall: 0.7345 - valid_f1: 0.7345\n",
      "Epoch 561 / 1000: train_loss: 0.0001 - valid_loss: 0.8962 - valid_accuracy: 0.7364 - valid_precision: 0.7364 - valid_recall: 0.7364 - valid_f1: 0.7364\n",
      "Epoch 571 / 1000: train_loss: 0.0001 - valid_loss: 0.8955 - valid_accuracy: 0.7377 - valid_precision: 0.7377 - valid_recall: 0.7377 - valid_f1: 0.7377\n",
      "Epoch 581 / 1000: train_loss: 0.0001 - valid_loss: 0.8948 - valid_accuracy: 0.7377 - valid_precision: 0.7377 - valid_recall: 0.7377 - valid_f1: 0.7377\n",
      "Epoch 591 / 1000: train_loss: 0.0001 - valid_loss: 0.8941 - valid_accuracy: 0.7396 - valid_precision: 0.7396 - valid_recall: 0.7396 - valid_f1: 0.7396\n",
      "Epoch 601 / 1000: train_loss: 0.0001 - valid_loss: 0.8934 - valid_accuracy: 0.7390 - valid_precision: 0.7390 - valid_recall: 0.7390 - valid_f1: 0.7390\n",
      "Epoch 611 / 1000: train_loss: 0.0001 - valid_loss: 0.8928 - valid_accuracy: 0.7390 - valid_precision: 0.7390 - valid_recall: 0.7390 - valid_f1: 0.7390\n",
      "Epoch 621 / 1000: train_loss: 0.0001 - valid_loss: 0.8921 - valid_accuracy: 0.7402 - valid_precision: 0.7402 - valid_recall: 0.7402 - valid_f1: 0.7402\n",
      "Epoch 631 / 1000: train_loss: 0.0001 - valid_loss: 0.8915 - valid_accuracy: 0.7409 - valid_precision: 0.7409 - valid_recall: 0.7409 - valid_f1: 0.7409\n",
      "Epoch 641 / 1000: train_loss: 0.0001 - valid_loss: 0.8908 - valid_accuracy: 0.7422 - valid_precision: 0.7422 - valid_recall: 0.7422 - valid_f1: 0.7422\n",
      "Epoch 651 / 1000: train_loss: 0.0001 - valid_loss: 0.8902 - valid_accuracy: 0.7434 - valid_precision: 0.7434 - valid_recall: 0.7434 - valid_f1: 0.7434\n",
      "Epoch 661 / 1000: train_loss: 0.0001 - valid_loss: 0.8896 - valid_accuracy: 0.7441 - valid_precision: 0.7441 - valid_recall: 0.7441 - valid_f1: 0.7441\n",
      "Epoch 671 / 1000: train_loss: 0.0001 - valid_loss: 0.8890 - valid_accuracy: 0.7454 - valid_precision: 0.7454 - valid_recall: 0.7454 - valid_f1: 0.7454\n",
      "Epoch 681 / 1000: train_loss: 0.0001 - valid_loss: 0.8884 - valid_accuracy: 0.7466 - valid_precision: 0.7466 - valid_recall: 0.7466 - valid_f1: 0.7466\n",
      "Epoch 691 / 1000: train_loss: 0.0001 - valid_loss: 0.8878 - valid_accuracy: 0.7479 - valid_precision: 0.7479 - valid_recall: 0.7479 - valid_f1: 0.7479\n",
      "Epoch 701 / 1000: train_loss: 0.0001 - valid_loss: 0.8872 - valid_accuracy: 0.7479 - valid_precision: 0.7479 - valid_recall: 0.7479 - valid_f1: 0.7479\n",
      "Epoch 711 / 1000: train_loss: 0.0001 - valid_loss: 0.8867 - valid_accuracy: 0.7479 - valid_precision: 0.7479 - valid_recall: 0.7479 - valid_f1: 0.7479\n",
      "Epoch 721 / 1000: train_loss: 0.0001 - valid_loss: 0.8861 - valid_accuracy: 0.7479 - valid_precision: 0.7479 - valid_recall: 0.7479 - valid_f1: 0.7479\n",
      "Epoch 731 / 1000: train_loss: 0.0001 - valid_loss: 0.8855 - valid_accuracy: 0.7479 - valid_precision: 0.7479 - valid_recall: 0.7479 - valid_f1: 0.7479\n",
      "Epoch 741 / 1000: train_loss: 0.0001 - valid_loss: 0.8850 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 751 / 1000: train_loss: 0.0001 - valid_loss: 0.8845 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 761 / 1000: train_loss: 0.0001 - valid_loss: 0.8839 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 771 / 1000: train_loss: 0.0001 - valid_loss: 0.8834 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 781 / 1000: train_loss: 0.0001 - valid_loss: 0.8829 - valid_accuracy: 0.7505 - valid_precision: 0.7505 - valid_recall: 0.7505 - valid_f1: 0.7505\n",
      "Epoch 791 / 1000: train_loss: 0.0001 - valid_loss: 0.8824 - valid_accuracy: 0.7511 - valid_precision: 0.7511 - valid_recall: 0.7511 - valid_f1: 0.7511\n",
      "Epoch 801 / 1000: train_loss: 0.0001 - valid_loss: 0.8819 - valid_accuracy: 0.7524 - valid_precision: 0.7524 - valid_recall: 0.7524 - valid_f1: 0.7524\n",
      "Epoch 811 / 1000: train_loss: 0.0001 - valid_loss: 0.8814 - valid_accuracy: 0.7530 - valid_precision: 0.7530 - valid_recall: 0.7530 - valid_f1: 0.7530\n",
      "Epoch 821 / 1000: train_loss: 0.0001 - valid_loss: 0.8809 - valid_accuracy: 0.7537 - valid_precision: 0.7537 - valid_recall: 0.7537 - valid_f1: 0.7537\n",
      "Epoch 831 / 1000: train_loss: 0.0001 - valid_loss: 0.8804 - valid_accuracy: 0.7537 - valid_precision: 0.7537 - valid_recall: 0.7537 - valid_f1: 0.7537\n",
      "Epoch 841 / 1000: train_loss: 0.0001 - valid_loss: 0.8799 - valid_accuracy: 0.7537 - valid_precision: 0.7537 - valid_recall: 0.7537 - valid_f1: 0.7537\n",
      "Epoch 851 / 1000: train_loss: 0.0001 - valid_loss: 0.8795 - valid_accuracy: 0.7543 - valid_precision: 0.7543 - valid_recall: 0.7543 - valid_f1: 0.7543\n",
      "Epoch 861 / 1000: train_loss: 0.0001 - valid_loss: 0.8790 - valid_accuracy: 0.7543 - valid_precision: 0.7543 - valid_recall: 0.7543 - valid_f1: 0.7543\n",
      "Epoch 871 / 1000: train_loss: 0.0001 - valid_loss: 0.8786 - valid_accuracy: 0.7562 - valid_precision: 0.7562 - valid_recall: 0.7562 - valid_f1: 0.7562\n",
      "Epoch 881 / 1000: train_loss: 0.0001 - valid_loss: 0.8781 - valid_accuracy: 0.7562 - valid_precision: 0.7562 - valid_recall: 0.7562 - valid_f1: 0.7562\n",
      "Epoch 891 / 1000: train_loss: 0.0001 - valid_loss: 0.8776 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 901 / 1000: train_loss: 0.0001 - valid_loss: 0.8772 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 911 / 1000: train_loss: 0.0001 - valid_loss: 0.8768 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 921 / 1000: train_loss: 0.0001 - valid_loss: 0.8763 - valid_accuracy: 0.7582 - valid_precision: 0.7582 - valid_recall: 0.7582 - valid_f1: 0.7582\n",
      "Epoch 931 / 1000: train_loss: 0.0001 - valid_loss: 0.8759 - valid_accuracy: 0.7588 - valid_precision: 0.7588 - valid_recall: 0.7588 - valid_f1: 0.7588\n",
      "Epoch 941 / 1000: train_loss: 0.0001 - valid_loss: 0.8755 - valid_accuracy: 0.7601 - valid_precision: 0.7601 - valid_recall: 0.7601 - valid_f1: 0.7601\n",
      "Epoch 951 / 1000: train_loss: 0.0001 - valid_loss: 0.8751 - valid_accuracy: 0.7601 - valid_precision: 0.7601 - valid_recall: 0.7601 - valid_f1: 0.7601\n",
      "Epoch 961 / 1000: train_loss: 0.0001 - valid_loss: 0.8746 - valid_accuracy: 0.7607 - valid_precision: 0.7607 - valid_recall: 0.7607 - valid_f1: 0.7607\n",
      "Epoch 971 / 1000: train_loss: 0.0001 - valid_loss: 0.8742 - valid_accuracy: 0.7614 - valid_precision: 0.7614 - valid_recall: 0.7614 - valid_f1: 0.7614\n",
      "Epoch 981 / 1000: train_loss: 0.0001 - valid_loss: 0.8738 - valid_accuracy: 0.7614 - valid_precision: 0.7614 - valid_recall: 0.7614 - valid_f1: 0.7614\n",
      "Epoch 991 / 1000: train_loss: 0.0001 - valid_loss: 0.8734 - valid_accuracy: 0.7614 - valid_precision: 0.7614 - valid_recall: 0.7614 - valid_f1: 0.7614\n",
      "Epoch 1000 / 1000: train_loss: 0.0001 - valid_loss: 0.8731 - valid_accuracy: 0.7614 - valid_precision: 0.7614 - valid_recall: 0.7614 - valid_f1: 0.7614\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 1000\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0925d",
   "metadata": {},
   "source": [
    "### 2.5 MLP using Tensorflow (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ed9e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "\n",
    "class MLPTF(Model):\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model using tensorflow.keras\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.dense1 = Dense(hidden_dim, activation='sigmoid', kernel_regularizer=regularizers.l2(lambda_))\n",
    "        self.dense2 = Dense(num_class, activation='sigmoid', kernel_regularizer=regularizers.l2(lambda_))\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        # End\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Forward function of tf. It should be named 'call'\n",
    "        \n",
    "        Args:\n",
    "            x: (n x feature_dim) tensor\n",
    "        Returns:\n",
    "            y_hat: (n x num_class) tensor\n",
    "        \"\"\"\n",
    "        # Start your code here (Forward)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        y_hat = self.softmax(x)\n",
    "        # End\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e034877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlptf\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  2124928   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  387       \n",
      "                                                                 \n",
      " softmax (Softmax)           multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,125,315\n",
      "Trainable params: 2,125,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "np.random.seed(6666)\n",
    "tf.random.set_seed(6666)\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 1000\n",
    "lr = 1e-1\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "\n",
    "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "model_tf.build(input_shape=(None, vocab_size))\n",
    "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a556bbb6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "110/110 [==============================] - 5s 12ms/step - loss: 0.9672 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9657 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 2/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9622 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 3/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9618 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 4/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9615 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 5/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9615 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 6/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9615 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 7/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9664 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 8/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 9/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 10/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 11/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 12/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 13/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9613 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 14/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9611 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 15/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9654 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 16/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9611 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 17/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9611 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 18/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9611 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 19/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9611 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9656 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 20/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 21/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 22/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 23/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 24/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 25/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9612 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 26/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 27/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 28/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 29/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 30/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9647 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 31/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9611 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9646 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 32/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 33/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 34/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 35/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 36/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 37/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9652 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 38/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 39/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 40/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 41/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 42/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 43/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 44/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9652 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 45/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 46/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 47/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 48/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9651 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 49/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 50/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 51/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9646 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 52/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9610 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9651 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 53/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 54/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 55/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9646 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 56/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 57/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9647 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 58/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 59/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 60/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 61/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 62/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 63/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 64/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 65/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9608 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 66/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 67/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9609 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 68/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9655 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 69/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 70/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 71/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 72/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 73/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9654 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 74/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 75/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 76/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9646 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 77/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 78/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9651 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 79/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 80/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 81/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 82/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 83/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9607 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 84/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 85/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9649 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 86/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 87/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 88/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 89/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 90/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 91/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 92/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 93/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9647 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 94/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9646 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 95/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9606 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 96/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 97/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9648 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 98/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 99/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 100/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 101/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 102/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 103/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9605 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 104/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 105/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 106/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9646 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 107/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 108/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 109/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 110/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 111/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 112/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 113/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 114/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 115/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 116/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 117/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 118/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9604 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 119/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 120/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 121/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 122/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9602 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9648 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 123/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9643 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 124/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 125/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 126/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 127/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9602 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 128/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9603 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 129/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9602 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 130/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9602 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 131/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9602 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 132/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 133/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 134/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9602 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 135/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9602 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 136/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 137/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 138/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 139/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 140/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 141/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 142/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9650 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 143/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 144/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 145/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 146/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9601 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 147/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 148/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 149/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9651 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 150/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 151/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9645 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 152/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9644 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 153/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9600 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 154/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9646 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 155/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 156/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9598 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 157/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 158/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 159/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 160/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 161/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9599 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 162/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9598 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 163/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9598 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 164/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9598 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 165/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9598 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 166/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9598 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 167/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9598 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 168/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9597 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 169/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9596 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 170/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9597 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 171/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9597 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 172/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9596 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 173/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9597 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 174/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9596 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9642 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 175/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9597 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 176/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9596 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9639 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 177/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9596 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 178/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9596 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 179/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9595 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9641 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 180/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9595 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 181/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9595 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 182/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9595 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 183/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9650 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 184/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9595 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 185/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9595 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 186/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 187/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 188/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 189/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 190/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9640 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 191/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 192/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9593 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 193/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 194/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9594 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 195/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9593 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 196/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9593 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 197/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9592 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 198/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9592 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 199/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9592 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 200/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9593 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 201/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9592 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 202/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9592 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 203/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9592 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 204/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9591 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 205/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9591 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 206/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9591 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 207/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9591 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 208/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9591 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 209/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9591 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 210/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9590 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 211/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9590 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 212/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9590 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 213/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9589 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 214/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9589 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 215/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9589 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 216/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9589 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 217/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9589 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9638 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 218/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9589 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 219/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9588 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 220/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9588 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 221/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9588 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9635 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 222/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9587 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 223/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9587 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 224/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9587 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 225/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9587 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 226/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9586 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 227/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9587 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 228/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9586 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9637 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 229/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9585 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 230/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9586 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9634 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 231/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9586 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 232/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9585 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9636 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 233/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9585 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 234/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9584 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 235/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9585 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 236/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9585 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 237/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9585 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 238/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9584 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 239/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9583 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 240/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9584 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 241/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9583 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 242/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9583 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 243/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9583 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9629 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 244/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9583 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 245/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9582 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 246/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9582 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 247/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9582 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 248/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9581 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 249/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9581 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 250/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9581 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 251/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9581 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 252/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9580 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9629 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 253/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9580 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9628 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 254/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9580 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 255/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9579 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9629 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 256/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9579 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 257/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9578 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 258/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9578 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9626 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 259/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9579 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 260/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9578 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 261/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9578 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9632 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 262/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9578 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9630 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 263/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9577 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 264/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9577 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9633 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 265/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9576 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9629 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 266/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9576 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9626 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 267/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9577 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9628 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 268/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9575 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 269/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9572 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 270/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9577 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 271/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9575 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9628 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 272/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9574 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 273/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9574 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9629 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 274/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9574 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9629 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 275/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9573 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9631 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 276/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9572 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9624 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 277/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9574 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9625 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 278/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9572 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 279/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9572 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9629 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 280/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9572 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 281/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9571 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9623 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 282/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9571 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9624 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 283/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9571 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9623 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 284/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9570 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9628 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 285/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9570 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9628 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 286/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9570 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9624 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 287/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9569 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 288/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9569 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9623 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 289/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9569 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9623 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 290/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9568 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9622 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 291/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9568 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9626 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 292/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9568 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9624 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 293/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9567 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9622 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 294/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9567 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9623 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 295/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9566 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9625 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 296/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9566 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9621 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 297/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9566 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9623 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 298/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9565 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9627 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 299/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9565 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9621 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 300/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9565 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9620 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 301/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9564 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9625 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 302/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9562 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9620 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 303/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9564 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9622 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 304/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9563 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9622 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 305/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9562 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9626 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 306/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9562 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9621 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 307/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9562 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9621 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 308/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9561 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9620 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 309/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9561 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9622 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 310/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9560 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9622 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 311/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9560 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9620 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 312/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9560 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9618 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 313/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9559 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9623 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 314/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9559 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9619 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 315/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9559 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9619 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 316/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9558 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9617 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 317/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9557 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9625 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 318/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9556 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9616 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 319/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9557 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9621 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 320/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9556 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9622 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 321/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9555 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9617 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 322/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9555 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9624 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 323/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9555 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9618 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 324/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9554 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9615 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 325/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9554 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9617 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 326/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9553 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9620 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 327/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9553 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9616 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 328/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9552 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9624 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 329/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9552 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9620 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 330/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9552 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9616 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 331/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9552 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9616 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 332/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9551 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9621 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 333/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9551 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9615 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 334/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9550 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9615 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 335/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9549 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9614 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 336/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9549 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9615 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 337/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9549 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9616 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 338/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9548 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9617 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 339/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9547 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9620 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 340/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9547 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9614 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 341/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9547 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9613 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 342/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9546 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9616 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 343/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9544 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9617 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 344/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9545 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9612 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 345/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9544 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9612 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 346/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9544 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9615 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 347/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9543 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9611 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 348/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9543 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9613 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 349/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9542 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9612 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 350/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9542 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9612 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 351/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9541 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9615 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 352/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9541 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9610 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 353/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9540 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9613 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 354/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9540 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9610 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 355/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9539 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9611 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 356/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9538 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9610 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 357/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9538 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9611 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 358/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9537 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9607 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 359/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9537 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9610 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 360/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9536 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9611 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 361/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9536 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9611 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 362/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9534 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9617 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 363/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9534 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9606 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 364/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9534 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9607 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 365/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9533 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9613 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 366/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9533 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9611 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 367/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9532 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9607 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 368/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9532 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9605 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 369/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9531 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9604 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 370/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9531 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9609 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 371/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9531 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9608 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 372/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9529 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9607 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 373/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9529 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9606 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 374/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9528 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9607 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 375/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9528 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9610 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 376/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9527 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9602 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 377/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9526 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9603 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 378/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9525 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9605 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 379/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9525 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9605 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 380/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9525 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9604 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 381/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9524 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9601 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 382/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9523 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9602 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 383/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9523 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9601 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 384/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9522 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9608 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 385/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9522 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9603 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 386/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9521 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9599 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 387/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9521 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9603 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 388/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9520 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9600 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 389/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9520 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9600 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 390/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9518 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9600 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 391/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9518 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9600 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 392/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9517 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9601 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 393/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9516 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9599 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 394/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9516 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9599 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 395/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9515 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 396/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9514 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9598 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 397/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9514 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 398/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9513 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 399/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9512 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9603 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 400/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9512 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 401/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9510 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 402/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9510 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9594 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 403/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9510 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9600 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 404/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9509 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 405/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9508 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9592 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 406/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9508 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9595 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 407/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9507 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9594 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 408/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9506 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9598 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 409/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9505 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 410/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9504 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9594 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 411/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9504 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9592 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 412/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9504 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9592 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 413/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9503 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9594 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 414/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9502 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9590 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 415/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9501 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9591 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 416/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9500 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9592 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 417/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9499 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9592 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 418/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9499 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9592 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 419/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9498 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9588 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 420/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9498 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9591 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 421/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9496 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9596 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 422/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9496 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9594 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 423/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9496 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9587 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 424/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9495 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9591 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 425/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9494 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9586 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 426/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9493 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9586 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 427/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9492 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9585 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 428/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9492 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9585 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 429/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9491 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9585 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 430/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9490 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9585 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 431/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9489 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9584 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 432/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9489 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9586 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 433/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9488 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9584 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 434/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9487 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9584 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 435/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9486 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9586 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 436/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9485 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9582 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 437/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9485 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9586 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 438/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9484 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9585 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 439/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9483 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9580 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 440/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9482 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9583 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 441/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9481 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9583 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 442/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9481 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9578 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 443/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9480 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9578 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 444/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9480 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9579 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 445/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9479 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9578 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 446/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9477 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9577 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 447/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9477 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9577 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 448/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9476 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9581 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 449/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9475 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9580 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 450/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9475 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9575 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 451/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9473 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9583 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 452/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9473 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9575 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 453/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9472 - categorical_accuracy: 0.5690 - f1_score: 0.5690 - val_loss: 0.9576 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 454/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9471 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9575 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 455/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9470 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9573 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 456/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9469 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9578 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 457/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9469 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9576 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 458/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9468 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9573 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 459/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9467 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9575 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 460/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9466 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9579 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 461/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9466 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9576 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 462/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9464 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9570 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 463/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9464 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9573 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 464/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9462 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9574 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 465/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9462 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9576 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 466/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9462 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9572 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 467/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9460 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9577 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 468/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9459 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9568 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 469/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9458 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9570 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 470/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9457 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9570 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 471/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9457 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9572 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 472/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9456 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9566 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 473/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9454 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9576 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 474/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9455 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9567 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 475/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9454 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9566 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 476/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9452 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9568 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 477/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9451 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9564 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 478/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9451 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9566 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 479/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9450 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9562 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 480/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9449 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9563 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 481/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9448 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9565 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 482/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9447 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9560 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 483/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9446 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9563 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 484/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9445 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9562 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 485/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9444 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9559 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 486/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9444 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9561 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 487/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9443 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9565 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 488/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9442 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9562 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 489/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9441 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9558 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 490/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9439 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9565 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 491/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9439 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9561 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 492/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9438 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9563 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 493/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9437 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9560 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 494/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9436 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9556 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 495/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9435 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9554 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 496/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9434 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9558 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 497/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9434 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9554 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 498/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9433 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9554 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 499/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9432 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9555 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 500/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9431 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9553 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 501/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9430 - categorical_accuracy: 0.5691 - f1_score: 0.5691 - val_loss: 0.9555 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 502/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9429 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9551 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 503/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9428 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9554 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 504/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9427 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9549 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 505/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9426 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9550 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 506/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9425 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9550 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 507/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9423 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9553 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 508/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9423 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9547 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 509/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9423 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9549 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 510/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9421 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9547 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 511/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9420 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9552 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 512/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9419 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9545 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 513/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9419 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9550 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 514/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9418 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9554 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 515/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9417 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9544 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 516/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9416 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9544 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 517/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9415 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9543 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 518/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9414 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9551 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 519/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9413 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9544 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 520/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9412 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9550 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 521/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9411 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9541 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 522/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9410 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9546 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 523/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9409 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9543 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 524/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9408 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9540 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 525/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9407 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9538 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 526/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9406 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9538 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 527/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9406 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9539 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 528/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9404 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9538 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 529/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9403 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9535 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 530/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9402 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9538 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 531/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9402 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9542 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 532/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9400 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9534 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 533/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9398 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9544 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 534/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9399 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9536 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 535/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9397 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9542 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 536/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9397 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9537 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 537/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9396 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9536 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 538/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9394 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9532 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 539/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9393 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9538 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 540/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9393 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9531 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 541/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9391 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9532 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 542/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9391 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9530 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 543/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9389 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9530 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 544/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9388 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9527 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 545/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9388 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9527 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 546/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9387 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9531 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 547/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9386 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9526 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 548/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9385 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9525 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 549/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9383 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9533 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 550/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9382 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9529 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 551/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9382 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9525 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 552/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9381 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9523 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 553/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9379 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9525 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 554/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9379 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9526 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 555/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9378 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9523 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 556/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9377 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9522 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 557/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9375 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9526 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 558/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9374 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9523 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 559/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9373 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9526 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 560/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9373 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9523 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 561/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9372 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9522 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 562/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9370 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9521 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 563/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9369 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9521 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 564/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9368 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9516 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 565/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9368 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9516 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 566/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9366 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9521 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 567/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9364 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9522 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 568/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9365 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9514 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 569/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9363 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9512 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 570/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9363 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9514 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 571/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9362 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9515 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 572/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9360 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9516 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 573/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9359 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9510 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 574/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9359 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9514 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 575/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9358 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9513 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 576/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9356 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9510 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 577/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9355 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9510 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 578/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9354 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9514 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 579/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9354 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9514 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 580/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9352 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9514 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 581/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9351 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9510 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 582/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9350 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9505 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 583/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9349 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9505 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 584/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9348 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9509 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 585/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9347 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9508 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 586/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9346 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9501 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 587/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9344 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9510 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 588/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9344 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9503 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 589/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9343 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9504 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 590/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9341 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9500 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 591/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9341 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9499 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 592/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9340 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9498 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 593/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9339 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9501 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 594/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9337 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9503 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 595/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9336 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9506 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 596/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9336 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9501 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 597/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9335 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9500 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 598/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9334 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9500 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 599/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9333 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9499 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 600/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9332 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9494 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 601/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9330 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9501 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 602/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9330 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9495 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 603/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9328 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9493 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 604/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9328 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9495 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 605/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9327 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9492 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 606/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9326 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9491 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 607/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9325 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9491 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 608/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9323 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9488 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 609/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9322 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9493 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 610/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9321 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9493 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 611/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9320 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9492 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 612/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9320 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9487 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 613/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9317 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9501 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 614/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9318 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9491 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 615/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9317 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9486 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 616/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9315 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9490 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 617/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9314 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9485 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 618/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9313 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9483 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 619/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9312 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9482 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 620/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9311 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9487 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 621/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9310 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9486 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 622/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9309 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9487 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 623/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9308 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9486 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 624/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9307 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9478 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 625/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9305 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9489 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 626/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9305 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9478 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 627/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9304 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9476 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 628/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9303 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9478 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 629/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9302 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9481 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 630/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9301 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9477 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 631/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9300 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9475 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 632/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9299 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9476 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 633/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9297 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9475 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 634/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9297 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9475 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 635/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9296 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9473 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 636/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9295 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9473 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 637/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9294 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9473 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 638/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9293 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9474 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 639/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9292 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9469 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 640/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9291 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9469 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 641/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9290 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9470 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 642/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9288 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9466 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 643/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9288 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9469 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 644/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9286 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9465 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 645/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9286 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9468 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 646/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9284 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9468 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 647/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9284 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9465 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 648/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9283 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9465 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 649/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9281 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9464 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 650/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9280 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9461 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 651/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9280 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9466 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 652/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9278 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9460 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 653/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9277 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9465 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 654/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9275 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9459 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 655/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9276 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9460 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 656/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9274 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9458 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 657/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9273 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9464 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 658/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9272 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9456 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 659/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9271 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9466 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 660/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9270 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9463 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 661/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9269 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9454 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 662/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9268 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9456 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 663/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9267 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9454 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 664/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9266 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9452 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 665/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9265 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9460 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 666/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9264 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9452 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 667/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9263 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9450 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 668/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9261 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9449 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 669/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9261 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9452 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 670/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9260 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9455 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 671/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9259 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9453 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 672/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9258 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9448 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 673/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9257 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9447 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 674/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9255 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9450 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 675/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9255 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9448 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 676/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9254 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9449 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 677/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9253 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9449 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 678/1000\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.9252 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9445 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 679/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9252 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9443 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 680/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9250 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9448 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 681/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9249 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9443 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 682/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9248 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9440 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 683/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9247 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9446 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 684/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9246 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9445 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 685/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9245 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9440 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 686/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9244 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9443 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 687/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9243 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9438 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 688/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9242 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9439 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 689/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9241 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9439 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 690/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9240 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9436 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 691/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9239 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9441 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 692/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9238 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9436 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 693/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9237 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9437 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 694/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9236 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9434 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 695/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9235 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9432 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 696/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9234 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9433 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 697/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9233 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9433 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 698/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9232 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9437 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 699/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9231 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9430 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 700/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9230 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9434 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 701/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9229 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9430 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 702/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9228 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9428 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 703/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9227 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9429 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 704/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9226 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9427 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 705/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9225 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9429 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 706/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9224 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9427 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 707/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9223 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9424 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 708/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9222 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9425 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 709/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9221 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9424 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 710/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9220 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9423 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 711/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9220 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9423 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 712/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9218 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9427 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 713/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9217 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9426 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 714/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9217 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9426 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 715/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9215 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9430 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 716/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9215 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9420 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 717/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9214 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9421 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 718/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9213 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9421 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 719/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9211 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9420 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 720/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9210 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9416 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 721/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9209 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9421 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 722/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9208 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9414 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 723/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9208 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9415 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 724/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9207 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9414 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 725/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9206 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9414 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 726/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9205 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9411 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 727/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9204 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9417 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 728/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9203 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9414 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 729/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9202 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9411 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 730/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9201 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9411 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 731/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9200 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9409 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 732/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9199 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9412 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 733/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9198 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9409 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 734/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9197 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9407 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 735/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9196 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9412 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 736/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9196 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9407 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 737/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9194 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9405 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 738/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9193 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9406 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 739/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9193 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9404 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 740/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9192 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9405 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 741/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9191 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9407 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 742/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9190 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9405 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 743/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9189 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9403 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 744/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9188 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9404 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 745/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9187 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9404 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 746/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9186 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9403 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 747/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9185 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9399 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 748/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9184 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9400 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 749/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9183 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9401 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 750/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9181 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9405 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 751/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9180 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9395 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 752/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9180 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9399 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 753/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9180 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9397 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 754/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9178 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9400 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 755/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9178 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9394 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 756/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9176 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9393 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 757/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9175 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9401 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 758/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9175 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9396 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 759/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9174 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9397 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 760/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9173 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9390 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 761/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9172 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9393 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 762/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9171 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9391 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 763/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9170 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9391 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 764/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9169 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9390 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 765/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9168 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9388 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 766/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9167 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9385 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 767/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9167 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9389 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 768/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9165 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9389 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 769/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9164 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9389 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 770/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9163 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9385 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 771/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9163 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9386 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 772/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9162 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9383 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 773/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9160 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9381 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 774/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9160 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9380 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 775/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9159 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9381 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 776/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9158 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9380 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 777/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9157 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9380 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 778/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9156 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9380 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 779/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9155 - categorical_accuracy: 0.5693 - f1_score: 0.5693 - val_loss: 0.9381 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 780/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9154 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9380 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 781/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9154 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9381 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 782/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9153 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9382 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 783/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9152 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9378 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 784/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9151 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9382 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 785/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9150 - categorical_accuracy: 0.5692 - f1_score: 0.5692 - val_loss: 0.9374 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 786/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9149 - categorical_accuracy: 0.5693 - f1_score: 0.5693 - val_loss: 0.9373 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 787/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9148 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9378 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 788/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9147 - categorical_accuracy: 0.5693 - f1_score: 0.5693 - val_loss: 0.9377 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 789/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9147 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9377 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 790/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9146 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9372 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 791/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9145 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9374 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 792/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9144 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9372 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 793/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9142 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9368 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 794/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9143 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9372 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 795/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9141 - categorical_accuracy: 0.5693 - f1_score: 0.5693 - val_loss: 0.9369 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 796/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9140 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9369 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 797/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9140 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9373 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 798/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9139 - categorical_accuracy: 0.5694 - f1_score: 0.5694 - val_loss: 0.9369 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 799/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9138 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9366 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 800/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9137 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9364 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 801/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9136 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9366 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 802/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9134 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9363 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 803/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9134 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9365 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 804/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9133 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9362 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 805/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9132 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9362 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 806/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9132 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9365 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 807/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9130 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9364 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 808/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9130 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9364 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 809/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9129 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9360 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 810/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9128 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9365 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 811/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9128 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9358 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 812/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9126 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9358 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 813/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9125 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9356 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 814/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9125 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9356 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 815/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9124 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9360 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 816/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9123 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9356 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 817/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9122 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9361 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 818/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9121 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9355 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 819/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9120 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9363 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 820/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9120 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9355 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 821/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9119 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9351 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 822/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9118 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9351 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 823/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9116 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9361 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 824/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9116 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9350 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 825/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9115 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9352 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 826/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9115 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9352 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 827/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9113 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9356 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 828/1000\n",
      "110/110 [==============================] - 1s 8ms/step - loss: 0.9113 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9347 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 829/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9112 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9353 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 830/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9111 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9351 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 831/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9110 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9348 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 832/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9109 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9351 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 833/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9109 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9344 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 834/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9108 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9347 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 835/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9107 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9344 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 836/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9106 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9342 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 837/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9105 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9342 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 838/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9104 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9342 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 839/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9104 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9343 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 840/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9103 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9341 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 841/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9102 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9341 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 842/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9102 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9340 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 843/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9101 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9339 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 844/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9100 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9339 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 845/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9098 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9337 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 846/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9098 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9343 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 847/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9097 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9337 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 848/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9096 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9343 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 849/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9096 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9336 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 850/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9095 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9337 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 851/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9093 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9342 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 852/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9093 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9334 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 853/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9092 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9333 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 854/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9091 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9333 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 855/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9090 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9340 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 856/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9090 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9335 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 857/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9089 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9332 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 858/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9088 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9333 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 859/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9088 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9331 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 860/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9086 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9335 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 861/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9086 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9334 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 862/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9084 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9331 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 863/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9085 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9326 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 864/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9084 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9326 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 865/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9082 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9326 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 866/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9082 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9327 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 867/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9081 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9326 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 868/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9080 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9330 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 869/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9080 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9326 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 870/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9078 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9325 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 871/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9078 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9323 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 872/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9077 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9335 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 873/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9077 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9320 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 874/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9076 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9321 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 875/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9074 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9324 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 876/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9074 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9320 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 877/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9073 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9327 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 878/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9073 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9318 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 879/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9071 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9319 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 880/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9071 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9319 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 881/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9070 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9317 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 882/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9070 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9319 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 883/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9069 - categorical_accuracy: 0.5695 - f1_score: 0.5695 - val_loss: 0.9321 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 884/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9068 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9315 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 885/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9067 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9313 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 886/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9066 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9316 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 887/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9066 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9314 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 888/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9065 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9317 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 889/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9064 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9311 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 890/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9064 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9310 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 891/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9062 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9310 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 892/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9062 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9312 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 893/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9062 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9310 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 894/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9060 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9309 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 895/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9060 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9309 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 896/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9058 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9308 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 897/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9058 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9306 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 898/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9058 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9308 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 899/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9056 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9316 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 900/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9056 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9309 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 901/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9055 - categorical_accuracy: 0.5696 - f1_score: 0.5696 - val_loss: 0.9305 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 902/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9054 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9303 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 903/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9054 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9304 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 904/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9053 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9304 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 905/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9052 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9309 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 906/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9051 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9304 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 907/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9050 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9300 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 908/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9050 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9301 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 909/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9049 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9308 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 910/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9049 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9303 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 911/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9048 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9301 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 912/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9047 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9298 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 913/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9046 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9297 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 914/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9045 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9297 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 915/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9044 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9299 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 916/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9044 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9296 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 917/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9043 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9299 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 918/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9042 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9305 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 919/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9042 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9295 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 920/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9041 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9293 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 921/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9040 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9297 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 922/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9040 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9292 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 923/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9038 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9291 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 924/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9038 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9291 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 925/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9037 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9292 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 926/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9037 - categorical_accuracy: 0.5697 - f1_score: 0.5697 - val_loss: 0.9290 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 927/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9035 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9298 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 928/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9035 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9289 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 929/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9034 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9290 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 930/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9034 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9290 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 931/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9033 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9292 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 932/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9032 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9286 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 933/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9031 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9286 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 934/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9031 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9292 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 935/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9030 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9285 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 936/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9030 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9286 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 937/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9029 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9289 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 938/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9029 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9288 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 939/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9027 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9282 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 940/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9027 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9285 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 941/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9026 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9283 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 942/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9025 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9280 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 943/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9025 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9284 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 944/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9024 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9279 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 945/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9023 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9282 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 946/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9023 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9279 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 947/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9021 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9278 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 948/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9021 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9280 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 949/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9020 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9282 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 950/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9020 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9276 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 951/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9019 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9280 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 952/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9019 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9276 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 953/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9018 - categorical_accuracy: 0.5698 - f1_score: 0.5698 - val_loss: 0.9274 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 954/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9017 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9277 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 955/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9016 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9276 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 956/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9016 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9274 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 957/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9015 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9280 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 958/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9015 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9272 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 959/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9014 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9274 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 960/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9013 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9271 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 961/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9013 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9271 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 962/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9012 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9270 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 963/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9011 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9269 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 964/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9010 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9271 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 965/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9009 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9278 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 966/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9009 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9271 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 967/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9008 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9269 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 968/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9007 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9268 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 969/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9007 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9277 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 970/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9007 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9266 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 971/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9006 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9266 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 972/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9005 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9269 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 973/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9004 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9265 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 974/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9004 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9265 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 975/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9003 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9265 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 976/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9002 - categorical_accuracy: 0.5699 - f1_score: 0.5699 - val_loss: 0.9268 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 977/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9002 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9266 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 978/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9001 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9264 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 979/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9000 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9263 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 980/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.9000 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9264 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 981/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8999 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9261 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 982/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8998 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9264 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 983/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8998 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9264 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 984/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8997 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9259 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 985/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8996 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9259 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 986/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8996 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9258 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 987/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8994 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9256 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 988/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8995 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9258 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 989/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8994 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9266 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 990/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8993 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9262 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 991/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8992 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9255 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 992/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8991 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9259 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 993/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8990 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9257 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 994/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8991 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9254 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 995/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8990 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9253 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 996/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8989 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9253 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 997/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8989 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9257 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 998/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8988 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9253 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 999/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8987 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9250 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n",
      "Epoch 1000/1000\n",
      "110/110 [==============================] - 1s 7ms/step - loss: 0.8986 - categorical_accuracy: 0.5700 - f1_score: 0.5700 - val_loss: 0.9250 - val_categorical_accuracy: 0.5605 - val_f1_score: 0.5605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9533 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9539 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9522 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9522 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9523 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9526 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9519 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9521 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9523 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9540 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9524 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9538 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9521 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9526 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9535 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9525 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n"
     ]
    }
   ],
   "source": [
    "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2a285",
   "metadata": {},
   "source": [
    "#### Evaluation with Tensroflow\n",
    "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2668a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD: (0.7545431277194778, 0.7545431277194778, 0.7545431277194778, 0.7545431277194778)\n",
      "123/123 [==============================] - 0s 4ms/step - loss: 0.9271 - categorical_accuracy: 0.5577 - f1_score: 0.5577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9271218776702881, 0.5577169060707092, 0.5577169060707092]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test,False)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725ae0",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics on Test set\n",
    "Fill this table with the result you just printed (double click this cell to edit)\n",
    "|     Optimizer                     | Accuracy    | F1 Score    |\n",
    "|:---------------------------------:|-------------|-------------|\n",
    "|      **Your Implementation**      |     0.675753        | 0.675753            |\n",
    "| **Tensorflow**                    |    0.9722         |    0.554         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cb8e7",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49bc71a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
    "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390fc13c",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ca4c7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax = axes[0]\n",
    "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation Accuracy')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff50430",
   "metadata": {},
   "source": [
    "# 3. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15ce6b",
   "metadata": {},
   "source": [
    "Answer:My implementation achieved an accuracy and F1 score of approximately 0.676. These metrics suggest that the model is performing reasonably well, with a good balance between precision and recall.\n",
    "The TensorFlow implementation achieved a significantly higher accuracy (0.9722) but a lower F1 score (0.554). The high accuracy indicates that the model is making correct predictions for a large portion of the dataset. However, the lower F1 score suggests a potential imbalance between precision and recall, indicating that the model may struggle with correctly classifying certain classes.\n",
    "\n",
    "Perfect Fit: It's possible that your model has memorized the training data and achieved a perfect fit, resulting in zero training loss. This could indicate overfitting, especially if the validation performance is not improving.\n",
    "\n",
    "The increasing trends in both accuracy and F1 score suggest that my model is learning and improving its performance on the validation set over time. This is a positive sign, indicating that your model is continuously learning from the data and adapting to the task.\n",
    "The constant trends in both accuracy and F1 score for the TensorFlow implementation might suggest that the model may have reached a plateau in terms of learning from the validation set. It could be that the model has already learned most of the patterns in the data, and further training iterations do not significantly impact its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69316163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
