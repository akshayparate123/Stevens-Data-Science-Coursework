{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6cd68c",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: Akshay Parate\n",
    "#### Stevens ID: 20023008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc6720",
   "metadata": {},
   "source": [
    "## Part A: Multi-Layer Perceptron (MLP) (50 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
    "3. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
    "4. Implement the MLP with Tensorflow and compare to your implementation.\n",
    "5. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "267ae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may not run this cell after the first installation\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a5c983c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5d8a2",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data to text and labels\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3884dae",
   "metadata": {},
   "source": [
    "#### Download NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b1db93f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to a2-data\\nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk_path = os.path.join('a2-data', 'nltk')\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.data.path.append(nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "24d9535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    sys.stdout.write(str_ + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6c9ee164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c0b2d",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "\n",
    "- Load sentences and labels\n",
    "- Transform string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d93854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\" Load sentences and labels from the specified path\n",
    "    Args:\n",
    "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
    "        sentences: the raw text list of all sentences\n",
    "    Returns:\n",
    "        labels: the label list of all sentences\n",
    "    \"\"\"\n",
    "    sentences, labels = [], []\n",
    "    # Start your code here (load text and label from files)\n",
    "    file = open(data_path, \"r\", encoding='utf-8')  \n",
    "    fileData = file.readlines()\n",
    "    file.close()\n",
    "    for data in fileData:\n",
    "        splitData = data.split(\"\t\")\n",
    "        sentences.append(splitData[1].split(\"\\n\")[0])\n",
    "        labels.append(splitData[0])\n",
    "    # End\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "5e1278ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
      "Number of sentences and labels: 19536 19536\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('a2-data', 'books.txt')\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "label_map = {}\n",
    "for label in sorted(list(set(labels))):\n",
    "    label_map[label] = len(label_map)\n",
    "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
    "sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "print('Label map:', label_map)\n",
    "print('Number of sentences and labels:', len(sentences), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a9d7d",
   "metadata": {},
   "source": [
    "#### Split the data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "e009ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(sentences: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
    "    Args:\n",
    "        sentences: A numpy array containing all sentences\n",
    "        labels: A number array containing label ids\n",
    "        test_ratio: A float number to calculate the number of test data\n",
    "\n",
    "    Returns:\n",
    "        train_sentences: A numpy array containing all training sentences\n",
    "        train_labels: A number array containing all training label ids\n",
    "        test_sentences: A numpy array containing all test sentences\n",
    "        test_labels: A number array containing all test label ids\n",
    "    \"\"\"\n",
    "    \n",
    "    assert 0 < test_ratio < 1\n",
    "    assert len(sentences) == len(labels)\n",
    "\n",
    "    train_index, test_index = [], []\n",
    "    # Start your code here (split the index for training and test)\n",
    "    n= int(round(len(sentences) * test_ratio,0))\n",
    "    rand_list=list(range(0, len(sentences)))\n",
    "    shuffled_rand_list = random.sample(rand_list, len(rand_list))\n",
    "    train_index = shuffled_rand_list[n:]\n",
    "    test_index = shuffled_rand_list[:n]\n",
    "    # End\n",
    "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
    "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
    "    return train_sentences, train_labels, test_sentences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5cb00f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 14066\n",
      "Validation data length: 1563\n",
      "Test data length: 3907\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "(train_sentences, train_labels,\n",
    "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
    "(train_sentences, train_labels,\n",
    "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
    "\n",
    "print('Training data length:', len(train_sentences))\n",
    "print('Validation data length:', len(valid_sentences))\n",
    "print('Test data length:', len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d1d47c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(labels: np.ndarray, label_map: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: The labels of a dataset \n",
    "        label_map: The mapping from label to label id\n",
    "    Returns:\n",
    "        label_count: The mapping from label to its count\n",
    "    \"\"\"\n",
    "    label_count = {key: 0 for key in label_map.keys()}\n",
    "    # Start your code here (count the number of each label)\n",
    "    uniqueVal = label_map.values()\n",
    "    for u,k in zip(uniqueVal,list(label_count.keys())):\n",
    "        x = [i for i in labels if i==u]\n",
    "        label_count[k] = len(x)\n",
    "    # End\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4e37c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'Arthur Conan Doyle': 1825, 'Fyodor Dostoyevsky': 4326, 'Jane Austen': 7915}\n",
      "Validation: {'Arthur Conan Doyle': 207, 'Fyodor Dostoyevsky': 439, 'Jane Austen': 917}\n",
      "Test: {'Arthur Conan Doyle': 506, 'Fyodor Dostoyevsky': 1179, 'Jane Austen': 2222}\n"
     ]
    }
   ],
   "source": [
    "print('Training:', count_label(train_labels, label_map))\n",
    "print('Validation:', count_label(valid_labels, label_map))\n",
    "print('Test:', count_label(test_labels, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0e0b5",
   "metadata": {},
   "source": [
    "#### Dataset statistics\n",
    "Fill this table with the statistics you just printed (double click this cell to edit)\n",
    "\n",
    "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
    "|:--------------:|--------------------|--------------------|-------------|-------|\n",
    "|  **Training**  |      1865          |     4325           |    7876     | 14066 |\n",
    "| **Validation** |       188          |      491           |     884     |  1563 |\n",
    "|    **Test**    |       485          |     1128           |    2294     |  3907 |\n",
    "|    **Total**   |      2538          |     5944           |   11054     | 19536 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e93b7",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0fb98fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        sentence = url_pattern.sub(\"\", sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'\\d', '', sentence)\n",
    "        # End\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec028d49",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e6d7e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238aadd7",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "84e5e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"mightn't\", 'shan', 'only', 'with', 'to', 'himself', 'any', 'of', 'as', 'no']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "print(list(stopwords_set)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0fa11482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" Tokenize a sentence into tokens (words)\n",
    "    Args:\n",
    "        sentence: clean sentence\n",
    "    Returns:\n",
    "        tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    # Start your code here\n",
    "    #     Step 1. Split sentence into words\n",
    "    splitWords = sentence.split(\" \")\n",
    "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
    "    stemmedWords = [stemmer.stem(word) for word in splitWords]\n",
    "    #     Step 3. Remove stop words using the defined stopwords_set\n",
    "    words = [w for w in stemmedWords if not w in stopwords_set]\n",
    "    # End\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb0ba0",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "ce65f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "tokens = tokenize(clean_sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{tokens}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa5afa",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction\n",
    "\n",
    "TF-IDF:\n",
    "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
    "\n",
    "- $t$: A term\n",
    "- $d$: A document. Here, we regard a sentence as a document\n",
    "- $f_{t, d}$: Number of term $t$ in $d$\n",
    "- $N$: Number of document\n",
    "- $n_t$: Number of document containing $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b7239b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TfIdfEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.token2index = {}\n",
    "        self.df = defaultdict(int)\n",
    "        self.num_doc = 0\n",
    "        self.processor = Preprocessor()\n",
    "\n",
    "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
    "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
    "            In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
    "                2. Construct the document frequency map to tokens (self.df).\n",
    "                3. Construct the token to index map based on the frequency.\n",
    "                   The token with a higher frequency has the smaller index\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            token_num\n",
    "        \"\"\"\n",
    "        self.num_doc = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == len(sentences) - 1:\n",
    "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
    "            # Start your code here (step 1 & 2)\n",
    "            clean_sentence = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentence)\n",
    "            unique_tokens = set(tokens)\n",
    "            for token in unique_tokens:\n",
    "                self.vocab[token] += 1\n",
    "                self.df[token] += 1\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        # Start your code here (Step 3)\n",
    "        sorted_tokens = sorted(self.vocab.keys(), key=lambda x: self.vocab[x], reverse=True)\n",
    "        self.token2index = {token: idx for idx, token in enumerate(sorted_tokens)}\n",
    "        # End\n",
    "        token_num = len(self.token2index) \n",
    "        print('The number of distinct tokens:', token_num)\n",
    "        return token_num\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
    "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            features: A (n x token_num) matrix, where n is the number of sentences\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        features = np.zeros((n, len(self.token2index)))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
    "            # Start your code (calculate TF-IDF)\n",
    "            clean_sentence = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentence)\n",
    "            for token in tokens:\n",
    "                if token in self.token2index:\n",
    "                    features[i, self.token2index[token]] += 1 / self.df[token]\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238e87e",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "77d41a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 1444\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.11764706 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "encoder = TfIdfEncoder()\n",
    "encoder.fit(train_sentences[:100])\n",
    "features = encoder.encode(train_sentences[:10])\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b343de5",
   "metadata": {},
   "source": [
    "#### Encode training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d731b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 14066 / 14066\n",
      "The number of distinct tokens: 16850\n",
      "Encoding with TF-IDF encoder: 14066 / 14066\n",
      "Encoding with TF-IDF encoder: 1563 / 1563\n",
      "Encoding with TF-IDF encoder: 3907 / 3907\n",
      "The size of training set: (14066, 16850) (14066, 3)\n",
      "The size of validation set: (1563, 16850) (1563, 3)\n",
      "The size of test set: (3907, 16850) (3907, 3)\n"
     ]
    }
   ],
   "source": [
    "num_class = 3\n",
    "\n",
    "encoder = TfIdfEncoder()\n",
    "vocab_size = encoder.fit(train_sentences)\n",
    "\n",
    "x_train = encoder.encode(train_sentences)\n",
    "x_valid = encoder.encode(valid_sentences)\n",
    "x_test = encoder.encode(test_sentences)\n",
    "\n",
    "y_train = np.zeros((len(train_labels), num_class))\n",
    "y_valid = np.zeros((len(valid_labels), num_class))\n",
    "y_test = np.zeros((len(test_labels), num_class))\n",
    "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
    "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
    "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
    "\n",
    "print('The size of training set:', x_train.shape, y_train.shape)\n",
    "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
    "print('The size of test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ffa24",
   "metadata": {},
   "source": [
    "## 2. MLP (20 Points)\n",
    "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
    "\n",
    "The objective function of LR for multi-class classification:\n",
    "\n",
    "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
    "\n",
    "- $z_1 = w_1x$\n",
    "- $h_1 = activation(z_1)$\n",
    "- $z_2 = w_2 h_1$\n",
    "- $\\hat{y} = softmax(z_2)$\n",
    "\n",
    "- $n$: Number of samples\n",
    "- $d$: Dimension of $\\mathbf{w}$\n",
    "- Here, you can use `sigmoid` as the activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16af1ff",
   "metadata": {},
   "source": [
    "### 2.1 MLP Model (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6bdfa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\" The softmax activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
    "    Returns:\n",
    "        output: Softmax value of the specified dimension in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    x = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # End\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" The sigmoid activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "    Returns:\n",
    "        output: Sigmoid value of each entry in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    # End\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "eb8d4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.w1 = np.random.randn(feature_dim, hidden_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.w2 = np.random.randn(hidden_dim, num_class)\n",
    "        self.b2 = np.zeros((1, num_class))\n",
    "        # End\n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, x: np.ndarray, return_hiddens) -> np.ndarray:\n",
    "        \"\"\" Forward process of logistic regression\n",
    "            Calculate y_hat using x\n",
    "        Args:\n",
    "            x: Input data\n",
    "            return_hiddens: If true the function will return h1 for gradient calculation\n",
    "        Returns:\n",
    "            y_hat: Output\n",
    "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
    "        \"\"\"\n",
    "        y_hat = 0\n",
    "        h1 = 0, 0\n",
    "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
    "        # Start your code here (calculate y_hat of MLP using x)\n",
    "        z1 = np.dot(x, w1) + b1\n",
    "        h1 = sigmoid(z1)\n",
    "        z2 = np.dot(h1,w2) + b2\n",
    "        y_hat = sigmoid(z2)\n",
    "        # End\n",
    "        if return_hiddens:\n",
    "            return y_hat, h1\n",
    "        else:\n",
    "            return y_hat\n",
    "\n",
    "    def backward(self,\n",
    "                 x: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
    "        \"\"\" Backward process of logistic regression\n",
    "            Calculate the gradient of w and b\n",
    "        Args:\n",
    "            x: Input data\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "            h1: Hidden output of the hidden layer\n",
    "        Returns:\n",
    "            dw1: Gradient of w1\n",
    "            db1: Gradient of b1\n",
    "            dw2: Gradient of w2\n",
    "            db2: Gradient of b2\n",
    "        \"\"\"\n",
    "        w1, w2 = self.w1, self.w2\n",
    "        dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
    "        n = len(x)\n",
    "        # Start your code here (calculate the gradient of w and b)\n",
    "        dz2 = y_hat - y\n",
    "        dw2 = np.transpose(h1) @ dz2\n",
    "        db2 = np.sum(dz2,axis = 0,keepdims = True)\n",
    "        dz1 = dz2 @ np.transpose(w2) * (h1 * (1-h1))\n",
    "        dw1 = np.transpose(x) @ dz1\n",
    "        db1 = np.sum(dz1,axis = 0,keepdims = True)\n",
    "        # End\n",
    "        return dw1, db1, dw2, db2\n",
    "\n",
    "    def categorical_cross_entropy_loss(self,\n",
    "                                       y_hat: np.ndarray,\n",
    "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "        \"\"\" Calculate the binary cross-entropy loss\n",
    "        Args:\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            loss: BCE loss\n",
    "        \"\"\"\n",
    "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
    "        loss = 0\n",
    "        n = y.shape[0]\n",
    "        # Start your code here (Calculate the binary cross-entropy)\n",
    "#         print(y.shape)\n",
    "        loss = -1/n * np.sum(y * np.log(y_hat)) + self.lambda_ * (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))\n",
    "        # End\n",
    "        return loss\n",
    "\n",
    "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "\n",
    "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict the label using the output y_hat\n",
    "        Args:\n",
    "            y_hat: Model output\n",
    "        Returns:\n",
    "            pred: Prediction\n",
    "        \"\"\"\n",
    "        pred = np.zeros_like(y_hat)\n",
    "        index = np.argmax(y_hat, axis=-1)\n",
    "        pred[np.arange(len(y_hat)), index] = 1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "40ab16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6666)\n",
    "lossList = []\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "# uncomment this line\n",
    "# for i in range(0,100):\n",
    "#     if(i%50 == 0):\n",
    "#         print(\"Epoch #: \",i)\n",
    "#     y_hat,h1 = model_mbgd.forward(x_train,True)\n",
    "#     prob = softmax(y_hat)\n",
    "#     loss = model_mbgd.categorical_cross_entropy_loss(prob, y_train)\n",
    "#     lossList.append(loss)\n",
    "#     dw1, db1, dw2, db2 = model_mbgd.backward(x_train,y_hat,y_train,h1)\n",
    "#     # print(dw1.shape, db1.shape, dw2.shape, db2.shape)\n",
    "#     # print(model_mbgd.w1.shape, model_mbgd.b1.shape, model_mbgd.w2.shape, model_mbgd.b2.shape)\n",
    "#     model_mbgd.gradient_descent(dw1, db1, dw2, db2,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8499231",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation Metrics\n",
    "\n",
    "Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c18e32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
    "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
    "    Args:\n",
    "        y_pred: Prediction\n",
    "        y_true: Ground-truth\n",
    "    Returns:\n",
    "        accuracy: float number. The accuracy for the whole dataset\n",
    "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    # Start your code here\n",
    "    correct_predictions = 0\n",
    "    for i in range(0,len(y_true)):\n",
    "        if all(y_pred[i] == y_true[i]):\n",
    "            correct_predictions = correct_predictions + 1\n",
    "        else:\n",
    "            pass\n",
    "    total_examples = len(y_true)\n",
    "    accuracy = correct_predictions / total_examples\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f69d4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the metrics for test set and fill in the table below\n",
    "# y_hat = model_mbgd.forward(x_test,False)\n",
    "# y_pred = model_mbgd.predict(y_hat)\n",
    "# print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "# model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864559b1",
   "metadata": {},
   "source": [
    "### 2.3 AdaGrad (5 points)\n",
    "\n",
    "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
    "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4d123002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, init_lr, model):\n",
    "        self.init_lr = init_lr\n",
    "        self.model = model\n",
    "        \n",
    "        self.accumulative_dw1 = 0\n",
    "        self.accumulative_db1 = 0\n",
    "        self.accumulative_dw2 = 0\n",
    "        self.accumulative_db2 = 0\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
    "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
    "            2. Calculate the new gradient with the accumulative gradient\n",
    "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
    "        \n",
    "        Do not return anything\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1\n",
    "        self.accumulative_dw1 += dw1 ** 2\n",
    "        self.accumulative_db1 += db1 ** 2\n",
    "        self.accumulative_dw2 += dw2 ** 2\n",
    "        self.accumulative_db2 += db2 ** 2\n",
    "        # Step 2\n",
    "        new_dw1 = dw1 / (np.sqrt(self.accumulative_dw1) + self.eps)\n",
    "        new_db1 = db1 / (np.sqrt(self.accumulative_db1) + self.eps)\n",
    "        new_dw2 = dw2 / (np.sqrt(self.accumulative_dw2) + self.eps)\n",
    "        new_db2 = db2 / (np.sqrt(self.accumulative_db2) + self.eps)\n",
    "        # Step 3\n",
    "        self.model.gradient_descent(new_dw1, new_db1, new_dw2, new_db2, self.init_lr)\n",
    "        # End\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26ba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40336a46",
   "metadata": {},
   "source": [
    "### 2.4 Mini-batch Gradient Descent (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "eb2e3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train_mbgd(model,\n",
    "               x_train: np.ndarray,\n",
    "               y_train: np.ndarray,\n",
    "               x_valid: np.ndarray,\n",
    "               y_valid: np.ndarray,\n",
    "               lr: float,\n",
    "               num_epoch: int,\n",
    "               batch_size: int,\n",
    "               print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        batch_size: Number of training samples in a batch\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of training and validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    # IMPORTANT: YOU SHOULD USE THIS OPTIMIZER TO UPDATE THE MODEL\n",
    "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    n_batch = train_num_samples // batch_size\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        # Start your code here (training)\n",
    "        #     Step 1. Model forward\n",
    "        y_hat,h1 = model.forward(x_train,True)\n",
    "        #     Step 2. Calculate loss\n",
    "        prob = softmax(y_hat)\n",
    "        epoch_loss = model.categorical_cross_entropy_loss(prob, y_train)\n",
    "        #     Step 3. Model backward\n",
    "        dw1, db1, dw2, db2 = model.backward(x_train,y_hat,y_train,h1)\n",
    "        #     Step 4. Optimization with Adagrad\n",
    "        optimizer.update(dw1, db1, dw2, db2)\n",
    "        # End\n",
    "\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        # Start your code here (validation)\n",
    "        #     Step 1. Predict\n",
    "        y_hat = model.forward(x_valid,False)\n",
    "        pred = model.predict(y_hat)\n",
    "        #     Step 2. Calculate loss\n",
    "        prob = softmax(y_hat)\n",
    "        valid_loss = model.categorical_cross_entropy_loss(prob, y_valid)\n",
    "        valid_history[\"loss\"].append(loss)\n",
    "        #     Step 3. Calculate metrics\n",
    "        accuracy, precision, recall, f1 = get_metrics(pred,y_valid)\n",
    "        valid_history[\"f1\"].append(f1)\n",
    "        valid_history[\"recall\"].append(recall)\n",
    "        valid_history[\"precision\"].append(precision)\n",
    "        valid_history[\"accuracy\"].append(accuracy)\n",
    "#         End\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbec04",
   "metadata": {},
   "source": [
    "Run Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "adfd49fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100: train_loss: 0.0001 - valid_loss: 1.0512 - valid_accuracy: 0.5611 - valid_precision: 0.5611 - valid_recall: 0.5611 - valid_f1: 0.5611\n",
      "Epoch 11 / 100: train_loss: 0.0001 - valid_loss: 1.0087 - valid_accuracy: 0.5835 - valid_precision: 0.5835 - valid_recall: 0.5835 - valid_f1: 0.5835\n",
      "Epoch 21 / 100: train_loss: 0.0001 - valid_loss: 0.9941 - valid_accuracy: 0.6097 - valid_precision: 0.6097 - valid_recall: 0.6097 - valid_f1: 0.6097\n",
      "Epoch 31 / 100: train_loss: 0.0001 - valid_loss: 0.9845 - valid_accuracy: 0.6296 - valid_precision: 0.6296 - valid_recall: 0.6296 - valid_f1: 0.6296\n",
      "Epoch 41 / 100: train_loss: 0.0001 - valid_loss: 0.9767 - valid_accuracy: 0.6436 - valid_precision: 0.6436 - valid_recall: 0.6436 - valid_f1: 0.6436\n",
      "Epoch 51 / 100: train_loss: 0.0001 - valid_loss: 0.9702 - valid_accuracy: 0.6500 - valid_precision: 0.6500 - valid_recall: 0.6500 - valid_f1: 0.6500\n",
      "Epoch 61 / 100: train_loss: 0.0001 - valid_loss: 0.9646 - valid_accuracy: 0.6615 - valid_precision: 0.6615 - valid_recall: 0.6615 - valid_f1: 0.6615\n",
      "Epoch 71 / 100: train_loss: 0.0001 - valid_loss: 0.9597 - valid_accuracy: 0.6699 - valid_precision: 0.6699 - valid_recall: 0.6699 - valid_f1: 0.6699\n",
      "Epoch 81 / 100: train_loss: 0.0001 - valid_loss: 0.9554 - valid_accuracy: 0.6775 - valid_precision: 0.6775 - valid_recall: 0.6775 - valid_f1: 0.6775\n",
      "Epoch 91 / 100: train_loss: 0.0001 - valid_loss: 0.9515 - valid_accuracy: 0.6814 - valid_precision: 0.6814 - valid_recall: 0.6814 - valid_f1: 0.6814\n",
      "Epoch 100 / 100: train_loss: 0.0001 - valid_loss: 0.9482 - valid_accuracy: 0.6852 - valid_precision: 0.6852 - valid_recall: 0.6852 - valid_f1: 0.6852\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0925d",
   "metadata": {},
   "source": [
    "### 2.5 MLP using Tensorflow (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "9ed9e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "\n",
    "class MLPTF(Model):\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model using tensorflow.keras\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.dense1 = Dense(hidden_dim, activation='sigmoid', kernel_regularizer=regularizers.l2(lambda_))\n",
    "        self.dense2 = Dense(num_class, activation='sigmoid', kernel_regularizer=regularizers.l2(lambda_))\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        # End\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Forward function of tf. It should be named 'call'\n",
    "        \n",
    "        Args:\n",
    "            x: (n x feature_dim) tensor\n",
    "        Returns:\n",
    "            y_hat: (n x num_class) tensor\n",
    "        \"\"\"\n",
    "        # Start your code here (Forward)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        y_hat = self.softmax(x)\n",
    "        # End\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "e034877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mlptf_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             multiple                  2156928   \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  387       \n",
      "                                                                 \n",
      " softmax_3 (Softmax)         multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2157315 (8.23 MB)\n",
      "Trainable params: 2157315 (8.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "np.random.seed(6666)\n",
    "tf.random.set_seed(6666)\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-1\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "\n",
    "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "model_tf.build(input_shape=(None, vocab_size))\n",
    "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "a556bbb6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.9709 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9540 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9653 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9536 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.9648 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9539 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.9647 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9539 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9647 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9644 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9550 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9644 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9533 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9533 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9520 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.9643 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9547 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9535 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9516 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9526 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9641 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9538 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9531 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9550 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9641 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9542 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9521 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9516 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9641 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9519 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9539 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9641 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9517 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9642 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9641 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9538 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9554 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9536 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9524 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9533 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9641 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9537 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9522 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9542 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9526 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9640 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9523 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9526 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9558 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 1s 12ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9532 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9521 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9546 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9530 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9522 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9516 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9543 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9519 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9535 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9538 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9540 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9639 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9531 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9557 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9539 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9542 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9541 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9531 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9531 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9530 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9553 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 2s 15ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9533 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9638 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9533 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9637 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9539 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9522 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9522 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9523 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9526 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9519 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9534 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9521 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9523 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9540 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9636 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9524 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9538 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9521 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 2s 14ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9526 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9529 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9535 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9635 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9525 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 1s 13ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9527 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 1s 14ms/step - loss: 0.9634 - categorical_accuracy: 0.5627 - f1_score: 0.5627 - val_loss: 0.9528 - val_categorical_accuracy: 0.5867 - val_f1_score: 0.5867\n"
     ]
    }
   ],
   "source": [
    "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2a285",
   "metadata": {},
   "source": [
    "#### Evaluation with Tensroflow\n",
    "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d2668a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD: (0.6757102636293831, 0.6757102636293831, 0.6757102636293831, 0.6757102636293831)\n",
      "123/123 [==============================] - 0s 2ms/step - loss: 0.9604 - categorical_accuracy: 0.5687 - f1_score: 0.5687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9603794813156128, 0.5687227845191956, 0.5687227845191956]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test,False)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725ae0",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics on Test set\n",
    "Fill this table with the result you just printed (double click this cell to edit)\n",
    "|     Optimizer                     | Accuracy    | F1 Score    |\n",
    "|:---------------------------------:|-------------|-------------|\n",
    "|      **Your Implementation**      |     0.675753        | 0.675753            |\n",
    "| **Tensorflow**                    |    0.9722         |    0.554         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cb8e7",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "49bc71a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
    "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390fc13c",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "1ca4c7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax = axes[0]\n",
    "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation Accuracy')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff50430",
   "metadata": {},
   "source": [
    "# 3. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15ce6b",
   "metadata": {},
   "source": [
    "Answer:My implementation achieved an accuracy and F1 score of approximately 0.676. These metrics suggest that the model is performing reasonably well, with a good balance between precision and recall.\n",
    "The TensorFlow implementation achieved a significantly higher accuracy (0.9722) but a lower F1 score (0.554). The high accuracy indicates that the model is making correct predictions for a large portion of the dataset. However, the lower F1 score suggests a potential imbalance between precision and recall, indicating that the model may struggle with correctly classifying certain classes.\n",
    "\n",
    "Perfect Fit: It's possible that your model has memorized the training data and achieved a perfect fit, resulting in zero training loss. This could indicate overfitting, especially if the validation performance is not improving.\n",
    "\n",
    "The increasing trends in both accuracy and F1 score suggest that my model is learning and improving its performance on the validation set over time. This is a positive sign, indicating that your model is continuously learning from the data and adapting to the task.\n",
    "The constant trends in both accuracy and F1 score for the TensorFlow implementation might suggest that the model may have reached a plateau in terms of learning from the validation set. It could be that the model has already learned most of the patterns in the data, and further training iterations do not significantly impact its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69316163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
