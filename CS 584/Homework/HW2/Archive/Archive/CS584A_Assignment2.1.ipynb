{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6cd68c",
   "metadata": {},
   "source": [
    "# CS 584 Assignment 2 -- MLP and Word Vectors\n",
    "\n",
    "#### Name: Akshay Parate\n",
    "#### Stevens ID: 20023008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc6720",
   "metadata": {},
   "source": [
    "## Part A: Multi-Layer Perceptron (MLP) (50 Points)\n",
    "\n",
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the data loading, preprocessing, tokenization, and TF-IDF feature extraction.\n",
    "3. Implement MLP model, evaluation metrics, and Mini-batch GD with AdaGrad.\n",
    "4. Implement the MLP with Tensorflow and compare to your implementation.\n",
    "5. Analysis the results in the Conlusion part.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons, pandas) using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "267ae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may not run this cell after the first installation\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a5c983c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5d8a2",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)\n",
    "\n",
    "* Download the dataset from Canvas\n",
    "* Load data to text and labels\n",
    "* Preprocessing\n",
    "* Tokenization\n",
    "* Split data\n",
    "* Feature extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3884dae",
   "metadata": {},
   "source": [
    "#### Download NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1db93f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to a2-data\\nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk_path = os.path.join('a2-data', 'nltk')\n",
    "nltk.download('stopwords', download_dir=nltk_path)\n",
    "nltk.data.path.append(nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24d9535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    sys.stdout.write(str_ + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c9ee164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c0b2d",
   "metadata": {},
   "source": [
    "### 1.1 Load data\n",
    "\n",
    "- Load sentences and labels\n",
    "- Transform string labels into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d93854c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_label(data_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\" Load sentences and labels from the specified path\n",
    "    Args:\n",
    "        data_path: data_path: path to the data file, e.g., 'a1-data/SMSSpamCollection'\n",
    "        sentences: the raw text list of all sentences\n",
    "    Returns:\n",
    "        labels: the label list of all sentences\n",
    "    \"\"\"\n",
    "    sentences, labels = [], []\n",
    "    # Start your code here (load text and label from files)\n",
    "    file = open(data_path, \"r\", encoding='utf-8')  \n",
    "    fileData = file.readlines()\n",
    "    file.close()\n",
    "    for data in fileData:\n",
    "        splitData = data.split(\"\t\")\n",
    "        sentences.append(splitData[1].split(\"\\n\")[0])\n",
    "        labels.append(splitData[0])\n",
    "    # End\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56034dbe-a4ef-445f-abe6-89f51bb7c0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "85bb60a1-a1e5-4330-aad7-7e9385f8a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataDf = pd.DataFrame()\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "dataDf[\"Sentences\"] = sentences\n",
    "dataDf[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5e1278ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map: {'Arthur Conan Doyle': 0, 'Fyodor Dostoyevsky': 1, 'Jane Austen': 2}\n",
      "Number of sentences and labels: (19536,) (19536,)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join('a2-data', 'books.txt')\n",
    "sentences, labels = load_sentence_label(data_path)\n",
    "label_map = {}\n",
    "for label in sorted(list(set(labels))):\n",
    "    label_map[label] = len(label_map)\n",
    "labels = np.array([label_map[label] for label in labels], dtype=int)\n",
    "sentences = np.array(sentences, dtype=object)\n",
    "\n",
    "print('Label map:', label_map)\n",
    "print('Number of sentences and labels:', sentences.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a9d7d",
   "metadata": {},
   "source": [
    "#### Split the data into training, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e009ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(sentences: np.ndarray,\n",
    "                     labels: np.ndarray,\n",
    "                     test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Split the sentences and labels into training and test data by shuffling\n",
    "    Args:\n",
    "        sentences: A numpy array containing all sentences\n",
    "        labels: A number array containing label ids\n",
    "        test_ratio: A float number to calculate the number of test data\n",
    "\n",
    "    Returns:\n",
    "        train_sentences: A numpy array containing all training sentences\n",
    "        train_labels: A number array containing all training label ids\n",
    "        test_sentences: A numpy array containing all test sentences\n",
    "        test_labels: A number array containing all test label ids\n",
    "    \"\"\"\n",
    "    \n",
    "    assert 0 < test_ratio < 1\n",
    "    assert len(sentences) == len(labels)\n",
    "\n",
    "    train_index, test_index = [], []\n",
    "    # Start your code here (split the index for training and test)\n",
    "    n= int(round(len(sentences) * test_ratio,0))\n",
    "    rand_list=list(range(0, len(sentences)))\n",
    "    shuffled_rand_list = random.sample(rand_list, len(rand_list))\n",
    "    train_index = shuffled_rand_list[n:]\n",
    "    test_index = shuffled_rand_list[:n]\n",
    "    # End\n",
    "    train_sentences, train_labels = sentences[train_index], labels[train_index]\n",
    "    test_sentences, test_labels = sentences[test_index], labels[test_index]\n",
    "    return train_sentences, train_labels, test_sentences, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5cb00f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 14066\n",
      "Validation data length: 1563\n",
      "Test data length: 3907\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "test_ratio = 0.2\n",
    "valid_ratio = 0.1\n",
    "(train_sentences, train_labels,\n",
    "    test_sentences, test_labels) = train_test_split(sentences, labels, test_ratio)\n",
    "(train_sentences, train_labels,\n",
    "    valid_sentences, valid_labels) = train_test_split(train_sentences, train_labels, valid_ratio)\n",
    "\n",
    "print('Training data length:', len(train_sentences))\n",
    "print('Validation data length:', len(valid_sentences))\n",
    "print('Test data length:', len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d1d47c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_label(labels: np.ndarray, label_map: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: The labels of a dataset \n",
    "        label_map: The mapping from label to label id\n",
    "    Returns:\n",
    "        label_count: The mapping from label to its count\n",
    "    \"\"\"\n",
    "    label_count = {key: 0 for key in label_map.keys()}\n",
    "    # Start your code here (count the number of each label)\n",
    "    uniqueVal = label_map.values()\n",
    "    for u,k in zip(uniqueVal,list(label_count.keys())):\n",
    "        x = [i for i in labels if i==u]\n",
    "        label_count[k] = len(x)\n",
    "    # End\n",
    "    return label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4e37c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: {'Arthur Conan Doyle': 1856, 'Fyodor Dostoyevsky': 4290, 'Jane Austen': 7920}\n",
      "Validation: {'Arthur Conan Doyle': 192, 'Fyodor Dostoyevsky': 458, 'Jane Austen': 913}\n",
      "Test: {'Arthur Conan Doyle': 490, 'Fyodor Dostoyevsky': 1196, 'Jane Austen': 2221}\n"
     ]
    }
   ],
   "source": [
    "print('Training:', count_label(train_labels, label_map))\n",
    "print('Validation:', count_label(valid_labels, label_map))\n",
    "print('Test:', count_label(test_labels, label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0e0b5",
   "metadata": {},
   "source": [
    "#### Dataset statistics\n",
    "Fill this table with the statistics you just printed (double click this cell to edit)\n",
    "\n",
    "|                | Arthur Conan Doyle | Fyodor Dostoyevsky | Jane Austen | Total |\n",
    "|:--------------:|--------------------|--------------------|-------------|-------|\n",
    "|  **Training**  |      1865          |     4325           |    7876     | 14066 |\n",
    "| **Validation** |       188          |      491           |     884     |  1563 |\n",
    "|    **Test**    |       485          |     1128           |    2294     |  3907 |\n",
    "|    **Total**   |      2538          |     5944           |   11054     | 19536 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e93b7",
   "metadata": {},
   "source": [
    "### 1.2 Preprocess\n",
    "In this section, you need to remove all the unrelated characters, including punctuation, urls, and numbers. Please fill up the functions and test them by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0fb98fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, punctuation=True, url=True, number=True):\n",
    "        self.punctuation = punctuation\n",
    "        self.url = url\n",
    "        self.number = number\n",
    "\n",
    "    def apply(self, sentence: str) -> str:\n",
    "        \"\"\" Apply the preprocessing rules to the sentence\n",
    "        Args:\n",
    "            sentence: raw sentence\n",
    "        Returns:\n",
    "            sentence: clean sentence\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        if self.url:\n",
    "            sentence = Preprocessor.remove_url(sentence)\n",
    "        if self.punctuation:\n",
    "            sentence = Preprocessor.remove_punctuation(sentence)\n",
    "        if self.number:\n",
    "            sentence = Preprocessor.remove_number(sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(sentence: str) -> str:\n",
    "        \"\"\" Remove punctuations in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible punctuations\n",
    "        Returns:\n",
    "            sentence: sentence without punctuations\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(sentence: str) -> str:\n",
    "        \"\"\" Remove urls in text with re\n",
    "        Args:\n",
    "            sentence: sentence with possible urls\n",
    "        Returns:\n",
    "            sentence: sentence without urls\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        sentence = url_pattern.sub(\"\", sentence)\n",
    "        # End\n",
    "        return sentence\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_number(sentence: str) -> str:\n",
    "        \"\"\" Remove numbers in sentence with re\n",
    "        Args:\n",
    "            sentence: sentence with possible numbers\n",
    "        Returns:\n",
    "            sentence: sentence without numbers\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        sentence = re.sub(r'\\d', '', sentence)\n",
    "        # End\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec028d49",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e6d7e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"interest rates are trimmed to by the south african central bank but the lack of warning hits the rand and surprises markets\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{clean_sentence}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238aadd7",
   "metadata": {},
   "source": [
    "### 1.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84e5e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['below', 'all', 'were', 'am', 'itself', 'not', 'once', 'his', 'himself', 'y']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "print(list(stopwords_set)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0fa11482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\" Tokenize a sentence into tokens (words)\n",
    "    Args:\n",
    "        sentence: clean sentence\n",
    "    Returns:\n",
    "        tokens\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    # Start your code here\n",
    "    #     Step 1. Split sentence into words\n",
    "    splitWords = sentence.split(\" \")\n",
    "    #     Step 2. Extract word stem using the defined stemmer (PorterStemmer) by calling stemmer.stem(word)\n",
    "    stemmedWords = [stemmer.stem(word) for word in splitWords]\n",
    "    #     Step 3. Remove stop words using the defined stopwords_set\n",
    "    words = [w for w in stemmedWords if not w in stopwords_set]\n",
    "    # End\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb0ba0",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ce65f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
      "===>\n",
      "\"['interest', 'rate', 'trim', 'south', 'african', 'central', 'bank', 'lack', 'warn', 'hit', 'rand', 'surpris', 'market']\"\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Interest rates are trimmed to 7.5 by the South African central bank (https://www.xxx.xxx), but the lack of warning hits the rand and surprises markets.\"\n",
    "\n",
    "processor = Preprocessor()\n",
    "clean_sentence = processor.apply(sentence)\n",
    "tokens = tokenize(clean_sentence)\n",
    "\n",
    "print(f'\"{sentence}\"') \n",
    "print('===>')\n",
    "print(f'\"{tokens}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa5afa",
   "metadata": {},
   "source": [
    "### 1.5 Feature Extraction\n",
    "\n",
    "TF-IDF:\n",
    "$$\\text{TF-IDF}(t, d) = \\frac{f_{t, d}}{\\sum_{t'}{f_{t', d}}} \\times \\log{\\frac{N}{n_t}}$$\n",
    "\n",
    "- $t$: A term\n",
    "- $d$: A document. Here, we regard a sentence as a document\n",
    "- $f_{t, d}$: Number of term $t$ in $d$\n",
    "- $N$: Number of document\n",
    "- $n_t$: Number of document containing $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b7239b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TfIdfEncoder:\n",
    "    def __init__(self):\n",
    "        self.vocab = defaultdict(int)\n",
    "        self.token2index = {}\n",
    "        self.df = defaultdict(int)\n",
    "        self.num_doc = 0\n",
    "        self.processor = Preprocessor()\n",
    "\n",
    "    def fit(self, sentences: Union[List[str], np.ndarray]) -> int:\n",
    "        \"\"\" Using the given texts to store key information in TF-IDF calculation\n",
    "            In this function, you are required to implement the fitting process.\n",
    "                1. Construct the vocabulary and store the frequency of tokens (self.vocab).\n",
    "                2. Construct the document frequency map to tokens (self.df).\n",
    "                3. Construct the token to index map based on the frequency.\n",
    "                   The token with a higher frequency has the smaller index\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            token_num\n",
    "        \"\"\"\n",
    "        self.num_doc = len(sentences)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == len(sentences) - 1:\n",
    "                print_line('Fitting TF-IDF encoder:', (i + 1), '/', len(sentences))\n",
    "            # Start your code here (step 1 & 2)\n",
    "            clean_sentence = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentence)\n",
    "            unique_tokens = set(tokens)\n",
    "            for token in unique_tokens:\n",
    "                self.vocab[token] += 1\n",
    "                self.df[token] += 1\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        # Start your code here (Step 3)\n",
    "        sorted_tokens = sorted(self.vocab.keys(), key=lambda x: self.vocab[x], reverse=True) #sorting tokens in descending order based on the frequency.\n",
    "        self.token2index = {token: idx for idx, token in enumerate(sorted_tokens)} #Maps each token to its index in the sorted list.\n",
    "        # End\n",
    "        token_num = len(self.token2index) \n",
    "        print('The number of distinct tokens:', token_num)\n",
    "        return token_num\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], np.ndarray]) -> np.ndarray:\n",
    "        \"\"\" Encode the sentences into TF-IDF feature vector\n",
    "            Note: if a token in a sentence does not exist in the fit encoder, we just ignore it.\n",
    "        Args:\n",
    "            sentences: Raw sentences\n",
    "        Returns:\n",
    "            features: A (n x token_num) matrix, where n is the number of sentences\n",
    "        \"\"\"\n",
    "        n = len(sentences)\n",
    "        features = np.zeros((n, len(self.token2index)))\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0 or i == n - 1:\n",
    "                print_line('Encoding with TF-IDF encoder:', (i + 1), '/', n)\n",
    "            # Start your code (calculate TF-IDF)\n",
    "            clean_sentence = self.processor.apply(sentence)\n",
    "            tokens = tokenize(clean_sentence)\n",
    "            for token in tokens:\n",
    "                if token in self.token2index:\n",
    "                    features[i, self.token2index[token]] += 1 / self.df[token]\n",
    "            # End\n",
    "        print_line('\\n')\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238e87e",
   "metadata": {},
   "source": [
    "##### Test your implementation by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77d41a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 100 / 100\n",
      "The number of distinct tokens: 1351\n",
      "Encoding with TF-IDF encoder: 10 / 10\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03333333 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.04545455 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "encoder = TfIdfEncoder()\n",
    "encoder.fit(train_sentences[:100])\n",
    "features = encoder.encode(train_sentences[:10])\n",
    "\n",
    "print(features[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b343de5",
   "metadata": {},
   "source": [
    "#### Encode training, validation, and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ffa24",
   "metadata": {},
   "source": [
    "## 2. MLP (20 Points)\n",
    "In this section, you are required to implement a two-layer MLP model (input -> hidden layer -> output layer) with $L_2$ regularization from scratch. \n",
    "\n",
    "The objective function of LR for multi-class classification:\n",
    "\n",
    "$$J = L(\\mathbf{x}, \\mathbf{y} \\mid \\mathbf{w}, \\mathbf{b}) = -\\frac{1}{n}\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik}log\\frac{e^{f_k}}{\\sum_{c=1}^{K}e^{f_c}} + \\lambda \\sum_{j=1}^{d}w_{kj}^2$$\n",
    "\n",
    "- $z_1 = w_1x$\n",
    "- $h_1 = activation(z_1)$\n",
    "- $z_2 = w_2 h_1$\n",
    "- $\\hat{y} = softmax(z_2)$\n",
    "\n",
    "- $n$: Number of samples\n",
    "- $d$: Dimension of $\\mathbf{w}$\n",
    "- Here, you can use `sigmoid` as the activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d731b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF encoder: 14066 / 14066\n",
      "The number of distinct tokens: 16798\n",
      "16798\n",
      "Encoding with TF-IDF encoder: 14066 / 14066\n",
      "Encoding with TF-IDF encoder: 1563 / 1563\n",
      "Encoding with TF-IDF encoder: 3907 / 3907\n",
      "The size of training set: (14066, 16798) (14066, 3)\n",
      "The size of validation set: (1563, 16798) (1563, 3)\n",
      "The size of test set: (3907, 16798) (3907, 3)\n"
     ]
    }
   ],
   "source": [
    "num_class = 3\n",
    "\n",
    "encoder = TfIdfEncoder()\n",
    "vocab_size = encoder.fit(train_sentences)\n",
    "print(vocab_size)\n",
    "\n",
    "x_train = encoder.encode(train_sentences)\n",
    "x_valid = encoder.encode(valid_sentences)\n",
    "x_test = encoder.encode(test_sentences)\n",
    "\n",
    "y_train = np.zeros((len(train_labels), num_class))\n",
    "y_valid = np.zeros((len(valid_labels), num_class))\n",
    "y_test = np.zeros((len(test_labels), num_class))\n",
    "#One hot encoding - The code uses NumPy indexing to assign 1 to the appropriate column for each sample in the training, validation, and test sets.\n",
    "y_train[np.arange(len(train_labels)), train_labels] = 1\n",
    "y_valid[np.arange(len(valid_labels)), valid_labels] = 1\n",
    "y_test[np.arange(len(test_labels)), test_labels] = 1\n",
    "\n",
    "print('The size of training set:', x_train.shape, y_train.shape)\n",
    "print('The size of validation set:', x_valid.shape, y_valid.shape)\n",
    "print('The size of test set:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16af1ff",
   "metadata": {},
   "source": [
    "### 2.1 MLP Model (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "eb8d4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.w1 = np.random.randn(feature_dim, hidden_dim)\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.w2 = np.random.randn(hidden_dim, num_class)\n",
    "        self.b2 = np.zeros((1, num_class))\n",
    "        # End\n",
    "        self.lambda_ = lambda_\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, x: np.ndarray, return_hiddens) -> np.ndarray:\n",
    "        \"\"\" Forward process of logistic regression\n",
    "            Calculate y_hat using x\n",
    "        Args:\n",
    "            x: Input data\n",
    "            return_hiddens: If true the function will return h1 for gradient calculation\n",
    "        Returns:\n",
    "            y_hat: Output\n",
    "            h1: Hidden output, used for gradient calculation. Returned if return_hiddens is set to True\n",
    "        \"\"\"\n",
    "        y_hat = 0\n",
    "        h1 = 0, 0\n",
    "        w1, b1, w2, b2 = self.w1, self.b1, self.w2, self.b2\n",
    "        # Start your code here (calculate y_hat of MLP using x)\n",
    "        z1 = np.dot(x, w1) + b1\n",
    "        h1 = sigmoid(z1)\n",
    "        z2 = np.dot(h1,w2) + b2\n",
    "        y_hat = sigmoid(z2)\n",
    "        # End\n",
    "        if return_hiddens:\n",
    "            return y_hat, h1\n",
    "        else:\n",
    "            return y_hat\n",
    "\n",
    "    def backward(self,\n",
    "                 x: np.ndarray,\n",
    "                 y_hat: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 h1: np.array) -> Tuple[np.ndarray, Union[float, np.ndarray], np.ndarray, Union[float, np.ndarray]]:\n",
    "        \"\"\" Backward process of logistic regression\n",
    "            Calculate the gradient of w and b\n",
    "        Args:\n",
    "            x: Input data\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "            h1: Hidden output of the hidden layer\n",
    "        Returns:\n",
    "            dw1: Gradient of w1\n",
    "            db1: Gradient of b1\n",
    "            dw2: Gradient of w2\n",
    "            db2: Gradient of b2\n",
    "        \"\"\"\n",
    "        w1, w2 = self.w1, self.w2\n",
    "        dw1, db1, dw2, db2 = 0.0, 0.0, 0.0, 0.0\n",
    "        n = len(x)\n",
    "        # Start your code here (calculate the gradient of w and b)\n",
    "        dz2 = y_hat - y\n",
    "        dw2 = np.transpose(h1) @ dz2\n",
    "        db2 = np.sum(dz2,axis = 0,keepdims = True)\n",
    "        dz1 = dz2 @ np.transpose(w2) * (h1 * (1-h1))\n",
    "        dw1 = np.transpose(x) @ dz1\n",
    "        db1 = np.sum(dz1,axis = 0,keepdims = True)\n",
    "        # End\n",
    "        return dw1, db1, dw2, db2\n",
    "\n",
    "    def categorical_cross_entropy_loss(self,\n",
    "                                       y_hat: np.ndarray,\n",
    "                                       y: np.ndarray) -> Union[float, np.ndarray]:\n",
    "        \"\"\" Calculate the binary cross-entropy loss\n",
    "        Args:\n",
    "            y_hat: Output of forward\n",
    "            y: Ground-truth\n",
    "        Returns:\n",
    "            loss: BCE loss\n",
    "        \"\"\"\n",
    "        y_hat = np.clip(y_hat, a_min=self.eps, a_max=1 - self.eps)\n",
    "        loss = 0\n",
    "        n = y.shape[0]\n",
    "        # Start your code here (Calculate the binary cross-entropy)\n",
    "#         print(y.shape)\n",
    "        loss = -1/n * np.sum(y * np.log(y_hat)) + self.lambda_ * (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))\n",
    "        # End\n",
    "        return loss\n",
    "\n",
    "    def gradient_descent(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float], lr: float):\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "\n",
    "    def predict(self, y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Predict the label using the output y_hat\n",
    "        Args:\n",
    "            y_hat: Model output\n",
    "        Returns:\n",
    "            pred: Prediction\n",
    "        \"\"\"\n",
    "        pred = np.zeros_like(y_hat)\n",
    "        index = np.argmax(y_hat, axis=-1)\n",
    "        pred[np.arange(len(y_hat)), index] = 1\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6bdfa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    \"\"\" The softmax activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "        axis: The dimension of x that needs to run softmax, default -1, i.e., the last dimension\n",
    "    Returns:\n",
    "        output: Softmax value of the specified dimension in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    exp_scores = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    x = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # End\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" The sigmoid activation function\n",
    "    Args:\n",
    "        x: Input matrix or vector\n",
    "    Returns:\n",
    "        output: Sigmoid value of each entry in x\n",
    "    \"\"\"\n",
    "    # Start your code here\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    # End\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40ab16de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\AppData\\Local\\Temp\\ipykernel_8\\2069942790.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #:  50\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "lossList = []\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "# uncomment this line\n",
    "for i in range(0,100):\n",
    "    if(i%50 == 0):\n",
    "        print(\"Epoch #: \",i)\n",
    "    y_hat,h1 = model_mbgd.forward(x_train,True)\n",
    "    prob = softmax(y_hat)\n",
    "    loss = model_mbgd.categorical_cross_entropy_loss(prob, y_train)\n",
    "    lossList.append(loss)\n",
    "    dw1, db1, dw2, db2 = model_mbgd.backward(x_train,y_hat,y_train,h1)\n",
    "    # print(dw1.shape, db1.shape, dw2.shape, db2.shape)\n",
    "    # print(model_mbgd.w1.shape, model_mbgd.b1.shape, model_mbgd.w2.shape, model_mbgd.b2.shape)\n",
    "    model_mbgd.gradient_descent(dw1, db1, dw2, db2,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8499231",
   "metadata": {},
   "source": [
    "### 2.2 Evaluation Metrics\n",
    "\n",
    "Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c18e32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def get_metrics(y_pred: np.ndarray, y_true: np.ndarray) -> Tuple[float, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Calculate the accuracy, precision, recall, and f1 score.\n",
    "        You are allowed to use precision_recall_fscore_support from scikit-learn. Please set average to 'micro'\n",
    "    Args:\n",
    "        y_pred: Prediction\n",
    "        y_true: Ground-truth\n",
    "    Returns:\n",
    "        accuracy: float number. The accuracy for the whole dataset\n",
    "        precision, recall, f1: np.ndarray (num_class, ). The precision, recall, f1 for each class\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "    # Start your code here\n",
    "    correct_predictions = 0\n",
    "    for i in range(0,len(y_true)):\n",
    "        if all(y_pred[i] == y_true[i]):\n",
    "            correct_predictions = correct_predictions + 1\n",
    "        else:\n",
    "            pass\n",
    "    total_examples = len(y_true)\n",
    "    accuracy = correct_predictions / total_examples\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f69d4af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini-batch GD: (0.29818274891220886, 0.29818274891220886, 0.29818274891220886, 0.29818274891220886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\AppData\\Local\\Temp\\ipykernel_8\\2069942790.py:24: RuntimeWarning: overflow encountered in exp\n",
      "  x = 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test,False)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "# model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864559b1",
   "metadata": {},
   "source": [
    "### 2.3 AdaGrad (5 points)\n",
    "\n",
    "$$ \\mathbf{G}^{(t + 1)} \\leftarrow \\mathbf{G}^{(t)} + \\boldsymbol{g}^{(t + 1)} \\cdot \\boldsymbol{g}^{(t + 1)} $$\n",
    "$$ \\mathbf{w}^{(t + 1)} \\leftarrow \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}}\\boldsymbol{g}^{(t + 1)} = \\mathbf{w}^{(t)} - \\eta\\frac{\\boldsymbol{g}^{(t + 1)}}{\\sqrt{\\mathbf{G}^{(t + 1)} + \\epsilon}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d123002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, init_lr, model):\n",
    "        self.init_lr = init_lr\n",
    "        self.model = model\n",
    "        \n",
    "        self.accumulative_dw1 = 0\n",
    "        self.accumulative_db1 = 0\n",
    "        self.accumulative_dw2 = 0\n",
    "        self.accumulative_db2 = 0\n",
    "        self.eps = 1e-9\n",
    "        \n",
    "    def update(self, dw1: np.ndarray, db1: Union[np.ndarray, float], dw2: np.ndarray, db2: Union[np.ndarray, float]):\n",
    "        \"\"\" 1. Use the gradient in the current step to update the accumulative gradient of each parameter.\n",
    "            2. Calculate the new gradient with the accumulative gradient\n",
    "            3. Use the init learning rate the new gradient to update the parameter with model.gradient_descent()\n",
    "        \n",
    "        Do not return anything\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1\n",
    "        self.accumulative_dw1 += dw1 ** 2\n",
    "        self.accumulative_db1 += db1 ** 2\n",
    "        self.accumulative_dw2 += dw2 ** 2\n",
    "        self.accumulative_db2 += db2 ** 2\n",
    "        # Step 2\n",
    "        new_dw1 = dw1 / (np.sqrt(self.accumulative_dw1) + self.eps)\n",
    "        new_db1 = db1 / (np.sqrt(self.accumulative_db1) + self.eps)\n",
    "        new_dw2 = dw2 / (np.sqrt(self.accumulative_dw2) + self.eps)\n",
    "        new_db2 = db2 / (np.sqrt(self.accumulative_db2) + self.eps)\n",
    "        # Step 3\n",
    "        self.model.gradient_descent(new_dw1, new_db1, new_dw2, new_db2, self.init_lr)\n",
    "        # End\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26ba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40336a46",
   "metadata": {},
   "source": [
    "### 2.4 Mini-batch Gradient Descent (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb2e3176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def train_mbgd(model,\n",
    "               x_train: np.ndarray,\n",
    "               y_train: np.ndarray,\n",
    "               x_valid: np.ndarray,\n",
    "               y_valid: np.ndarray,\n",
    "               lr: float,\n",
    "               num_epoch: int,\n",
    "               batch_size: int,\n",
    "               print_every: int = 10) -> Tuple[dict[str, List], dict[str, List]]:\n",
    "    \"\"\" Training with Gradient Descent\n",
    "    Args:\n",
    "        model: The logistic regression model\n",
    "        x_train: Training feature, (n x d) matrix\n",
    "        y_train: Training label, (n, ) vector\n",
    "        x_valid: Validation feature, (n x d) matrix\n",
    "        y_valid: Validation label, (n, ) vector\n",
    "        lr: Learning rate\n",
    "        num_epoch: Number of training epochs\n",
    "        batch_size: Number of training samples in a batch\n",
    "        print_every: Print log every {print_every} epochs\n",
    "    Returns:\n",
    "        train_history: Log of training information. The format of training history is\n",
    "                       { 'loss': [] }\n",
    "                       It records the average loss of each epoch.\n",
    "        valid_history: Log of validation information. The format of training and validation history is\n",
    "                       {\n",
    "                           'loss': [],\n",
    "                           'accuracy': [],\n",
    "                           'precision': [],\n",
    "                           'recall': [],\n",
    "                           'f1': []\n",
    "                       }\n",
    "    \"\"\"\n",
    "    train_history = OrderedDict({'loss': []})\n",
    "    valid_history = OrderedDict({\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    })\n",
    "\n",
    "    def format_output(epoch, num_epoch, train_history, valid_history):\n",
    "        epoch_log = f'Epoch {epoch + 1} / {num_epoch}'\n",
    "        train_log = ' - '.join([f'train_{key}: {val[-1]:.4f}' for key, val in train_history.items()])\n",
    "        valid_log = ' - '.join([f'valid_{key}: {val[-1]:.4f}' for key, val in valid_history.items()])\n",
    "        log = f'{epoch_log}: {train_log} - {valid_log}'\n",
    "        return log\n",
    "\n",
    "    # IMPORTANT: YOU SHOULD USE THIS OPTIMIZER TO UPDATE THE MODEL\n",
    "    optimizer = AdaGrad(init_lr=lr, model=model)\n",
    "\n",
    "    train_num_samples = len(x_train)\n",
    "    n_batch = train_num_samples // batch_size\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        # Start your code here (training)\n",
    "        #     Step 1. Model forward\n",
    "        y_hat,h1 = model.forward(x_train,True)\n",
    "        #     Step 2. Calculate loss\n",
    "        prob = softmax(y_hat)\n",
    "        epoch_loss = model.categorical_cross_entropy_loss(prob, y_train)\n",
    "        #     Step 3. Model backward\n",
    "        dw1, db1, dw2, db2 = model.backward(x_train,y_hat,y_train,h1)\n",
    "        #     Step 4. Optimization with Adagrad\n",
    "        optimizer.update(dw1, db1, dw2, db2)\n",
    "        # End\n",
    "\n",
    "        valid_loss = 0.\n",
    "        accuracy, precision, recall, f1 = 0.0, 0.0, 0.0, 0.0\n",
    "        # Start your code here (validation)\n",
    "        #     Step 1. Predict\n",
    "        y_hat = model.forward(x_valid,False)\n",
    "        pred = model.predict(y_hat)\n",
    "        #     Step 2. Calculate loss\n",
    "        prob = softmax(y_hat)\n",
    "        valid_loss = model.categorical_cross_entropy_loss(prob, y_valid)\n",
    "        valid_history[\"loss\"].append(loss)\n",
    "        #     Step 3. Calculate metrics\n",
    "        accuracy, precision, recall, f1 = get_metrics(pred,y_valid)\n",
    "        valid_history[\"f1\"].append(f1)\n",
    "        valid_history[\"recall\"].append(recall)\n",
    "        valid_history[\"precision\"].append(precision)\n",
    "        valid_history[\"accuracy\"].append(accuracy)\n",
    "#         End\n",
    "        train_history['loss'].append(epoch_loss / train_num_samples)\n",
    "        for vals, val in zip(valid_history.values(), [valid_loss, accuracy, precision, recall, f1]):\n",
    "            vals.append(val)\n",
    "        log = format_output(epoch, num_epoch, train_history, valid_history)\n",
    "        if epoch % print_every == 0 or epoch == num_epoch - 1:\n",
    "            print(log)\n",
    "        else:\n",
    "            print_line(log)\n",
    "    return train_history, valid_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbec04",
   "metadata": {},
   "source": [
    "Run Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "adfd49fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1000: train_loss: 0.0001 - valid_loss: 1.2245 - valid_accuracy: 0.5381 - valid_precision: 0.5381 - valid_recall: 0.5381 - valid_f1: 0.5381\n",
      "Epoch 11 / 1000: train_loss: 0.0001 - valid_loss: 1.1961 - valid_accuracy: 0.5771 - valid_precision: 0.5771 - valid_recall: 0.5771 - valid_f1: 0.5771\n",
      "Epoch 21 / 1000: train_loss: 0.0001 - valid_loss: 1.1842 - valid_accuracy: 0.6040 - valid_precision: 0.6040 - valid_recall: 0.6040 - valid_f1: 0.6040\n",
      "Epoch 31 / 1000: train_loss: 0.0001 - valid_loss: 1.1764 - valid_accuracy: 0.6161 - valid_precision: 0.6161 - valid_recall: 0.6161 - valid_f1: 0.6161\n",
      "Epoch 41 / 1000: train_loss: 0.0001 - valid_loss: 1.1692 - valid_accuracy: 0.6270 - valid_precision: 0.6270 - valid_recall: 0.6270 - valid_f1: 0.6270\n",
      "Epoch 51 / 1000: train_loss: 0.0001 - valid_loss: 1.1628 - valid_accuracy: 0.6360 - valid_precision: 0.6360 - valid_recall: 0.6360 - valid_f1: 0.6360\n",
      "Epoch 61 / 1000: train_loss: 0.0001 - valid_loss: 1.1571 - valid_accuracy: 0.6424 - valid_precision: 0.6424 - valid_recall: 0.6424 - valid_f1: 0.6424\n",
      "Epoch 71 / 1000: train_loss: 0.0001 - valid_loss: 1.1521 - valid_accuracy: 0.6513 - valid_precision: 0.6513 - valid_recall: 0.6513 - valid_f1: 0.6513\n",
      "Epoch 81 / 1000: train_loss: 0.0001 - valid_loss: 1.1477 - valid_accuracy: 0.6571 - valid_precision: 0.6571 - valid_recall: 0.6571 - valid_f1: 0.6571\n",
      "Epoch 91 / 1000: train_loss: 0.0001 - valid_loss: 1.1438 - valid_accuracy: 0.6609 - valid_precision: 0.6609 - valid_recall: 0.6609 - valid_f1: 0.6609\n",
      "Epoch 101 / 1000: train_loss: 0.0001 - valid_loss: 1.1402 - valid_accuracy: 0.6673 - valid_precision: 0.6673 - valid_recall: 0.6673 - valid_f1: 0.6673\n",
      "Epoch 111 / 1000: train_loss: 0.0001 - valid_loss: 1.1370 - valid_accuracy: 0.6718 - valid_precision: 0.6718 - valid_recall: 0.6718 - valid_f1: 0.6718\n",
      "Epoch 121 / 1000: train_loss: 0.0001 - valid_loss: 1.1340 - valid_accuracy: 0.6763 - valid_precision: 0.6763 - valid_recall: 0.6763 - valid_f1: 0.6763\n",
      "Epoch 131 / 1000: train_loss: 0.0001 - valid_loss: 1.1312 - valid_accuracy: 0.6795 - valid_precision: 0.6795 - valid_recall: 0.6795 - valid_f1: 0.6795\n",
      "Epoch 141 / 1000: train_loss: 0.0001 - valid_loss: 1.1287 - valid_accuracy: 0.6871 - valid_precision: 0.6871 - valid_recall: 0.6871 - valid_f1: 0.6871\n",
      "Epoch 151 / 1000: train_loss: 0.0001 - valid_loss: 1.1263 - valid_accuracy: 0.6903 - valid_precision: 0.6903 - valid_recall: 0.6903 - valid_f1: 0.6903\n",
      "Epoch 161 / 1000: train_loss: 0.0001 - valid_loss: 1.1240 - valid_accuracy: 0.6942 - valid_precision: 0.6942 - valid_recall: 0.6942 - valid_f1: 0.6942\n",
      "Epoch 171 / 1000: train_loss: 0.0001 - valid_loss: 1.1219 - valid_accuracy: 0.6967 - valid_precision: 0.6967 - valid_recall: 0.6967 - valid_f1: 0.6967\n",
      "Epoch 181 / 1000: train_loss: 0.0001 - valid_loss: 1.1199 - valid_accuracy: 0.6967 - valid_precision: 0.6967 - valid_recall: 0.6967 - valid_f1: 0.6967\n",
      "Epoch 191 / 1000: train_loss: 0.0001 - valid_loss: 1.1180 - valid_accuracy: 0.6980 - valid_precision: 0.6980 - valid_recall: 0.6980 - valid_f1: 0.6980\n",
      "Epoch 201 / 1000: train_loss: 0.0001 - valid_loss: 1.1162 - valid_accuracy: 0.6993 - valid_precision: 0.6993 - valid_recall: 0.6993 - valid_f1: 0.6993\n",
      "Epoch 211 / 1000: train_loss: 0.0001 - valid_loss: 1.1145 - valid_accuracy: 0.7019 - valid_precision: 0.7019 - valid_recall: 0.7019 - valid_f1: 0.7019\n",
      "Epoch 221 / 1000: train_loss: 0.0001 - valid_loss: 1.1129 - valid_accuracy: 0.7044 - valid_precision: 0.7044 - valid_recall: 0.7044 - valid_f1: 0.7044\n",
      "Epoch 231 / 1000: train_loss: 0.0001 - valid_loss: 1.1113 - valid_accuracy: 0.7070 - valid_precision: 0.7070 - valid_recall: 0.7070 - valid_f1: 0.7070\n",
      "Epoch 241 / 1000: train_loss: 0.0001 - valid_loss: 1.1098 - valid_accuracy: 0.7108 - valid_precision: 0.7108 - valid_recall: 0.7108 - valid_f1: 0.7108\n",
      "Epoch 251 / 1000: train_loss: 0.0001 - valid_loss: 1.1084 - valid_accuracy: 0.7115 - valid_precision: 0.7115 - valid_recall: 0.7115 - valid_f1: 0.7115\n",
      "Epoch 261 / 1000: train_loss: 0.0001 - valid_loss: 1.1070 - valid_accuracy: 0.7140 - valid_precision: 0.7140 - valid_recall: 0.7140 - valid_f1: 0.7140\n",
      "Epoch 271 / 1000: train_loss: 0.0001 - valid_loss: 1.1057 - valid_accuracy: 0.7179 - valid_precision: 0.7179 - valid_recall: 0.7179 - valid_f1: 0.7179\n",
      "Epoch 281 / 1000: train_loss: 0.0001 - valid_loss: 1.1044 - valid_accuracy: 0.7198 - valid_precision: 0.7198 - valid_recall: 0.7198 - valid_f1: 0.7198\n",
      "Epoch 291 / 1000: train_loss: 0.0001 - valid_loss: 1.1032 - valid_accuracy: 0.7217 - valid_precision: 0.7217 - valid_recall: 0.7217 - valid_f1: 0.7217\n",
      "Epoch 301 / 1000: train_loss: 0.0001 - valid_loss: 1.1020 - valid_accuracy: 0.7242 - valid_precision: 0.7242 - valid_recall: 0.7242 - valid_f1: 0.7242\n",
      "Epoch 311 / 1000: train_loss: 0.0001 - valid_loss: 1.1009 - valid_accuracy: 0.7255 - valid_precision: 0.7255 - valid_recall: 0.7255 - valid_f1: 0.7255\n",
      "Epoch 321 / 1000: train_loss: 0.0001 - valid_loss: 1.0998 - valid_accuracy: 0.7262 - valid_precision: 0.7262 - valid_recall: 0.7262 - valid_f1: 0.7262\n",
      "Epoch 331 / 1000: train_loss: 0.0001 - valid_loss: 1.0987 - valid_accuracy: 0.7274 - valid_precision: 0.7274 - valid_recall: 0.7274 - valid_f1: 0.7274\n",
      "Epoch 341 / 1000: train_loss: 0.0001 - valid_loss: 1.0977 - valid_accuracy: 0.7281 - valid_precision: 0.7281 - valid_recall: 0.7281 - valid_f1: 0.7281\n",
      "Epoch 351 / 1000: train_loss: 0.0001 - valid_loss: 1.0966 - valid_accuracy: 0.7281 - valid_precision: 0.7281 - valid_recall: 0.7281 - valid_f1: 0.7281\n",
      "Epoch 361 / 1000: train_loss: 0.0001 - valid_loss: 1.0957 - valid_accuracy: 0.7287 - valid_precision: 0.7287 - valid_recall: 0.7287 - valid_f1: 0.7287\n",
      "Epoch 371 / 1000: train_loss: 0.0001 - valid_loss: 1.0947 - valid_accuracy: 0.7300 - valid_precision: 0.7300 - valid_recall: 0.7300 - valid_f1: 0.7300\n",
      "Epoch 381 / 1000: train_loss: 0.0001 - valid_loss: 1.0938 - valid_accuracy: 0.7306 - valid_precision: 0.7306 - valid_recall: 0.7306 - valid_f1: 0.7306\n",
      "Epoch 391 / 1000: train_loss: 0.0001 - valid_loss: 1.0929 - valid_accuracy: 0.7319 - valid_precision: 0.7319 - valid_recall: 0.7319 - valid_f1: 0.7319\n",
      "Epoch 401 / 1000: train_loss: 0.0001 - valid_loss: 1.0920 - valid_accuracy: 0.7326 - valid_precision: 0.7326 - valid_recall: 0.7326 - valid_f1: 0.7326\n",
      "Epoch 411 / 1000: train_loss: 0.0001 - valid_loss: 1.0912 - valid_accuracy: 0.7326 - valid_precision: 0.7326 - valid_recall: 0.7326 - valid_f1: 0.7326\n",
      "Epoch 421 / 1000: train_loss: 0.0001 - valid_loss: 1.0904 - valid_accuracy: 0.7345 - valid_precision: 0.7345 - valid_recall: 0.7345 - valid_f1: 0.7345\n",
      "Epoch 431 / 1000: train_loss: 0.0001 - valid_loss: 1.0895 - valid_accuracy: 0.7351 - valid_precision: 0.7351 - valid_recall: 0.7351 - valid_f1: 0.7351\n",
      "Epoch 441 / 1000: train_loss: 0.0001 - valid_loss: 1.0888 - valid_accuracy: 0.7358 - valid_precision: 0.7358 - valid_recall: 0.7358 - valid_f1: 0.7358\n",
      "Epoch 451 / 1000: train_loss: 0.0001 - valid_loss: 1.0880 - valid_accuracy: 0.7364 - valid_precision: 0.7364 - valid_recall: 0.7364 - valid_f1: 0.7364\n",
      "Epoch 461 / 1000: train_loss: 0.0001 - valid_loss: 1.0873 - valid_accuracy: 0.7383 - valid_precision: 0.7383 - valid_recall: 0.7383 - valid_f1: 0.7383\n",
      "Epoch 471 / 1000: train_loss: 0.0001 - valid_loss: 1.0865 - valid_accuracy: 0.7383 - valid_precision: 0.7383 - valid_recall: 0.7383 - valid_f1: 0.7383\n",
      "Epoch 481 / 1000: train_loss: 0.0001 - valid_loss: 1.0858 - valid_accuracy: 0.7402 - valid_precision: 0.7402 - valid_recall: 0.7402 - valid_f1: 0.7402\n",
      "Epoch 491 / 1000: train_loss: 0.0001 - valid_loss: 1.0851 - valid_accuracy: 0.7422 - valid_precision: 0.7422 - valid_recall: 0.7422 - valid_f1: 0.7422\n",
      "Epoch 501 / 1000: train_loss: 0.0001 - valid_loss: 1.0844 - valid_accuracy: 0.7422 - valid_precision: 0.7422 - valid_recall: 0.7422 - valid_f1: 0.7422\n",
      "Epoch 511 / 1000: train_loss: 0.0001 - valid_loss: 1.0838 - valid_accuracy: 0.7441 - valid_precision: 0.7441 - valid_recall: 0.7441 - valid_f1: 0.7441\n",
      "Epoch 521 / 1000: train_loss: 0.0001 - valid_loss: 1.0831 - valid_accuracy: 0.7441 - valid_precision: 0.7441 - valid_recall: 0.7441 - valid_f1: 0.7441\n",
      "Epoch 531 / 1000: train_loss: 0.0001 - valid_loss: 1.0825 - valid_accuracy: 0.7447 - valid_precision: 0.7447 - valid_recall: 0.7447 - valid_f1: 0.7447\n",
      "Epoch 541 / 1000: train_loss: 0.0001 - valid_loss: 1.0819 - valid_accuracy: 0.7460 - valid_precision: 0.7460 - valid_recall: 0.7460 - valid_f1: 0.7460\n",
      "Epoch 551 / 1000: train_loss: 0.0001 - valid_loss: 1.0812 - valid_accuracy: 0.7454 - valid_precision: 0.7454 - valid_recall: 0.7454 - valid_f1: 0.7454\n",
      "Epoch 561 / 1000: train_loss: 0.0001 - valid_loss: 1.0806 - valid_accuracy: 0.7473 - valid_precision: 0.7473 - valid_recall: 0.7473 - valid_f1: 0.7473\n",
      "Epoch 571 / 1000: train_loss: 0.0001 - valid_loss: 1.0801 - valid_accuracy: 0.7466 - valid_precision: 0.7466 - valid_recall: 0.7466 - valid_f1: 0.7466\n",
      "Epoch 581 / 1000: train_loss: 0.0001 - valid_loss: 1.0795 - valid_accuracy: 0.7466 - valid_precision: 0.7466 - valid_recall: 0.7466 - valid_f1: 0.7466\n",
      "Epoch 591 / 1000: train_loss: 0.0001 - valid_loss: 1.0789 - valid_accuracy: 0.7479 - valid_precision: 0.7479 - valid_recall: 0.7479 - valid_f1: 0.7479\n",
      "Epoch 601 / 1000: train_loss: 0.0001 - valid_loss: 1.0784 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 611 / 1000: train_loss: 0.0001 - valid_loss: 1.0778 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 621 / 1000: train_loss: 0.0001 - valid_loss: 1.0773 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 631 / 1000: train_loss: 0.0001 - valid_loss: 1.0768 - valid_accuracy: 0.7486 - valid_precision: 0.7486 - valid_recall: 0.7486 - valid_f1: 0.7486\n",
      "Epoch 641 / 1000: train_loss: 0.0001 - valid_loss: 1.0763 - valid_accuracy: 0.7498 - valid_precision: 0.7498 - valid_recall: 0.7498 - valid_f1: 0.7498\n",
      "Epoch 651 / 1000: train_loss: 0.0001 - valid_loss: 1.0758 - valid_accuracy: 0.7505 - valid_precision: 0.7505 - valid_recall: 0.7505 - valid_f1: 0.7505\n",
      "Epoch 661 / 1000: train_loss: 0.0001 - valid_loss: 1.0753 - valid_accuracy: 0.7511 - valid_precision: 0.7511 - valid_recall: 0.7511 - valid_f1: 0.7511\n",
      "Epoch 671 / 1000: train_loss: 0.0001 - valid_loss: 1.0748 - valid_accuracy: 0.7511 - valid_precision: 0.7511 - valid_recall: 0.7511 - valid_f1: 0.7511\n",
      "Epoch 681 / 1000: train_loss: 0.0001 - valid_loss: 1.0743 - valid_accuracy: 0.7518 - valid_precision: 0.7518 - valid_recall: 0.7518 - valid_f1: 0.7518\n",
      "Epoch 691 / 1000: train_loss: 0.0001 - valid_loss: 1.0738 - valid_accuracy: 0.7524 - valid_precision: 0.7524 - valid_recall: 0.7524 - valid_f1: 0.7524\n",
      "Epoch 701 / 1000: train_loss: 0.0001 - valid_loss: 1.0734 - valid_accuracy: 0.7537 - valid_precision: 0.7537 - valid_recall: 0.7537 - valid_f1: 0.7537\n",
      "Epoch 711 / 1000: train_loss: 0.0001 - valid_loss: 1.0729 - valid_accuracy: 0.7537 - valid_precision: 0.7537 - valid_recall: 0.7537 - valid_f1: 0.7537\n",
      "Epoch 721 / 1000: train_loss: 0.0001 - valid_loss: 1.0725 - valid_accuracy: 0.7537 - valid_precision: 0.7537 - valid_recall: 0.7537 - valid_f1: 0.7537\n",
      "Epoch 731 / 1000: train_loss: 0.0001 - valid_loss: 1.0720 - valid_accuracy: 0.7537 - valid_precision: 0.7537 - valid_recall: 0.7537 - valid_f1: 0.7537\n",
      "Epoch 741 / 1000: train_loss: 0.0001 - valid_loss: 1.0716 - valid_accuracy: 0.7543 - valid_precision: 0.7543 - valid_recall: 0.7543 - valid_f1: 0.7543\n",
      "Epoch 751 / 1000: train_loss: 0.0001 - valid_loss: 1.0712 - valid_accuracy: 0.7543 - valid_precision: 0.7543 - valid_recall: 0.7543 - valid_f1: 0.7543\n",
      "Epoch 761 / 1000: train_loss: 0.0001 - valid_loss: 1.0708 - valid_accuracy: 0.7543 - valid_precision: 0.7543 - valid_recall: 0.7543 - valid_f1: 0.7543\n",
      "Epoch 771 / 1000: train_loss: 0.0001 - valid_loss: 1.0703 - valid_accuracy: 0.7556 - valid_precision: 0.7556 - valid_recall: 0.7556 - valid_f1: 0.7556\n",
      "Epoch 781 / 1000: train_loss: 0.0001 - valid_loss: 1.0699 - valid_accuracy: 0.7556 - valid_precision: 0.7556 - valid_recall: 0.7556 - valid_f1: 0.7556\n",
      "Epoch 791 / 1000: train_loss: 0.0001 - valid_loss: 1.0695 - valid_accuracy: 0.7556 - valid_precision: 0.7556 - valid_recall: 0.7556 - valid_f1: 0.7556\n",
      "Epoch 801 / 1000: train_loss: 0.0001 - valid_loss: 1.0691 - valid_accuracy: 0.7562 - valid_precision: 0.7562 - valid_recall: 0.7562 - valid_f1: 0.7562\n",
      "Epoch 811 / 1000: train_loss: 0.0001 - valid_loss: 1.0688 - valid_accuracy: 0.7569 - valid_precision: 0.7569 - valid_recall: 0.7569 - valid_f1: 0.7569\n",
      "Epoch 821 / 1000: train_loss: 0.0001 - valid_loss: 1.0684 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 831 / 1000: train_loss: 0.0001 - valid_loss: 1.0680 - valid_accuracy: 0.7569 - valid_precision: 0.7569 - valid_recall: 0.7569 - valid_f1: 0.7569\n",
      "Epoch 841 / 1000: train_loss: 0.0001 - valid_loss: 1.0676 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 851 / 1000: train_loss: 0.0001 - valid_loss: 1.0673 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 861 / 1000: train_loss: 0.0001 - valid_loss: 1.0669 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 871 / 1000: train_loss: 0.0001 - valid_loss: 1.0666 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 881 / 1000: train_loss: 0.0001 - valid_loss: 1.0662 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 891 / 1000: train_loss: 0.0001 - valid_loss: 1.0659 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 901 / 1000: train_loss: 0.0001 - valid_loss: 1.0655 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 911 / 1000: train_loss: 0.0001 - valid_loss: 1.0652 - valid_accuracy: 0.7575 - valid_precision: 0.7575 - valid_recall: 0.7575 - valid_f1: 0.7575\n",
      "Epoch 921 / 1000: train_loss: 0.0001 - valid_loss: 1.0649 - valid_accuracy: 0.7582 - valid_precision: 0.7582 - valid_recall: 0.7582 - valid_f1: 0.7582\n",
      "Epoch 931 / 1000: train_loss: 0.0001 - valid_loss: 1.0645 - valid_accuracy: 0.7588 - valid_precision: 0.7588 - valid_recall: 0.7588 - valid_f1: 0.7588\n",
      "Epoch 941 / 1000: train_loss: 0.0001 - valid_loss: 1.0642 - valid_accuracy: 0.7594 - valid_precision: 0.7594 - valid_recall: 0.7594 - valid_f1: 0.7594\n",
      "Epoch 951 / 1000: train_loss: 0.0001 - valid_loss: 1.0639 - valid_accuracy: 0.7601 - valid_precision: 0.7601 - valid_recall: 0.7601 - valid_f1: 0.7601\n",
      "Epoch 961 / 1000: train_loss: 0.0001 - valid_loss: 1.0636 - valid_accuracy: 0.7601 - valid_precision: 0.7601 - valid_recall: 0.7601 - valid_f1: 0.7601\n",
      "Epoch 971 / 1000: train_loss: 0.0001 - valid_loss: 1.0633 - valid_accuracy: 0.7601 - valid_precision: 0.7601 - valid_recall: 0.7601 - valid_f1: 0.7601\n",
      "Epoch 981 / 1000: train_loss: 0.0001 - valid_loss: 1.0630 - valid_accuracy: 0.7601 - valid_precision: 0.7601 - valid_recall: 0.7601 - valid_f1: 0.7601\n",
      "Epoch 991 / 1000: train_loss: 0.0001 - valid_loss: 1.0627 - valid_accuracy: 0.7607 - valid_precision: 0.7607 - valid_recall: 0.7607 - valid_f1: 0.7607\n",
      "Epoch 1000 / 1000: train_loss: 0.0001 - valid_loss: 1.0624 - valid_accuracy: 0.7607 - valid_precision: 0.7607 - valid_recall: 0.7607 - valid_f1: 0.7607\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6666)\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 100\n",
    "lr = 1e-2\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "print_every = 10\n",
    "\n",
    "model_mbgd = MLP(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "mbgd_train_history, mbgd_valid_history = train_mbgd(model_mbgd, x_train, y_train, x_valid, y_valid, lr, num_epoch, batch_size, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0925d",
   "metadata": {},
   "source": [
    "### 2.5 MLP using Tensorflow (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Softmax\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "\n",
    "class MLPTF(Model):\n",
    "    def __init__(self, feature_dim: int, hidden_dim: int, num_class: int, lambda_: float):\n",
    "        \"\"\" MLP Model using tensorflow.keras\n",
    "        Args:\n",
    "            feature_dim: feature dimension\n",
    "            hidden_dim: hidden units\n",
    "            num_class: number of class\n",
    "            lambda_: lambda in L2 regularizer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here (initialize weight and bias)\n",
    "        self.dense1 = Dense(hidden_dim, activation='sigmoid', kernel_regularizer=regularizers.l2(lambda_))\n",
    "        self.dense2 = Dense(num_class, activation='sigmoid', kernel_regularizer=regularizers.l2(lambda_))\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        # End\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\" Forward function of tf. It should be named 'call'\n",
    "        \n",
    "        Args:\n",
    "            x: (n x feature_dim) tensor\n",
    "        Returns:\n",
    "            y_hat: (n x num_class) tensor\n",
    "        \"\"\"\n",
    "        # Start your code here (Forward)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        y_hat = self.softmax(x)\n",
    "        # End\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "np.random.seed(6666)\n",
    "tf.random.set_seed(6666)\n",
    "\n",
    "\n",
    "hidden_dim = 128\n",
    "num_epoch = 1000\n",
    "lr = 1e-1\n",
    "batch_size = 128\n",
    "lambda_ = 1e-8\n",
    "\n",
    "model_tf = MLPTF(feature_dim=vocab_size, hidden_dim=hidden_dim, num_class=num_class, lambda_=lambda_)\n",
    "model_tf.build(input_shape=(None, vocab_size))\n",
    "model_tf.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tfa.metrics.F1Score(num_classes=num_class, average='micro')])\n",
    "model_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556bbb6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_history = model_tf.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=batch_size, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2a285",
   "metadata": {},
   "source": [
    "#### Evaluation with Tensroflow\n",
    "You are required to report the loss, accuracy, precision, recall, and f1 on test set and plot the the curve of them for both SGD and Mini-batch GD on train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2668a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics for test set and fill in the table below\n",
    "y_hat = model_mbgd.forward(x_test,False)\n",
    "y_pred = model_mbgd.predict(y_hat)\n",
    "print('Mini-batch GD:', get_metrics(y_pred, y_test))\n",
    "model_tf.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40725ae0",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics on Test set\n",
    "Fill this table with the result you just printed (double click this cell to edit)\n",
    "|     Optimizer                     | Accuracy    | F1 Score    |\n",
    "|:---------------------------------:|-------------|-------------|\n",
    "|      **Your Implementation**      |     0.675753        | 0.675753            |\n",
    "| **Tensorflow**                    |    0.9722         |    0.554         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cb8e7",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the training loss curve for Your implementation and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.plot(mbgd_train_history['loss'], label='My MLP')\n",
    "plt.plot(tf_history.history['loss'], label='TF MLP')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390fc13c",
   "metadata": {},
   "source": [
    "##### Please run the following cell to plot the validation metrics curve for SGD and Mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax = axes[0]\n",
    "ax.plot(mbgd_valid_history['accuracy'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_categorical_accuracy'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation Accuracy')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(mbgd_valid_history['f1'], label='My MLP')\n",
    "ax.plot(tf_history.history['val_f1_score'], label='TF MLP')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Validation F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff50430",
   "metadata": {},
   "source": [
    "# 3. Conclusion (5 Points)\n",
    "\n",
    "Provide an analysis for all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da15ce6b",
   "metadata": {},
   "source": [
    "Answer:My implementation achieved an accuracy and F1 score of approximately 0.676. These metrics suggest that the model is performing reasonably well, with a good balance between precision and recall.\n",
    "The TensorFlow implementation achieved a significantly higher accuracy (0.9722) but a lower F1 score (0.554). The high accuracy indicates that the model is making correct predictions for a large portion of the dataset. However, the lower F1 score suggests a potential imbalance between precision and recall, indicating that the model may struggle with correctly classifying certain classes.\n",
    "\n",
    "Perfect Fit: It's possible that your model has memorized the training data and achieved a perfect fit, resulting in zero training loss. This could indicate overfitting, especially if the validation performance is not improving.\n",
    "\n",
    "The increasing trends in both accuracy and F1 score suggest that my model is learning and improving its performance on the validation set over time. This is a positive sign, indicating that your model is continuously learning from the data and adapting to the task.\n",
    "The constant trends in both accuracy and F1 score for the TensorFlow implementation might suggest that the model may have reached a plateau in terms of learning from the validation set. It could be that the model has already learned most of the patterns in the data, and further training iterations do not significantly impact its performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69316163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
