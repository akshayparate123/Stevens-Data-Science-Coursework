{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a4b99b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# If you are going to use GPU, make sure the GPU in in the output\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b5892505-7548-4ac6-8510-7a4fcda72f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watching time chasers it obvious that it was made by a bunch of friends', 'maybe they were sitting around one day in film school and said hey let s pool our money together and make a really bad movie or something like that', 'what ever they said they still ended up making a really bad movie dull story bad script lame acting poor cinematography bottom of the barrel stock music etc', 'all corners were cut except the one that would have prevented this film s release', 'life s like that', 'i saw this film about years ago and remember it as being particularly nasty', 'i believe it is based on a true incident a young man breaks into a nurses home and rapes tortures and kills various women it is in black and white but saves the colour for one shocking shot at the end the film seems to be trying to make some political statement but it just comes across as confused and obscene avoid', 'minor spoilersin new york joan barnard elvire audrey is informed that her husband the archeologist arthur barnard john saxon was mysteriously murdered in italy while searching an etruscan tomb', 'joan decides to travel to italy in the company of her colleague who offers his support', 'once in italy she starts having visions relative to an ancient people and maggots many maggots']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "# In this imdb review, each line is a sentence seperated by a space.\n",
    "sentences = pickle.load(open(os.path.join('./Archive/Archive/a2-data', 'imdb_review.pickle'), 'rb'))\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2061ec-5fae-4d3b-a501-b33640ff3ff3",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529b505-9f71-4a12-b8c6-b50fe0372ad7",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "902e3c57-83da-4f55-be83-423569410edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "class Tokenization:\n",
    "    def __init__(self):\n",
    "        self.token_occurences = {\"sos\":0,\"eos\":0,\"unk\":0}\n",
    "        self.token_map = {\"sos\":0,\"eos\":1,\"unk\":2}\n",
    "        self.index_map = {0:\"sos\",1:\"eos\",2:\"unk\"}\n",
    "    def fit(self,data):\n",
    "        for sentence in data:\n",
    "            tokens = sentence.split(\" \")\n",
    "            for token in tokens:\n",
    "                if token in self.token_occurences:\n",
    "                    self.token_occurences[token] = self.token_occurences[token]+1\n",
    "                else:\n",
    "                    self.token_occurences[token] = 1\n",
    "        sorted_token_occurences = dict( sorted(self.token_occurences.items(), key=operator.itemgetter(1), reverse=True))\n",
    "        idx = 0\n",
    "        for key,value in zip(sorted_token_occurences.keys(),sorted_token_occurences.values()):\n",
    "            if key == \"sos\" or key == \"eos\" or key == \"unk\":\n",
    "                continue\n",
    "            self.token_map[key] = idx + 3\n",
    "            self.index_map[idx+3] = key\n",
    "            idx = idx+1\n",
    "    def encode(self,data):\n",
    "        sent_token_ids = []\n",
    "        for sentence in data:\n",
    "            token_ids = []\n",
    "            tokens = sentence.split(\" \")\n",
    "            for token in tokens:\n",
    "                if token in self.token_map:\n",
    "                    token_ids.append(self.token_map[token])\n",
    "                else:\n",
    "                    token_ids.append(self.token_map[\"unk\"])\n",
    "            if len(token_ids) > 2:\n",
    "                sent_token_ids.append(token_ids)\n",
    "        return sent_token_ids\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "91fa10fd-7328-4ffa-8a46-42b5c489f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization = Tokenization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b8869d4e-291e-4c2c-9fd0-5e95f45f1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization.fit(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d28fa970-b082-437e-9c36-152025365be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization.token_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5e5dff54-fb75-472d-95fd-791310766dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_token_ids = tokenization.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "43b63149-1abd-41e6-aac3-1c477c8c303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102297\n"
     ]
    }
   ],
   "source": [
    "token_ocur = sorted(tokenization.token_occurences.items(), key=lambda e: e[1], reverse=True)\n",
    "print(len(token_ocur))\n",
    "# print(token_ocur[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6bd3cf-05db-4c5a-bfad-46345c0048aa",
   "metadata": {},
   "source": [
    "## Training Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "eb1560a3-da8f-4051-b0e8-270ba6a23bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as math\n",
    "def positive_samples(sent_tokens_ids):\n",
    "    count = 0\n",
    "    positive_pairs = []\n",
    "    for token_ids in sent_token_ids:\n",
    "        center_word,context_words = 0,[]\n",
    "        length = len(token_ids)\n",
    "        # print(token_ids)\n",
    "        for i,tokens in enumerate(token_ids):\n",
    "            # print(tokens)\n",
    "            center_word = tokens\n",
    "            start_idx = max(0,i - 2)\n",
    "            end_idx = min(length,i+2+1)\n",
    "            # print(start_idx,end_idx)\n",
    "            context_words = [token_ids[j] for j in range(start_idx, end_idx) if j != i]\n",
    "            for context in context_words:\n",
    "                positive_pairs.append([center_word,context])\n",
    "            # print(context_words)\n",
    "        # print(positive_pairs)\n",
    "    return positive_pairs,count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e6376fa9-f106-41a1-9042-77cc33f85d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 9814, 70, 590, 540, 3, 31, 13, 61, 29, 9364, 12, 21, 14, 773]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token_ids[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bc12171b-9e35-4d1d-a597-4516ba6ca017",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pairs,count = positive_samples(sent_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c600e5-9919-496d-ade8-393830ee84da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "deff1597-99a9-4cd6-8fe9-3cfe8e552830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_negative_samples(vocab_size: int, batch_size: int, negative_sample_num: int) -> np.ndarray:\n",
    "    \"\"\" Generate negative words\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: number of tokens in the vocabulary\n",
    "        batch_size: number of samples (center word) in a batch\n",
    "        negative_sample_num: number of negative words sampled for a center word\n",
    "        \n",
    "    Return:\n",
    "        negative_words: Shape of (batch_size x negative_sample_num)\n",
    "        \n",
    "    Note: 1. You should NOT sample special token in the vocabulary, i.e., the token ids range should be [5, vocab_size)\n",
    "          2. Hint: See numpy.random.choice. Read carefully for each parameter of this function\n",
    "    \"\"\"\n",
    "    negative_words = None\n",
    "    # Start your code here\n",
    "    negative_words = np.random.choice(np.arange(3, vocab_size), size=(batch_size, negative_sample_num), replace=True)\n",
    "    # End\n",
    "    return negative_words\n",
    "\n",
    "def generate_training_data(positive_pairs,vocab_size,batch_size,negative_sample_num):\n",
    "    while True:\n",
    "        center_batch = [];context_batch = []\n",
    "        for pair in positive_pairs:\n",
    "            center_batch.append(pair[0])\n",
    "            context_batch.append(pair[1])\n",
    "            if(len(center_batch) == batch_size):\n",
    "                negative_words = get_negative_samples(vocab_size, batch_size, negative_sample_num)\n",
    "                yield center_batch,context_batch,negative_words\n",
    "                center_batch = [];context_batch = []\n",
    "        if len(center_batch) < batch_size:\n",
    "            new_batch_size = len(center_batch)\n",
    "            negative_words = get_negative_samples(vocab_size, new_batch_size, negative_sample_num)\n",
    "            yield center_batch,context_batch,negative_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2e0d7503-1ea4-420f-a219-e4a7a2ac23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.layers import Embedding\n",
    "        \n",
    "def negative_sampling_loss(center_embeddings, context_embeddings, negative_embeddings):\n",
    "    \"\"\" Calculate the negative sampling loss\n",
    "    \n",
    "    Args:\n",
    "        center_embeddings: v_c, (batch_size x embedding_dim)\n",
    "        context_embeddings: u_o, (batch_size x embedding_dim)\n",
    "        negative_embeddings: u_k, (batch_size x negative_sample_num x embedding_dim)\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    # Start your code here\n",
    "    # 1. Calculate positive dot product\n",
    "    positive_dot_product = tf.reduce_sum(tf.multiply(center_embeddings, context_embeddings), axis=1)\n",
    "\n",
    "    # 2. loss for the positive pairs\n",
    "    positive_loss = tf.reduce_mean(tf.math.log_sigmoid(positive_dot_product))\n",
    "    \n",
    "    # 3. Calculate negative dot product\n",
    "    negative_dot_product = tf.reduce_sum(tf.multiply(center_embeddings[:, tf.newaxis, :], negative_embeddings), axis=2)\n",
    "    \n",
    "    # 4. loss for the negative words\n",
    "    negative_loss = tf.reduce_mean(tf.math.log_sigmoid(-negative_dot_product))\n",
    "    \n",
    "    # Hint: See tf.reduce_sum, tf.expand_dims, tf.reduce_mean for help\n",
    "    loss = - (positive_loss + negative_loss)\n",
    "    # End\n",
    "    return loss\n",
    "\n",
    "\n",
    "class SkipGram(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\" Skip-gram model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Start your code here\n",
    "        # Initialize embedding layers\n",
    "        self.center_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='center_embedding')\n",
    "        self.context_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='context_embedding')\n",
    "        # Hint: See tf.keras.layers.Embedding\n",
    "\n",
    "        # End\n",
    "        \n",
    "    def call(self, center_words, context_words, negative_words):\n",
    "        \"\"\" Forward of the skip-gram model\n",
    "        \n",
    "        Args:\n",
    "            center_words: tensor (batch_size, )\n",
    "            context_words: tensor (batch_size, )\n",
    "            negative_words: tensor (batch_size, negative_embeddings)\n",
    "            \n",
    "        Return:\n",
    "            center_embeddings, context_embeddings, negative_embeddings: The input for the negative_sampling_loss.\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        center_embeddings = self.center_embeddings(center_words)\n",
    "        context_embeddings = self.context_embeddings(context_words)\n",
    "        negative_embeddings = self.context_embeddings(negative_words)\n",
    "        # End\n",
    "        \n",
    "        return center_embeddings, context_embeddings, negative_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "46ff68b1-e122-4976-9d1f-986afd45bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = int(np.ceil(len(positive_pairs) / batch_size))\n",
    "embedding_dim = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "0834c72f-8bbd-4967-bb99-cb774031fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size, embedding_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bbebdda8-0550-4327-84eb-b36fe174e31d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[211], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m batch \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(d, tf\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape: \u001b[38;5;66;03m#operations performed within this context are recorded as tape\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m negative_sampling_loss(\u001b[38;5;241m*\u001b[39moutput)\n\u001b[0;32m     12\u001b[0m trainable_vars \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;66;03m#variables who is going to get optimized (weights and biases)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenization.token_occurences)\n",
    "batch_size = 1024\n",
    "negative_sample_num = 5\n",
    "train_gen = generate_training_data(positive_pairs,vocab_size,batch_size,negative_sample_num)\n",
    "for i in range(0,1):\n",
    "    for j in range(0,n_batch):\n",
    "        batch = next(train_gen)\n",
    "        batch = [tf.convert_to_tensor(d, tf.int64) for d in batch]\n",
    "        with tf.GradientTape() as tape: #operations performed within this context are recorded as tape\n",
    "            output = model(*batch)\n",
    "            loss = negative_sampling_loss(*output)\n",
    "        trainable_vars = model.trainable_variables #variables who is going to get optimized (weights and biases)\n",
    "        gradients = tape.gradient(loss, trainable_vars) #calculate the gradients\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        epoch_loss += loss * real_batch_size\n",
    "        trainable_vars = model.trainable_variables #variables who is going to get optimized (weights and biases)\n",
    "        gradients = tape.gradient(loss, trainable_vars) #calculate the gradients\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        epoch_loss += loss * len(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
