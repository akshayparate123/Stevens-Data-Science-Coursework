{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba6cfda",
   "metadata": {
    "id": "8ba6cfda"
   },
   "source": [
    "# CS 584 Assignment 4 -- Sequence to Sequence Models\n",
    "\n",
    "#### Name: Akshay Parate\n",
    "#### Stevens ID: 20032008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "u-3mMwGSEE7K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-3mMwGSEE7K",
    "outputId": "78275ed1-9d7a-4635-ce27-f2811dc52f69"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZsvuIwI-EFnt",
   "metadata": {
    "id": "ZsvuIwI-EFnt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b106027",
   "metadata": {
    "id": "6b106027"
   },
   "source": [
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the seq2seq (translation) model.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951ed7ae-2ed9-44a8-b253-e06d4a567c66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "951ed7ae-2ed9-44a8-b253-e06d4a567c66",
    "outputId": "0a72cad0-af35-478c-8465-fc8282f0d19c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9a9bad",
   "metadata": {
    "id": "9f9a9bad"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    print('\\r' + str_, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ffd6958",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ffd6958",
    "outputId": "a5c62f06-2923-4973-87ca-5dc937f62def"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# If you are going to use GPU, make sure the GPU in in the output\n",
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d226bd",
   "metadata": {
    "id": "63d226bd"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union, Dict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441930a",
   "metadata": {
    "id": "9441930a"
   },
   "source": [
    "## 1. Data preparation (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745adba",
   "metadata": {
    "id": "0745adba"
   },
   "source": [
    "### 1.1 Load and describe data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb159ef8",
   "metadata": {
    "id": "eb159ef8"
   },
   "source": [
    "Here, we use the [iwslt2017](https://huggingface.co/datasets/iwslt2017) dataset. More specifically, this translation task is from French to English: fr-en."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2326b71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2326b71",
    "outputId": "01460f68-19d2-4a45-e25a-5439113d228e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\datasets\\load.py:2524: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# The load_dataset function is provided by the huggingface datasets\n",
    "# https://huggingface.co/docs/datasets/index\n",
    "\n",
    "\n",
    "dataset_path = os.path.join('a4-data', 'dataset')\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-fr', cache_dir=dataset_path, ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da78bb3",
   "metadata": {
    "id": "4da78bb3"
   },
   "source": [
    "Let's first print some basic statistics of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f96c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0f96c96",
    "outputId": "19a47d2b-6d6e-4303-81c2-c212c7778974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 232825\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 8597\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 890\n",
      "    })\n",
      "})\n",
      "232825 890 8597\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(len(dataset['train']['translation']), len(dataset['validation']['translation']), len(dataset['test']['translation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3r_Y0ruDVUSL",
   "metadata": {
    "id": "3r_Y0ruDVUSL"
   },
   "outputs": [],
   "source": [
    "# dataset['train']['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65f0a1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e65f0a1d",
    "outputId": "109d9bf6-16cf-4441-eb3a-f7d2d1528c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'fr': \"Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['translation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1471b654",
   "metadata": {
    "id": "1471b654"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "# The tokenizer is provided by the huggingface tokenizers\n",
    "# https://huggingface.co/docs/tokenizers/index\n",
    "# Here, I already pretrained a BPE tokenizer and you can simply load the json\n",
    "# The token numbers of both English and French are 10,000\n",
    "# All tokens should be lower-case.\n",
    "# en_tokenizer = Tokenizer.from_file('./drive/MyDrive/a4-data/en_tokenizer.json')\n",
    "# fr_tokenizer = Tokenizer.from_file('./drive/MyDrive/a4-data/fr_tokenizer.json')\n",
    "en_tokenizer = Tokenizer.from_file('./a4-data/en_tokenizer.json')\n",
    "fr_tokenizer = Tokenizer.from_file('./a4-data/fr_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ade73b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ade73b8",
    "outputId": "41340c6f-cd3d-475b-fb31-6d19fc9416bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 122, 279, 4987, 17, 1]\n",
      "['<s>', 'Ġi', 'Ġlike', 'Ġsports', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "encoding = en_tokenizer.encode(\"i like sports.\")\n",
    "print(encoding.ids)\n",
    "print(encoding.tokens)\n",
    "# >>> [0, 122, 279, 4987, 17, 1]\n",
    "# >>> ['<s>', 'Ġi', 'Ġlike', 'Ġsports', '.', '</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd9b4c",
   "metadata": {
    "id": "80dd9b4c"
   },
   "source": [
    "Extract English and French sentences for training, validation, and test sets.\n",
    "\n",
    "Note: Every sentence is lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1608b13",
   "metadata": {
    "id": "a1608b13"
   },
   "outputs": [],
   "source": [
    "train_en_sentences, train_fr_sentences = zip(*[(pair['en'].lower(), pair['fr'].lower()) for pair in dataset['train']['translation']])\n",
    "valid_en_sentences, valid_fr_sentences = zip(*[(pair['en'].lower(), pair['fr'].lower()) for pair in dataset['validation']['translation']])\n",
    "test_en_sentences, test_fr_sentences = zip(*[(pair['en'].lower(), pair['fr'].lower()) for pair in dataset['test']['translation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Jw5Qk7fcV8_K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Jw5Qk7fcV8_K",
    "outputId": "a320427f-b924-4298-ed90-18aeb7e56b9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'several years ago here at ted, peter skillman  introduced a design challenge  called the marshmallow challenge.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "NxJsfIMjWJM1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "NxJsfIMjWJM1",
    "outputId": "ba454fde-71e4-4ee0-9d0c-26bace2782b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"il y a plusieurs années, ici à ted, peter skillman a présenté une épreuve de conception appelée l'épreuve du marshmallow.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fr_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486e49b",
   "metadata": {
    "id": "4486e49b"
   },
   "source": [
    "### 1.2 Encode data (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ccb3e5d",
   "metadata": {
    "id": "9ccb3e5d"
   },
   "outputs": [],
   "source": [
    "def encode(tokenizer: 'Tokenizer', sentences: List[str]) -> List[List[int]]:\n",
    "    \"\"\" Encode the sentences with the pretrained tokenizer.\n",
    "        You can directly call `tokenizer.encode()` to encode the sentences.\n",
    "        It will automatically add the <s> and </s> token.\n",
    "\n",
    "        Note: Please be carefull with the return value of the encode function.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A pretrained en/fr tokenizer\n",
    "        sentences: A list of strings\n",
    "    Return:\n",
    "        sent_token_ids: A list of token ids\n",
    "    \"\"\"\n",
    "    sent_token_ids = []\n",
    "    n = len(sentences)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i % 100 == 0 or i == n - 1:\n",
    "            print_line('Encoding with Tokenizer:', (i + 1), '/', n)\n",
    "        # Start your code here\n",
    "        sent_token_ids.append(tokenizer.encode(sentence).ids)\n",
    "        # End\n",
    "    print_line('\\n')\n",
    "    return sent_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771ab620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "771ab620",
    "outputId": "8987015d-70e8-4fdf-9de7-a2ff48228a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Encoding with Tokenizer: 232825 / 232825\n",
      "Encoding with Tokenizer: 890 / 890\n",
      "Encoding with Tokenizer: 8597 / 8597\n",
      "fr\n",
      "Encoding with Tokenizer: 232825 / 232825\n",
      "Encoding with Tokenizer: 890 / 890\n",
      "Encoding with Tokenizer: 8597 / 8597\n"
     ]
    }
   ],
   "source": [
    "print('en')\n",
    "train_en = encode(en_tokenizer, train_en_sentences)\n",
    "valid_en = encode(en_tokenizer, valid_en_sentences)\n",
    "test_en = encode(en_tokenizer, test_en_sentences)\n",
    "print('fr')\n",
    "train_fr = encode(fr_tokenizer, train_fr_sentences)\n",
    "valid_fr = encode(fr_tokenizer, valid_fr_sentences)\n",
    "test_fr = encode(fr_tokenizer, test_fr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mqNpGXj-RvbX",
   "metadata": {
    "id": "mqNpGXj-RvbX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e910335c",
   "metadata": {
    "id": "e910335c"
   },
   "source": [
    "Check your implementation with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "763287e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "763287e4",
    "outputId": "ac01eea5-e544-4841-f800-526fd96db44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'fr': \"Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\"}\n",
      "[0, 658, 162, 188, 494, 15, 2843, 17, 138, 165, 178, 2775, 121, 630, 4502, 140, 222, 124, 1930, 140, 625, 140, 185, 2122, 3446, 30, 122, 400, 2576, 5818, 17, 1] [0, 763, 478, 15, 3016, 17, 145, 10, 178, 487, 169, 8981, 152, 1038, 2055, 266, 323, 2425, 220, 1760, 586, 17, 214, 459, 378, 9952, 17, 1]\n",
      " thank you so much, chris. and it's truly a great honor to have the opportunity to come to this stage twice; i'm extremely grateful.  merci beaucoup, chris. c'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. je suis très reconnaissant.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['translation'][0])\n",
    "print(train_en[0], train_fr[0])\n",
    "print(en_tokenizer.decode(train_en[0]), fr_tokenizer.decode(train_fr[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4bd4aa",
   "metadata": {
    "id": "8e4bd4aa"
   },
   "source": [
    "## 2. Sequence to sequence model (40 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9202d2",
   "metadata": {
    "id": "2f9202d2"
   },
   "source": [
    "### 2.1 Encoder (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db70c22",
   "metadata": {
    "id": "9db70c22"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, GRU, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, units: int):\n",
    "        \"\"\" The encoder model for the src sentences.\n",
    "            It contains an embedding part and a GRU part.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: The src vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here\n",
    "        # Note: Please know what the decoder needs from encoder. This determines the parameters of the GRU layer\n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.gru = GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        # End\n",
    "\n",
    "    def call(self, src_ids, src_mask):\n",
    "        \"\"\" Encoder forward\n",
    "        Args:\n",
    "            src_ids: Tensor, (batch_size x max_len), the token ids of input sentences in a batch\n",
    "            src_mask: Tensor, (batch_size x max_len), the mask of the src input. True value in the mask means this timestep is valid, otherwise this timestep is ignored\n",
    "        Returns:\n",
    "            enc_output: Tensor, (batch_size x max_len x units), the output of GRU for all timesteps\n",
    "            final_state: Tensor, (batch_size x units), the state of the final valid timestep\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1. Retrieve embedding\n",
    "        #      2. GRU\n",
    "        # Please refer to the calling arguments of GRU: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#call-arguments\n",
    "        # print(src_ids)\n",
    "        # print(src_ids[0])\n",
    "        # print()\n",
    "        embedded = self.embedding(src_ids)\n",
    "        # print(embedded)\n",
    "        enc_outputs, final_state = self.gru(embedded, mask=src_mask)\n",
    "        # End\n",
    "        return enc_outputs, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312246f",
   "metadata": {
    "id": "a312246f"
   },
   "source": [
    "### 2.2 Decoder (15 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "607e79c2",
   "metadata": {
    "id": "607e79c2"
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, units: int, dropout_rate: float):\n",
    "        \"\"\" The decoder model for the tgt sentences.\n",
    "            It contains an embedding part, a GRU part, a dropout part, and a classifier part.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: The tgt vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "            dropout_rate: The classifier has a (units x vocab_size) weight. This is a large weight matrix. We apply a dropout layer to avoid overfitting.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here\n",
    "        # Note: 1. Please correctly set the parameter of GRU\n",
    "        #       2. No softmax here because we will need the sequence to sequence loss later\n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.gru = GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.classifier = Dense(vocab_size)\n",
    "        # End\n",
    "\n",
    "    def call(self, tgt_ids, initial_state, tgt_mask):\n",
    "        \"\"\" Decoder forward.\n",
    "            It is called by decoder(tgt_ids=..., initial_state=..., tgt_mask=...)\n",
    "\n",
    "        Args:\n",
    "            tgt_ids: Tensor, (batch_size x max_len), the token ids of input sentences in a batch\n",
    "            initial_state: Tensor, (batch_size x units), the state of the final valid timestep from the encoder\n",
    "            tgt_mask: Tensor, (batch_size x max_len), the mask of the tgt input. True value in the mask means this timestep is valid, otherwise this timestep is ignored\n",
    "        Return:\n",
    "            dec_outputs: Tensor, (batch_size x max_len x vocab_size), the output of GRU for all timesteps\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1. Retrieve embedding\n",
    "        #      2. GRU\n",
    "        #      3. Apply dropout to the GRU output\n",
    "        #      4. Classifier\n",
    "        # Note: Please refer to the calling arguments of GRU: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#call-arguments\n",
    "        embedded = self.embedding(tgt_ids)\n",
    "        dec_output, final_state = self.gru(embedded, initial_state=initial_state, mask=tgt_mask)\n",
    "        dec_outputs = self.dropout(dec_output)\n",
    "        dec_outputs = self.classifier(dec_outputs)\n",
    "        # End\n",
    "        return dec_outputs\n",
    "\n",
    "    def predict(self, tgt_ids, initial_state):\n",
    "        \"\"\" Decoder prediction.\n",
    "            This is a step in recursive prediction. We use the previous prediction and state to predict current token.\n",
    "            Note that we only need to use the gru_cell instead of GRU becasue we only need to calculate one timestep.\n",
    "\n",
    "        Args:\n",
    "            tgt_ids: Tensor, (batch_size, ) -> (1, ), the token id of the current timestep in the current sentence.\n",
    "            initial_state: Tensor, (batch_size x units) -> (1 x units), the state of the final valid timestep from the encoder or the previous hidden state in prediction.\n",
    "        Return:\n",
    "            dec_outputs: Tensor, (batch_size x vocab_size) -> (1 x vocab_size), the output of GRU for this timestep.\n",
    "            state: Tensor, (batch_size x units) -> (1 x units), the state of this timestep.\n",
    "        \"\"\"\n",
    "        gru_cell = self.gru.cell\n",
    "        # Start your code here\n",
    "        # Step 1. Retrieve embedding\n",
    "        #      2. GRU Cell, see https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell#call-arguments\n",
    "        #      3. Classifier (No dropout)\n",
    "        embedded = self.embedding(tgt_ids)\n",
    "        dec_output, state = self.gru(embedded, initial_state=initial_state)\n",
    "        dec_outputs = self.classifier(dec_output)\n",
    "        # End\n",
    "        return dec_outputs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2570e4c",
   "metadata": {
    "id": "f2570e4c"
   },
   "source": [
    "### 2.3 Seq2seq (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7be2a3c",
   "metadata": {
    "id": "c7be2a3c"
   },
   "outputs": [],
   "source": [
    "class Seq2seq(Model):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, embedding_size: int, units: int, dropout_rate: float):\n",
    "        \"\"\" The sequence to sequence model.\n",
    "            It contains an encoder and a decoder.\n",
    "\n",
    "        Args:\n",
    "            src_vocab_size: The src vocabulary size\n",
    "            tgt_vocab_size: The tgt vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "            dropout_rate: The dropout rate used in the decoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here\n",
    "        self.encoder = Encoder(src_vocab_size, embedding_size, units)\n",
    "        self.decoder = Decoder(tgt_vocab_size, embedding_size, units, dropout_rate)\n",
    "        # End\n",
    "\n",
    "    def call(self, src_ids, src_seq_lens, tgt_ids, tgt_seq_lens):\n",
    "        \"\"\" Seq2seq forward (for the loss calculation in training/validation only).\n",
    "            It is called by model(src_ids=..., src_seq_lens=..., tgt_ids=..., tgt_seq_lens=)\n",
    "            Note: In prediction, we will also need to set `training=False`.\n",
    "\n",
    "        Args:\n",
    "            src_ids: Tensor, (batch_size x max_len), the token ids of src sentences in a batch\n",
    "            src_seq_lens: Tensor, (batch_size, ), the length of src sentences in a batch\n",
    "            tgt_ids: Tensor, (batch_size x max_len), the token ids of tgt sentences in a batch\n",
    "            tgt_seq_lens: Tensor, (batch_size, ), the length of src sentences in a batch\n",
    "        Returns:\n",
    "            dec_outputs: Tensor, (batch_size x max_len x units), the decoder predictions\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1. build mask for src and tgt\n",
    "        #      2. encoder forward\n",
    "        #      3. decoder forward\n",
    "        src_mask = tf.sequence_mask(src_seq_lens)\n",
    "        tgt_mask = tf.sequence_mask(tgt_seq_lens)\n",
    "        # print(src_ids[0])\n",
    "        # print()\n",
    "        enc_output, enc_state = self.encoder(src_ids, src_mask)\n",
    "        dec_outputs = self.decoder(tgt_ids, enc_state, tgt_mask)\n",
    "        # End\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8314e49-8580-4389-85e7-33ef7b050028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0de39f",
   "metadata": {
    "id": "0c0de39f"
   },
   "source": [
    "### 2.4 Seq2seq loss (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56185972",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56185972",
    "outputId": "0cf08112-da88-4b40-a6be-e5b29fea48f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.seq2seq import sequence_loss\n",
    "\n",
    "\n",
    "def seq2seq_loss(logits, target, seq_lens):\n",
    "    \"\"\" Calculate the sequence to sequence loss using the sequence_loss from tensorflow\n",
    "\n",
    "    Args:\n",
    "        logits: Tensor (batch_size x max_seq_len x vocab_size). The output of the RNN model.\n",
    "        target: Tensor (batch_size x max_seq_len). The groud-truth of words.\n",
    "        seq_lens: Tensor (batch_size, ). The real sequence length before padding.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    # Start your code here\n",
    "    # 1. make a sequence mask (batch_size x max_seq_len) using tf.sequence_mask. This is to build a mask with 1 and 0.\n",
    "    mask = tf.sequence_mask(seq_lens, maxlen=tf.shape(target)[1], dtype=tf.float32)\n",
    "    #    Entry with 1 is the valid time step without padding. Entry with 0 is the time step with padding. We need to exclude this time step.\n",
    "    # 2. calculate the loss with sequence_loss. Carefully read the documentation of each parameter\n",
    "    loss = sequence_loss(logits, target,mask,average_across_timesteps = True,average_across_batch = True)\n",
    "    # End\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343fbf8",
   "metadata": {
    "id": "8343fbf8"
   },
   "source": [
    "## 3. Training (50 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce39702",
   "metadata": {
    "id": "bce39702"
   },
   "source": [
    "### 3.1 Pad batch (15 Points)\n",
    "\n",
    "`pad_src_batch`: 5 Points\n",
    "`pad_tgt_batch`: 10 Points\n",
    "\n",
    "Pad the batch to the equal length and make tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be42f754",
   "metadata": {
    "id": "be42f754"
   },
   "outputs": [],
   "source": [
    "def pad_src_batch(src_batch: List[List[int]], src_seq_lens: List[int], pad_val: int):\n",
    "    \"\"\" Pad the batch for src sentences.\n",
    "        Note: Do not use append/extend that can modify the input inplace.\n",
    "\n",
    "    Args:\n",
    "        src_batch: A list of src token ids\n",
    "        src_seq_lens: A list of src lens\n",
    "        pad_val: The padding value\n",
    "\n",
    "    Returns:\n",
    "        src_batch: Tensor, (batch_size x max_len)\n",
    "        src_seq_lens_batch: Tensor, (batch_size, )\n",
    "    \"\"\"\n",
    "    max_src_len = max(src_seq_lens)\n",
    "    # Start your code here\n",
    "    num_sent = len(src_seq_lens)\n",
    "    # Please refer to tf.convert_to_tensor. The dtype should be tf.int64\n",
    "    # Padding\n",
    "    src_batch_padded = [sentence + [pad_val] * (max_src_len - len(sentence)) for sentence in src_batch]\n",
    "    # Convert to tensor\n",
    "    src_batch = tf.convert_to_tensor(src_batch_padded, dtype=tf.int64)\n",
    "    src_seq_lens_batch = tf.convert_to_tensor(src_seq_lens, dtype=tf.int64)\n",
    "    # End\n",
    "    return src_batch, src_seq_lens_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dfa26ac",
   "metadata": {
    "id": "9dfa26ac"
   },
   "outputs": [],
   "source": [
    "def pad_tgt_batch(tgt_batch: List[List[int]], tgt_seq_lens: List[int], pad_val: int):\n",
    "    \"\"\" Pad the batch for tgt sentences.\n",
    "        Note: 1. Do not use append/extend that can modify the input inplace.\n",
    "              2. We need to build the x (feature) and y (label) for tgt sentences.\n",
    "                 Please understand what the feature and label are in translation.\n",
    "\n",
    "    Args:\n",
    "        tgt_batch: A list of src token ids\n",
    "        tgt_seq_lens: A list of src lens\n",
    "        pad_val: The padding value\n",
    "\n",
    "    Returns:\n",
    "        tgt_x_batch: Tensor, (batch_size x max_len)\n",
    "        tgt_y_batch: Tensor, (batch_size x max_len)\n",
    "        src_seq_lens_batch: Tensor, (batch_size, )\n",
    "    \"\"\"\n",
    "    tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch = [], [], []\n",
    "    for sent, seq_len in zip(tgt_batch, tgt_seq_lens):\n",
    "        # Start your code here\n",
    "        # Append x, y, and seq_len\n",
    "        tgt_x_batch.append(sent[:-1])  # Exclude last token for y\n",
    "        tgt_y_batch.append(sent[1:])   # Exclude first token for x\n",
    "        tgt_seq_lens_batch.append(seq_len - 1)  # Reduce sequence length by 1 for y\n",
    "        # End\n",
    "\n",
    "    max_tgt_len = max(tgt_seq_lens_batch)\n",
    "    # Start your code here\n",
    "    # Please refer to tf.convert_to_tensor. The dtype should be tf.int64\n",
    "    # Padding\n",
    "    tgt_x_batch = [sentence + [pad_val] * (max_tgt_len - len(sentence)) for sentence in tgt_x_batch]\n",
    "    tgt_y_batch = [sentence + [pad_val] * (max_tgt_len - len(sentence)) for sentence in tgt_y_batch]\n",
    "\n",
    "    # Convert to tensor\n",
    "    tgt_x_batch = tf.convert_to_tensor(tgt_x_batch, dtype=tf.int64)\n",
    "    tgt_y_batch = tf.convert_to_tensor(tgt_y_batch, dtype=tf.int64)\n",
    "    tgt_seq_lens_batch = tf.convert_to_tensor(tgt_seq_lens_batch, dtype=tf.int64)\n",
    "    # End\n",
    "    return tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bdfff87",
   "metadata": {
    "id": "7bdfff87"
   },
   "outputs": [],
   "source": [
    "def pad_batch(src_batch: List[List[int]], src_seq_lens: List[int], tgt_batch: List[List[int]], tgt_seq_lens: List[int], pad_val: int):\n",
    "    src_batch, src_seq_lens_batch = pad_src_batch(src_batch, src_seq_lens, pad_val)\n",
    "    tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch = pad_tgt_batch(tgt_batch, tgt_seq_lens, pad_val)\n",
    "    return src_batch, src_seq_lens_batch, tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83129bde",
   "metadata": {
    "id": "83129bde"
   },
   "source": [
    "### 3.2 Batch Index Sampler (10 Points)\n",
    "\n",
    "Create a index sampler to sample data index for each batch.\n",
    "\n",
    "This is to make the sentences in each batch have similar lengths to speed up training.\n",
    "\n",
    "Example:\n",
    "```\n",
    "Assume the sentence lengths are: [5, 2, 3, 6, 2, 3, 6] and batch_size is 2.\n",
    "We can make the indices in the batches as follows:\n",
    "[1, 4] of length 2\n",
    "[2, 5] of length 3\n",
    "[0, 3] of lengths 5 and 6\n",
    "[6] of length 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7954420",
   "metadata": {
    "id": "d7954420"
   },
   "outputs": [],
   "source": [
    "class SeqLenBatchSampler:\n",
    "    def __init__(self, seq_lens: List[int], batch_size: int, seed: int = 6666):\n",
    "        \"\"\" The index sampler.\n",
    "            It can be used with iteration:\n",
    "            ```\n",
    "            n_batch = len(sampler)\n",
    "            for indices in sampler:\n",
    "                ...\n",
    "            ```\n",
    "\n",
    "            Args:\n",
    "                seq_lens: A list training sequence lengths (src)\n",
    "                batch_size: .\n",
    "                seed: .\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.seq_lens = seq_lens\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = self._make_batch_index()\n",
    "\n",
    "        self.n_batch = len(self.batches)\n",
    "        self.counter = -1\n",
    "\n",
    "    def _make_batch_index(self) -> List[List[int]]:\n",
    "        \"\"\" Build the indexes in each batch.\n",
    "\n",
    "            Return:\n",
    "                batches: A list of indices batch, e.g., [[0, 2, 8], [3, 6, 4], [5, 1, 7], ...]\n",
    "        \"\"\"\n",
    "        n = len(self.seq_lens)\n",
    "        n_batch = int(np.ceil(n / self.batch_size))\n",
    "        batches = []\n",
    "        # Start your code here\n",
    "        # Step 1. Use np.argsort to get all indices with sorted length\n",
    "        #      2. Split the indices into batches using a for loop: `for i in range(n_batch):`\n",
    "        sorted_indices = np.argsort(self.seq_lens)\n",
    "        batches = [sorted_indices[i::n_batch] for i in range(n_batch)]\n",
    "        # End\n",
    "        return batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batch\n",
    "\n",
    "    def __item__(self, index):\n",
    "        return self.batches[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.batches)\n",
    "        self.counter = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.counter += 1\n",
    "        if self.counter < self.n_batch:\n",
    "            return self.batches[self.counter]\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e8f1e",
   "metadata": {
    "id": "e78e8f1e"
   },
   "source": [
    "### 3.3 Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd73f9b",
   "metadata": {
    "id": "0bd73f9b"
   },
   "source": [
    "Generate the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8c8ad7",
   "metadata": {
    "id": "7e8c8ad7"
   },
   "outputs": [],
   "source": [
    "np.random.seed(6666)\n",
    "train_seq_lens_en = [len(en_sent) for en_sent in train_en]\n",
    "train_seq_lens_fr = [len(fr_sent) for fr_sent in train_fr]\n",
    "valid_seq_lens_en = [len(en_sent) for en_sent in valid_en]\n",
    "valid_seq_lens_fr = [len(fr_sent) for fr_sent in valid_fr]\n",
    "test_seq_lens_en = [len(en_sent) for en_sent in test_en]\n",
    "test_seq_lens_fr = [len(fr_sent) for fr_sent in test_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038af815",
   "metadata": {
    "id": "038af815"
   },
   "source": [
    "Create np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "affa5645",
   "metadata": {
    "id": "affa5645"
   },
   "outputs": [],
   "source": [
    "train_en = np.array(train_en, dtype=object)\n",
    "train_seq_lens_en = np.array(train_seq_lens_en)\n",
    "train_fr = np.array(train_fr, dtype=object)\n",
    "train_seq_lens_fr = np.array(train_seq_lens_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38e14b",
   "metadata": {
    "id": "bc38e14b"
   },
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5baa89ed",
   "metadata": {
    "id": "5baa89ed"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "seed = 6666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1407ae0",
   "metadata": {
    "id": "d1407ae0"
   },
   "outputs": [],
   "source": [
    "src_vocab_size = len(fr_tokenizer.get_vocab())\n",
    "tgt_vocab_size = len(en_tokenizer.get_vocab())\n",
    "hidden_units = 256\n",
    "embedding_dim = 128\n",
    "dropout_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f48a9b3",
   "metadata": {
    "id": "7f48a9b3"
   },
   "outputs": [],
   "source": [
    "model = Seq2seq(src_vocab_size, tgt_vocab_size, embedding_dim, hidden_units, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb52f7-d0be-42be-b1fd-fda10648180f",
   "metadata": {
    "id": "99bb52f7-d0be-42be-b1fd-fda10648180f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "053d5167",
   "metadata": {
    "id": "053d5167"
   },
   "outputs": [],
   "source": [
    "num_epoch = 15\n",
    "# batch_size = 256\n",
    "# num_epoch = 1\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f91b82a",
   "metadata": {
    "id": "0f91b82a"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_batch_sampler = SeqLenBatchSampler(train_seq_lens_fr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd8527-290b-4e7a-b0b4-9a3b86e0f02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48cff587",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48cff587",
    "outputId": "32bc0eb4-8599-464b-90c3-bd0fc527e22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 15 - Step 7276 / 7276 - train loss: 4.5990 - valid loss: 4.4556\n",
      "Epoch 2 / 15 - Step 7276 / 7276 - train loss: 3.7599 - valid loss: 4.1394\n",
      "Epoch 3 / 15 - Step 7276 / 7276 - train loss: 3.4482 - valid loss: 4.0177\n",
      "Epoch 4 / 15 - Step 7276 / 7276 - train loss: 3.2521 - valid loss: 3.9648\n",
      "Epoch 5 / 15 - Step 7276 / 7276 - train loss: 3.1115 - valid loss: 3.9091\n",
      "Epoch 6 / 15 - Step 7276 / 7276 - train loss: 3.0056 - valid loss: 3.9491\n",
      "Epoch 7 / 15 - Step 7276 / 7276 - train loss: 2.9224 - valid loss: 3.9134\n",
      "Epoch 8 / 15 - Step 7276 / 7276 - train loss: 2.8532 - valid loss: 3.9331\n",
      "Epoch 9 / 15 - Step 7276 / 7276 - train loss: 2.7957 - valid loss: 3.9571\n",
      "Epoch 10 / 15 - Step 7276 / 7276 - train loss: 2.7465 - valid loss: 3.9695\n",
      "Epoch 11 / 15 - Step 7276 / 7276 - train loss: 2.7041 - valid loss: 3.9746\n",
      "Epoch 12 / 15 - Step 7276 / 7276 - train loss: 2.6681 - valid loss: 3.9969\n",
      "Epoch 13 / 15 - Step 7276 / 7276 - train loss: 2.6360 - valid loss: 3.9928\n",
      "Epoch 14 / 15 - Step 7276 / 7276 - train loss: 2.6077 - valid loss: 4.0397\n",
      "Epoch 15 / 15 - Step 7276 / 7276 - train loss: 2.5825 - valid loss: 4.0501\n"
     ]
    }
   ],
   "source": [
    "n_training_samples = len(train_fr)\n",
    "n_valid_batch = int(np.ceil(len(valid_fr) / batch_size))\n",
    "pad_token_id = fr_tokenizer.token_to_id('<pad>')\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, data_index in enumerate(train_batch_sampler):\n",
    "        src_batch, src_seq_lens = train_fr[data_index], train_seq_lens_fr[data_index]\n",
    "\n",
    "        tgt_batch, tgt_seq_lens = train_en[data_index], train_seq_lens_en[data_index]\n",
    "        real_batch_size = len(src_batch)\n",
    "        (src_batch, src_seq_lens_batch,\n",
    "         tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch) = pad_batch(src_batch, src_seq_lens,\n",
    "                                                                   tgt_batch, tgt_seq_lens,\n",
    "                                                                   pad_val=pad_token_id)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # print(src_batch[0])\n",
    "            # print()\n",
    "            output = model(src_batch, src_seq_lens_batch, tgt_x_batch, tgt_seq_lens_batch)\n",
    "            loss = seq2seq_loss(output, tgt_y_batch, tgt_seq_lens_batch)\n",
    "\n",
    "        print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {len(train_batch_sampler)} - loss: {loss:.4f}')\n",
    "\n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        epoch_loss += loss * real_batch_size\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    for batch_idx in range(n_valid_batch):\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        src_batch, src_seq_lens = valid_fr[start:end], valid_seq_lens_fr[start:end]\n",
    "        tgt_batch, tgt_seq_lens = valid_en[start:end], valid_seq_lens_en[start:end]\n",
    "        real_batch_size = len(src_batch)\n",
    "        (src_batch, src_seq_lens_batch,\n",
    "         tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch) = pad_batch(src_batch, src_seq_lens,\n",
    "                                                                   tgt_batch, tgt_seq_lens,\n",
    "                                                                   pad_val=pad_token_id)\n",
    "        output = model(src_batch, src_seq_lens_batch, tgt_x_batch, tgt_seq_lens_batch, training=False)\n",
    "        loss = seq2seq_loss(output, tgt_y_batch, tgt_seq_lens_batch)\n",
    "\n",
    "        if batch_idx % 1 == 0 or batch_idx == len(valid_en) - 1:\n",
    "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_valid_batch} - loss: {loss:.4f}')\n",
    "\n",
    "        valid_loss += loss * real_batch_size\n",
    "    train_epoch_loss = epoch_loss / n_training_samples\n",
    "    valid_epoch_loss = valid_loss / len(valid_en)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    valid_losses.append(valid_epoch_loss)\n",
    "    print(f'\\rEpoch {epoch + 1} / {num_epoch} - Step {len(train_batch_sampler)} / {len(train_batch_sampler)} - train loss: {train_epoch_loss:.4f} - valid loss: {valid_epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf29ac",
   "metadata": {
    "id": "a9cf29ac"
   },
   "source": [
    "If you implement everything correctly, the valid loss will be around 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5605580",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5605580",
    "outputId": "331e0ee2-f632-4417-e87f-d28fa706f7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"seq2seq\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  1576448   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| embedding (Embedding)     multiple                  1280000   |\n",
      "|                                                               |\n",
      "| gru (GRU)                 multiple                  296448    |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      " decoder (Decoder)           multiple                  4146448   \n",
      "|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\n",
      "| embedding_1 (Embedding)   multiple                  1280000   |\n",
      "|                                                               |\n",
      "| gru_1 (GRU)               multiple                  296448    |\n",
      "|                                                               |\n",
      "| dropout (Dropout)         multiple                  0         |\n",
      "|                                                               |\n",
      "| dense (Dense)             multiple                  2570000   |\n",
      "¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\n",
      "=================================================================\n",
      "Total params: 5,722,896\n",
      "Trainable params: 5,722,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52a3977b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "52a3977b",
    "outputId": "746e6b0f-30b5-47f0-e33c-aae5a2fcacd4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAGwCAYAAABGogSnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqFklEQVR4nO3dd3hUVf7H8fek95AEUgkhQAi9t2CXSBEV7PJDgRV1RXRl17aoWBAF26qsu9hhdUVWXUEXRZpSlF4NVWoSIAUS0km/vz+GDAQCBJLMzSSf1/PcJ5k7d+Z8BzT5cM6551gMwzAQERERaSSczC5ARERExJ4UfkRERKRRUfgRERGRRkXhR0RERBoVhR8RERFpVBR+REREpFFR+BEREZFGxcXsAuqj8vJyjhw5gq+vLxaLxexyREREpBoMwyA3N5fw8HCcnM7dv6PwU4UjR44QGRlpdhkiIiJyCZKTk2nevPk5n1f4qYKvry9g/cPz8/MzuRoRERGpjpycHCIjI22/x89F4acKFUNdfn5+Cj8iIiIO5kJTVjThWURERBoVhR8RERFpVBR+REREpFHRnB8REWlUysrKKCkpMbsMuQSurq44OzvX+H0UfkREpFEwDIPU1FSysrLMLkVqoEmTJoSGhtZoHT6FHxERaRQqgk9wcDBeXl5axNbBGIZBQUEB6enpAISFhV3yeyn8iIhIg1dWVmYLPkFBQWaXI5fI09MTgPT0dIKDgy95CEwTnkVEpMGrmOPj5eVlciVSUxV/hzWZt6XwIyIijYaGuhxfbfwdKvyIiIhIo6LwIyIiIo2Kwo+IiEgj07JlS95++23T38MsCj92VF5usCctl4y8IrNLERERB2CxWM57vPDCC5f0vuvXr+eBBx6o3WIdiG51t6OHPt/Ej9tTefGmjozu39LsckREpJ5LSUmxff+f//yH5557jt27d9vO+fj42L43DIOysjJcXC78q71Zs2a1W6iDUc+PHbUP8wNgQ+JxkysRERHDMCgoLjXlMAyjWjWGhobaDn9/fywWi+3xrl278PX1ZcGCBfTs2RN3d3d++eUX9u3bx7BhwwgJCcHHx4fevXuzZMmSSu975pCVxWLho48+4uabb8bLy4uYmBi+++67i/rzTEpKYtiwYfj4+ODn58cdd9xBWlqa7fmtW7dyzTXX4Ovri5+fHz179mTDhg0AJCYmcuONNxIQEIC3tzcdO3bkhx9+uKj2L4Z6fuyoV8sAADYezDS5EhEROVFSRofnFprS9o7Jg/Byq51fwX/961954403aNWqFQEBASQnJ3P99dfz8ssv4+7uzqeffsqNN97I7t27adGixTnf58UXX+S1117j9ddf5+9//zsjR44kMTGRwMDAC9ZQXl5uCz7Lly+ntLSU8ePHc+edd7Js2TIARo4cSffu3ZkxYwbOzs5s2bIFV1dXAMaPH09xcTErVqzA29ubHTt2VOrVqm0KP3bULbIJzk4WjmQXciTrBOFNPM0uSUREHNzkyZO57rrrbI8DAwPp2rWr7fFLL73E3Llz+e6773j44YfP+T5jxoxhxIgRALzyyitMnz6ddevWMXjw4AvWsHTpUhISEjhw4ACRkZEAfPrpp3Ts2JH169fTu3dvkpKSeOKJJ2jXrh0AMTExttcnJSVx66230rlzZwBatWp1EX8CF0/hx4683V1oH+bLtsM5bEg8zk0KPyIipvF0dWbH5EGmtV1bevXqVelxXl4eL7zwAt9//z0pKSmUlpZy4sQJkpKSzvs+Xbp0sX3v7e2Nn5+fbR+tC9m5cyeRkZG24APQoUMHmjRpws6dO+nduzd/+ctfuO+++/jss8+Ij4/n9ttvp3Xr1gD86U9/Yty4cSxatIj4+HhuvfXWSvXUNs35sbNeUdbuQw19iYiYy2Kx4OXmYspRmytNe3t7V3r8+OOPM3fuXF555RVWrlzJli1b6Ny5M8XFxed9n4ohqNP/fMrLy2utzhdeeIHt27czdOhQfvrpJzp06MDcuXMBuO+++9i/fz/33HMPCQkJ9OrVi7///e+11vaZFH7srGeUdd6PJj2LiEhd+PXXXxkzZgw333wznTt3JjQ0lIMHD9Zpm+3btyc5OZnk5GTbuR07dpCVlUWHDh1s59q2bcuf//xnFi1axC233MLMmTNtz0VGRvLggw/yzTff8Nhjj/Hhhx/WWb0KP3ZWMel5Z0oOeUWlJlcjIiINTUxMDN988w1btmxh69at/N///V+t9uBUJT4+ns6dOzNy5Eg2bdrEunXrGDVqFFdddRW9evXixIkTPPzwwyxbtozExER+/fVX1q9fT/v27QGYMGECCxcu5MCBA2zatImff/7Z9lxdUPixszB/TyKaeFJuwJakLLPLERGRBuZvf/sbAQEB9O/fnxtvvJFBgwbRo0ePOm3TYrHw7bffEhAQwJVXXkl8fDytWrXiP//5DwDOzs5kZGQwatQo2rZtyx133MGQIUN48cUXASgrK2P8+PG0b9+ewYMH07ZtW/75z3/WXb1GdRcbaERycnLw9/cnOzsbPz+/Wn//P32xme+2HmFCfAwT4tvW+vuLiEhlhYWFHDhwgOjoaDw8PMwuR2rgfH+X1f39rZ4fE9jW+9G8HxEREbtT+DFBxaTnzUlZlJWr401ERMSeFH5M0C7UDx93F/KKStmdmmt2OSIiIo2Kwo8JnJ0sdG/RBICNiVrvR0RExJ7qTfiZNm0aFouFCRMmnPOaWbNmYbFYKh1nTnYyDIPnnnuOsLAwPD09iY+PZ8+ePXVc/cXTej8iIiLmqBfhZ/369bz//vvVWsraz8+PlJQU25GYmFjp+ddee43p06fz3nvvsXbtWry9vRk0aBCFhYV1Vf4lqVjpecNBhR8RERF7Mj385OXlMXLkSD788EMCAgIueL3FYiE0NNR2hISE2J4zDIO3336bZ599lmHDhtGlSxc+/fRTjhw5wrx58875nkVFReTk5FQ66lq3Fk1wssDhrBOkZtevYCYiItKQmR5+xo8fz9ChQ4mPj6/W9Xl5eURFRREZGcmwYcPYvn277bkDBw6Qmppa6b38/f3p27cvq1evPud7Tp06FX9/f9tx+sZsdcXH3YX2YdY1CDZo3o+IiIjdmBp+5syZw6ZNm5g6dWq1ro+NjeWTTz7h22+/5d///jfl5eX079+fQ4cOAZCamgpQqTeo4nHFc1WZOHEi2dnZtuP0vUnqUq+KeT8a+hIRkTp09dVXV5pT27JlS95+++3zvsZisZx31OTM93QkLmY1nJyczKOPPsrixYurvdpmXFwccXFxtsf9+/enffv2vP/++7z00kuXXIu7uzvu7u6X/PpL1bNlIP9anajFDkVEpEo33ngjJSUl/Pjjj2c9t3LlSq688kq2bt1arTmzp1u/fv1Zu8E3Jqb1/GzcuJH09HR69OiBi4sLLi4uLF++nOnTp+Pi4kJZWdkF38PV1ZXu3buzd+9eAEJDQwFIS0urdF1aWprtufqkoudnR0oO+drkVEREzjB27FgWL15sG+E43cyZM+nVq9dFBx+AZs2a4eXlVRslOiTTws+AAQNISEhgy5YttqNXr16MHDmSLVu24OzsfMH3KCsrIyEhgbCwMACio6MJDQ1l6dKltmtycnJYu3ZtpR6j+iK8iSfh/h6UlRtsTc4yuxwREalnbrjhBpo1a8asWbMqnc/Ly+Orr75i7NixZGRkMGLECCIiIvDy8qJz58588cUX533fM4e99uzZw5VXXomHhwcdOnRg8eLFF13r8ePHGTVqFAEBAXh5eTFkyJBKS80kJiZy4403EhAQgLe3Nx07duSHH36wvXbkyJE0a9YMT09PYmJimDlz5kXXUF2mDXv5+vrSqVOnSue8vb0JCgqynR81ahQRERG2OUGTJ0+mX79+tGnThqysLF5//XUSExO57777AGzrBE2ZMoWYmBiio6OZNGkS4eHhDB8+3K6fr7p6tgzkyNYjbEg8Tv82Tc0uR0Sk8TAMKCkwp21XL7BYLniZi4sLo0aNYtasWTzzzDNYTr7mq6++oqysjBEjRpCXl0fPnj156qmn8PPz4/vvv+eee+6hdevW9OnT54JtlJeXc8sttxASEsLatWvJzs6+pLk8Y8aMYc+ePXz33Xf4+fnx1FNPcf3117Njxw5cXV0ZP348xcXFrFixAm9vb3bs2IGPjw8AkyZNYseOHSxYsICmTZuyd+9eTpw4cdE1VJdp4ac6kpKScHI61Tl1/Phx7r//flJTUwkICKBnz56sWrWKDh062K558sknyc/P54EHHiArK4vLL7+cH3/8sX7s4ltWAps/g24jwcU6x6hXVAD/Oxl+RETEjkoK4JVwc9p++gi4VW/Ozb333svrr7/O8uXLufrqqwHrkNett95qu0v58ccft13/yCOPsHDhQr788stqhZ8lS5awa9cuFi5cSHi49c/jlVdeYciQIdX+OBWh59dff6V///4AfP7550RGRjJv3jxuv/12kpKSuPXWW+ncuTMArVq1sr0+KSmJ7t2706tXL8DaM1WX6lX4WbZs2Xkfv/XWW7z11lvnfQ+LxcLkyZOZPHlyLVdXC2bfAft+goIMuPIJ4LRNThOPU1Zu4Ox04X8JiIhI49GuXTv69+/PJ598wtVXX83evXtZuXKl7fdcWVkZr7zyCl9++SWHDx+muLiYoqKias/p2blzJ5GRkbbgA1z0VJGdO3fi4uJC3759beeCgoKIjY1l586dAPzpT39i3LhxLFq0iPj4eG699VbbfKVx48Zx6623smnTJgYOHMjw4cNtIaou1Kvw0+B1/T9r+FnxBnS+HQJa0i7UF283Z3KLSvk9Lde29o+IiNQxVy9rD4xZbV+EsWPH8sgjj/CPf/yDmTNn0rp1a6666ioAXn/9dd555x3efvttOnfujLe3NxMmTKC4uLguKr9k9913H4MGDeL7779n0aJFTJ06lTfffJNHHnmEIUOGkJiYyA8//MDixYsZMGAA48eP54033qiTWkxf5LBR6XwbRF8JpYXww5NgGLg4O9Ht5CanGvoSEbEji8U69GTGUY35Pqe74447cHJyYvbs2Xz66afce++9tvk/v/76K8OGDePuu++ma9eutGrVit9//73a792+fXuSk5NJSUmxnVuzZs1F1de+fXtKS0tZu3at7VxGRga7d++uNDUlMjKSBx98kG+++YbHHnuMDz/80PZcs2bNGD16NP/+9795++23+eCDDy6qhouh8GNPFgtc/yY4ucKehbDrewB6ntzna+NBrfQsIiJn8/Hx4c4772TixImkpKQwZswY23MxMTEsXryYVatWsXPnTv74xz+eteTL+cTHx9O2bVtGjx7N1q1bWblyJc8888xF1RcTE8OwYcO4//77+eWXX9i6dSt33303ERERDBs2DIAJEyawcOFCDhw4wKZNm/j5559p3749AM899xzffvste/fuZfv27cyfP9/2XF1Q+LG3Zm3hsj9Zv1/wFBTlnVrpWT0/IiJyDmPHjuX48eMMGjSo0vycZ599lh49ejBo0CCuvvpqQkNDL+oOZycnJ+bOncuJEyfo06cP9913Hy+//PJF1zdz5kx69uzJDTfcQFxcHIZh8MMPP+Dq6gpY5yaNHz+e9u3bM3jwYNq2bcs///lPANzc3Jg4cSJdunThyiuvxNnZmTlz5lx0DdVlMQzDqLN3d1A5OTn4+/uTnZ2Nn18dzMEpLoB/9oWsJLjsUXKvmETXFxdRbsDapwcQ4lcP7kwTEWlACgsLOXDgANHR0fXj7l+5ZOf7u6zu72/1/JjBzQuGvG79fvU/8M3eQ2zoyU1Otc+XiIhInVL4MUvsYGh3A5SXwveP0cs26VnzfkREROqSwo+ZBk+z3u6YtIqbnVcCaJNTERGROqbwY6YmkXDVUwB03fkG/uSx/UgOBcXa5FRERKSuKPyYrd9D0KwdzicyeN7rv5SVG2zRJqciInVC9/g4vtr4O1T4MZuLGwx9E4Dh5YvoatnLRk16FhGpVRW3WxcUmLSRqdSair/Dir/TS6HtLeqDlpdD1xE4bf2Cl10/4Y2DfYAYs6sSEWkwnJ2dadKkCenp6QB4eXnZVkgWx2AYBgUFBaSnp9OkSROcnZ0v+b0UfuqL616ibOf3dCo+SGzyfygv74eTNjkVEak1oaGhALYAJI6pSZMmtr/LS6VFDqtQ54scnkPZuo9w/uExcgxP0katJKa1en9ERGpbWVkZJSUlZpchl8DV1fW8PT7V/f2tnp96xLnXH9i3+H1al/xOxpJJ0LrulvYWEWmsnJ2dazRkIo5PE57rEydnVrd/hjLDQnTKAti/zOyKREREGhyFn3omstNlfFZ2nfXB949BaZG5BYmIiDQwCj/1TPcWTfhb2R0cNfwhYy+smm52SSIiIg2Kwk894+fhSnhICC+V3G09seINyDxgblEiIiINiMJPPdSrZQDflffngG9PKC2EBU+CbsoTERGpFQo/9VCvqEDAwuvO94OTK+xZBLvmm12WiIhIg6DwUw/1jAoAYFG6PyX9HrGeXPBXKMozsSoREZGGQeGnHmoe4EmInzul5QabosZCkxaQcwhWvGZ2aSIiIg5P4aceslgsJ4e+YMORQhjyuvWJ1f+AtB0mViYiIuL4FH7qqYqhrw0HMyF2MLS7AcpLrWv/aPKziIjIJVP4qad6tbSGn42JxykvN2DwNHD1gqRVsPULk6sTERFxXAo/9VT7MD88XZ3JKSxl79E8aBIJVz1lfXLRJCjINLdAERERB6XwU0+5OjvRLbIJABsOHree7PcQNGsHBcdg6WTzihMREXFgCj/1WMXQ14bEk708Lm4w9G/W7zfOgkMbzClMRETEgSn81GMVk543Jh4/dbLlZdB1BGDA/D9DWak5xYmIiDioehN+pk2bhsViYcKECee85sMPP+SKK64gICCAgIAA4uPjWbduXaVrxowZg8ViqXQMHjy4jquvGz2iArBYIDGjgKO5p+3uft1L4OEPqb/Bho/NK1BERMQB1Yvws379et5//326dOly3uuWLVvGiBEj+Pnnn1m9ejWRkZEMHDiQw4cPV7pu8ODBpKSk2I4vvnDMu6P8PFyJDfEFYGPiaROcfZrBgOet3/80BXJTTahORETEMZkefvLy8hg5ciQffvghAQEB5732888/56GHHqJbt260a9eOjz76iPLycpYuXVrpOnd3d0JDQ23Hhd63Pju13s/xM54YAxE9oSgHFj5j/8JEREQclOnhZ/z48QwdOpT4+PiLfm1BQQElJSUEBgZWOr9s2TKCg4OJjY1l3LhxZGRknPd9ioqKyMnJqXTUF6cmPZ8RfpycYeibYHGCbV/Dvp9NqE5ERMTxmBp+5syZw6ZNm5g6deolvf6pp54iPDy8UnAaPHgwn376KUuXLuXVV19l+fLlDBkyhLKysnO+z9SpU/H397cdkZGRl1RPXajY5mL7kWwKS874DOHdofd91u9/eBxKixAREZHzMy38JCcn8+ijj/L555/j4eFx0a+fNm0ac+bMYe7cuZVef9ddd3HTTTfRuXNnhg8fzvz581m/fj3Lli0753tNnDiR7Oxs25GcnHwpH6lONA/wJNjXnZIyg63JWWdfcO2z4B0MGXvh1+l2r09ERMTRmBZ+Nm7cSHp6Oj169MDFxQUXFxeWL1/O9OnTcXFxOW9PzRtvvMG0adNYtGjRBSdJt2rViqZNm7J3795zXuPu7o6fn1+lo76wWCznHvoC611fg16xfr/yDcg8YMfqREREHI9p4WfAgAEkJCSwZcsW29GrVy9GjhzJli1bcHZ2rvJ1r732Gi+99BI//vgjvXr1umA7hw4dIiMjg7CwsNr+CHbT8+TQ18aqwg9A59sg+kooLYQFT2rjUxERkfMwLfz4+vrSqVOnSoe3tzdBQUF06tQJgFGjRjFx4kTba1599VUmTZrEJ598QsuWLUlNTSU1NZW8vDzAeufYE088wZo1azh48CBLly5l2LBhtGnThkGDBpnyOWvD6YsdlpdXEWwsFrj+TXByhT2LYNd8O1coIiLiOEy/2+t8kpKSSElJsT2eMWMGxcXF3HbbbYSFhdmON954AwBnZ2d+++03brrpJtq2bcvYsWPp2bMnK1euxN3d3ayPUWMdw/3wcHUi+0QJ+47mVX1Rs7Zw2aPW7xf8FYrOcZ2IiEgjZzEMjZGcKScnB39/f7Kzs+vN/J8731/N2gOZTL2lMyP6tKj6ouIC+GdfyEqC/n+CgS/Zt0gRERETVff3d73u+ZFTbJOez1zs8HRuXjDkdev3a/4JaTvsUJmIiIhjUfhxEL1sk54zz39h7GBodwOUl8L3j2nys4iIyBkUfhxEjxbWnp+DZ25yWpXB08DVC5JWwVbH3NdMRESkrij8OAh/L1fahvgA57nlvUKTSLjqKev3i56Fggv0FomIiDQiCj8OpGd1h74A+j0EzdpBQQYsnVzHlYmIiDgOhR8H0ivqPCs9n8nFDYb+zfr9xllwaEPdFSYiIuJAFH4cSMUdX9sOV7HJaVVaXgZdRwAGzP8zlJXWbYEiIiIOQOHHgbQI9KKpj3WT098OZVfvRde9ZN3/K/U3WP9R3RYoIiLiABR+HIjFYjlt6Kuak5h9msGA563f/zQFclPrqDoRERHHoPDjYCqGvjaeb7HDM/UcAxE9oTgXFj5dN4WJiIg4CIUfB2Pb5DTpHJucVsXJ2Tr52eIE2/4Lu3+swwpFRETqN4UfB9Mx3B93FyeyCkrYf+wiNi8N7wa977d+/8Vd8MOTUJRbJzWKiIjUZwo/DsbNxYmukU2AC+zzVZX456HLnYAB696Hf/SFXd/Xeo0iIiL1mcKPA7qo9X5O5+YNt3wA98yFgJaQcxjm/B/MGQk5R2q/UBERkXpI4ccB2SY9X2z4qdD6Whi3Gi7/Mzi5wK758G4fWPsBlFdj/SAREREHpvDjgCo2OT1wLJ9jeRfY5PRc3Lwg/gX44wpo3tt6J9iCJ+DjgZC6rfaKFRERqWcUfhxQEy83YoKrucnphYR0hHsXwfVvgLsfHN4A718Ji5+D4oJaqFZERKR+UfhxUBVDX5tqGn4AnJygz/0wfh20vwmMMvj1HfhnP9i7pObvLyIiUo8o/Dioih3eL3rS8/n4hcGdn8GIOeDXHLIS4d+3wtdjIS+99toRERExkcKPg6q44yvhUDU3Ob0YsUNg/Bro99DJhRG/hnd7w8Z/QXl57bYlIiJiZwo/DioqyIumPm4Ul5Wz7XA1Nzm9GO6+MHgq3LcUQrtAYRb8708waygc3V377YmIiNiJwo+Dslgstq0uanXo60wRPeD+n2Hgy+DqBUmrYMZl8PMrUFJYd+2KiIjUEYUfB9arYt7Pxa70fLGcXaD/w/DQGogZCOUlsPxVeO8yOLCybtsWERGpZQo/DqxnxR1fSccxjGpucloTAVHwf1/CbTPBOxgy9sK/boB546Egs+7bFxERqQUKPw6s08lNTjPzi9l/LN8+jVos0OkWeHg99PyD9dyWf1snRP/2JdgjhImIiNSAi9kFyKVzc3Gia/MmrDuYycaDx2ndzMd+jXs2gRvfhq53wf8ehaO74Jv7YesXMPRNCGxlv1pERMRcpcVQlGvdLaDoAkfFNb3vg+grTSlX4cfB9WwZwLqDmWxIzOSO3pH2L6BFP/jjSlj1Dix/Hfb9BP+Mg6uegv6PgLOr/WsSEZELKy+H4ryTgeTk16Kck1/zTgssOWdcU3HutMdll7DVUqurFX7k0lzyDu+1ycUNrnwCOt4C8yfAgRWw9EVI+BpufAcie5tXm4iIQH4GJK2GxFWQ+Ctk7LP2wNQ2Vy/rUimnH25nPHb3sW6n1CKu9tuvJoUfB1dxu/v+o/lk5hcT6O1mXjFBrWHUd7B1Dix8GtK3w8fXQe+xMOA58PA3rzYRkcYkJ8UachJXWY+jO899rZPLacHE72Rg8TkjsPidDC1nnDv9Ojcf693BDsAxqpRzauLlRptgH/am57Ex8TjXdQgxtyCLBbqNsN4Sv+gZ6xyg9R/Bru9hyKvWvcMsFnNrFBFpSAwDjh88FXQSf4XjB86+rll7iOpvPcK6gkcTa2hxcW90P5cVfhqAXlEB7E3PY0Nipvnhp4J3ENz8nnVC9Pw/Q+Z++HIUtB0C102GZm3NrlBExDEZBhz7vXLPTs7hytdYnCC0M0RdZg07Lfpbfy4LUI9udZ82bRoWi4UJEyac97qvvvqKdu3a4eHhQefOnfnhhx8qPW8YBs899xxhYWF4enoSHx/Pnj176rBy81UMfW2s68UOL0Wrq2HcKrjicWvX6u8L4B+94Z2u1lC0839wIsvsKkVE6q/yMkjZCmtmwH/uhtfbwD/6WH+GJnxlDT5OrhDZFy7/M4z8Gp46CH9cYd2mqP2NCj5nqBc9P+vXr+f999+nS5cu571u1apVjBgxgqlTp3LDDTcwe/Zshg8fzqZNm+jUqRMAr732GtOnT+df//oX0dHRTJo0iUGDBrFjxw48PDzs8XHsrldL60rPvx3Opqi0DHcXZ5MrOoOrJwyYBJ1vg0XPwv7l1i7aDZ9YD4szNO8Fra+1HuE9HGbcWESk1pWVwJEtp3p2ktZA0Rl7OLp4Wm8mqejZiegFbl6mlOuILIZdlgY+t7y8PHr06ME///lPpkyZQrdu3Xj77bervPbOO+8kPz+f+fPn287169ePbt268d5772EYBuHh4Tz22GM8/vjjAGRnZxMSEsKsWbO46667qlVTTk4O/v7+ZGdn4+fnV+PPWNcMw6DXlCVk5Bfz33Fx9Dy57UW9VZQLB3+13ha/7yfIOKNnzt0fWl15KgwFtDSlTBERuyg5AYc2nJqvc2g9lBRUvsbN17q0SFR/a+AJ726901Yqqe7vb9P/eT1+/HiGDh1KfHw8U6ZMOe+1q1ev5i9/+Uulc4MGDWLevHkAHDhwgNTUVOLj423P+/v707dvX1avXn3O8FNUVERR0ak1CnJyci7x05jDYrHQIyqAxTvS2HDweP0PP+6+EDvYegBkJcG+n61BaP8y6w7yO/9nPcC6YGJFEGp5BXjU/0AqIiYrL4f8o5BzCLIPQfbhk1+TITcFykvB2c06XORccbhZh+ed3U4eLhe45uQ5J9cLXH/mY2fr9kCJq6z/EDy80bpn4uk8A08Fnaj+1vk7TvWsV9+BmRp+5syZw6ZNm1i/fn21rk9NTSUkpPKE3pCQEFJTU23PV5w71zVVmTp1Ki+++OLFlF7v9KoIP4nH+aPZxVysJi2g52jrUV5m7e6t6BU6tM46WTpzv/WuMScXaN7ntCGybvqBINIYFeVWDjQ5Fd+fPHIOQ1mx2VVWn2/YqTuxoi6Hpm3Bqd5My21wTAs/ycnJPProoyxevNj0uTgTJ06s1KOUk5NDZKQJqyXXQK+KTU4TrZucWhz1tkUnZ2je03pc9QQU5sDBX06Focx9kLTKevw8xXqrZqurT4WhJo719yYiVSgrgZwjpwWa5NOCziFrb05h9oXfB4s1VPg3B/8I61e/5uAXbu2BKS+xBqSyUuvX8hJr22Unz5efPF9xrtauLwbf0FO9OlH9ISC60d1ubibTws/GjRtJT0+nR48etnNlZWWsWLGCd999l6KiIpydK/+LPjQ0lLS0tErn0tLSCA0NtT1fcS4sLKzSNd26dTtnLe7u7ri7u9f0I5mqU4Q/bi5OZOQXc+BYPq3suc9XXfLwg3bXWw+wTpTe9zPsWwr7V1iHyHbMsx4AQTGnDZFdbl2US0TMZxgng0CxdY5LzuFz99zkpgLVmI7q4Q/+kSdDzclwc/rhG6YtdqRKpoWfAQMGkJCQUOncH/7wB9q1a8dTTz11VvABiIuLY+nSpZVuh1+8eDFxcdYlsqOjowkNDWXp0qW2sJOTk8PatWsZN25cnX2W+sDdxZkuEf5sSDzOhsTjDSf8nCmgJfT6g/UoK4Ujm04bIttgnTydsQfWvW8dZ2/RD1pfYw1DoV3VjSyNU3m5dSuDE1nWHpOKvZhKi08FktKiM74vsvZSnHW+4vvik9ec/n3JuV97sUNQzm6nBZrIyj03FT057r518sclDZ9p4cfX19d2e3oFb29vgoKCbOdHjRpFREQEU6dOBeDRRx/lqquu4s0332To0KHMmTOHDRs28MEHHwDY1gmaMmUKMTExtlvdw8PDGT58uF0/nxl6tgxgQ+JxNh48zh29GsHwj7MLRPaxHlf/1fqD/eBKaxDauxSyEq2PD66EpZOtEwhbXwOtB0CbePCtJwtCilRHSaG1p7Mw+1SIKcw+eS7rtHNVXFOUA0a5icVXwSf0VKDxjzwt6ERYH3s11T9WpM6YfrfX+SQlJeF02n/8/fv3Z/bs2Tz77LM8/fTTxMTEMG/evEoh6sknnyQ/P58HHniArKwsLr/8cn788UfT5xXZQ6+oQN5nPxsSM80uxRyeTayLebW/0fo4c//JXqGfrWsLnciEbf+1HmBd3j1moPWI6KmJ01L3ysshLw3yUs8RYrLPDjEV5y5l1+wzuXhYh4rcfa3fO7uCs7v1lmlntzO+d7Nue+Dsbr3Oxf2M81V9f/I9Kn3vWvX76P83MZHp6/zUR462zk+FzPxiery0GIDNk64jwMxNTuubshLr7aR7l8LeJdbhstN5Blh7g2IGWnuGtBqqXIryMut8lawk6zyWrETr9xVH9qEa3oFksYYXD39r2Pfwt076P+txVef8wbXh/yNQGjeHWedHak+gtxutmnmz/2g+GxOPE19f9vmqD5xPzv9p0Q+ufQby0q1BaM8i6+TpE8ety8QnfAVYrD1BMQMh5joI6+bY3e9FeZCyBQ5vsoa+jH3WuVOhXSC0k3X9EL8I3WlSHeVl1jVibIHmjICTfejs9VrOZHEC72Br4K5WkDntsZuvY/+3KFJPqOenCo7a8wPw5Ndb+XLDIR68qjV/HdLO7HIcQ1mpdUXVvYutYSi18kR8vJtBm+sgJt46cdozwJw6q6O0CFK3WUNORdg5upsL3jnj0cQagkI7Q0gnayhq1s46RNGYlJWeEW4qQs3p4ab0/O9hcbbOXWnSovLhH2n96heuO5BE6kh1f38r/FTBkcPPl+uTefK/v9G7ZQBfPdjf7HIcU84R69DYnkWwb5n1LpkKFmfrBOuY66w9QyGdzOsxKS+zBpvTg07qtqp7HvwirMvhR/SwLp52/KA15KVug2O7q/6F7uQCTWNP9Q6FnPzq3bTOP1qdKSm0zrfJPnR2wMlKtP7dXyjcOLmcEW6iTgWbJi1O3l6tTnURMyj81IAjh599R/MY8OZy3FycSHhhYP3b5NTRlBZD8hprENqzGI7uqvy8b5g1CLW5zrrYYl1tvWEYcPzAyZCz2fo1ZSuU5J99rWegNeSE9zj19Xx3tpUWWT9X6jZI23YyFCVYJ9xWxTfsVBAK7WQdPgtsZe4E1rIS61Bmbqq15yY3pervTxy/8Hs5uVoXy7T12JzRg+Mbqsm6IvWUwk8NOHL4MQyDnlOWkJlfzH/H9adnVD0eonFEWUnWELRnMRxYXnnzQScXaBF36g6yZrGX3iuUk1K5R+fI5qp/cbt6W7f4OD3sNImqeW+UYVh7R9K2WUNR6m/W7zP3V329iyeEdKjcQxTSsebrsJSXQ8GxM8LMya85pwWb/KNUa1E8sN7l5BdxxrBU1MmvkdZbsDWvRsQhKfzUgCOHH4D7/rWBJTvTePr6djxwZWuzy2m4SgqtOzDvOTlXKHNf5ef9W5waHou+Aty8q36fgkxruDmyCQ6f/JqbcvZ1zm7WYHF60Gna1r69EEW5kLYD0hJODZul7zh7B+oKAdFnzCXqbB0yAmuYOzPQnPk1L+3Cw1AVnFysvVK+oSePsNO+hp167OGvyd0iDZTCTw04evh5b/k+pi3YxcAOIXwwqpfZ5TQeGftOzRU6sLLyuizO7tDyMmsQCm5feVLy8QNnv5fFyTrhOLwHRHS3fg3pZF07pb4pL7P2CKUmnDZstg1yj1R9vbvfyVWBC6vZgAV8gk8GmfAqgs3Jr15B6rERaeR0q3sj1uvkUNdGR9/k1NEEtbYeff8IxQXWlaX3LLIeWUmntuGoSkB05R6d0C6Osy+ZkzM0jbEenW45dT4/42QP0bZTwejoLutqwxU8AysHGL8qQo13sCYQi0it0k+UBqhThD9uztZNTg9mFBDd9BzDLVJ33Lyg7SDrYRhw7PeTw2ML4XjiyeGrkz064d3BK9Dsimufd5B1Enirq0+dKy2y9hK5eoFPiBbdExFTKPw0QB6uznRu7s/GxONsOJip8GM2i8U6+blZLPR/2OxqzOXibh32ExExkQbIG6jTh75ERETkFIWfBqqnwo+IiEiVFH4aqIrwsyc9j6yCmmykKCIi0rAo/DRQQT7utDo512dTknp/REREKij8NGAVvT8bDir8iIiIVFD4acB6tTwZfjTvR0RExEbhpwHrGWVdO2ZrchbFpeUmVyMiIlI/KPw0YK2beRPg5UpRaTnbj2SbXY6IiEi9oPDTgFksFt3yLiIicgaFnwauYuhLk55FRESsFH4auNMnPRuGYXI1IiIi5lP4aeA6n9zk9FheEUmZBWaXIyIiYjqFnwbOw9WZThF+gIa+REREQOGnUejV8uS8H016FhERUfhpDE7d8ZVpciUiIiLmU/hpBCrCz+9peWQXlJhcjYiIiLkUfhqBpj7uRGuTUxEREUDhp9GwbXKqoS8REWnkFH4aCe3wLiIiYqXw00j0Ohl+th7KoqRMm5yKiEjjpfDTSLRu5oO/pyuFJeVsP5JjdjkiIiKmMTX8zJgxgy5duuDn54efnx9xcXEsWLDgnNdfffXVWCyWs46hQ4farhkzZsxZzw8ePNgeH6dec3KynDb0pXk/IiLSeJkafpo3b860adPYuHEjGzZs4Nprr2XYsGFs3769yuu/+eYbUlJSbMe2bdtwdnbm9ttvr3Td4MGDK133xRdf2OPj1HsV4WfR9jTt8yUiIo2Wi5mN33jjjZUev/zyy8yYMYM1a9bQsWPHs64PDAys9HjOnDl4eXmdFX7c3d0JDQ2tdh1FRUUUFRXZHufkNMxhoZu6hjN96R7WHczk2y1HGN49wuySRERE7K7ezPkpKytjzpw55OfnExcXV63XfPzxx9x11114e3tXOr9s2TKCg4OJjY1l3LhxZGRknPd9pk6dir+/v+2IjIy85M9Rn0UGevHItW0AmPL9Di14KCIijZLFMHn8IyEhgbi4OAoLC/Hx8WH27Nlcf/31F3zdunXr6Nu3L2vXrqVPnz628xW9QdHR0ezbt4+nn34aHx8fVq9ejbOzc5XvVVXPT2RkJNnZ2fj5+dX8Q9YjxaXlXD99JXvT8xjRpwVTb+lsdkkiIiK1IicnB39//wv+/jY9/BQXF5OUlER2djZff/01H330EcuXL6dDhw7nfd0f//hHVq9ezW+//Xbe6/bv30/r1q1ZsmQJAwYMqFZN1f3Dc1Rr92dw5wdrAPjvuDh6RgVe4BUiIiL1X3V/f5s+7OXm5kabNm3o2bMnU6dOpWvXrrzzzjvnfU1+fj5z5sxh7NixF3z/Vq1a0bRpU/bu3VtbJTu8vq2CuK1ncwCembtN6/6IiEijYnr4OVN5eXmlIaiqfPXVVxQVFXH33Xdf8P0OHTpERkYGYWFhtVVig/D09e0J8HJlV2oun/xywOxyRERE7MbU8DNx4kRWrFjBwYMHSUhIYOLEiSxbtoyRI0cCMGrUKCZOnHjW6z7++GOGDx9OUFBQpfN5eXk88cQTrFmzhoMHD7J06VKGDRtGmzZtGDRokF0+k6MI9HZj4vXtAXh7yR4OHS8wuSIRERH7MDX8pKenM2rUKGJjYxkwYADr169n4cKFXHfddQAkJSWRkpJS6TW7d+/ml19+qXLIy9nZmd9++42bbrqJtm3bMnbsWHr27MnKlStxd3e3y2dyJLf3bE6f6EBOlJTx/LfbtfaPiIg0CqZPeK6PGvqE59PtScvl+ukrKSkzeO/ungzuVP31kUREROoTh5nwLOaKCfHlgStbAfDCd9vJKyo1uSIREZG6pfAjPHJtDC0CvUjNKeRvi343uxwREZE6pfAjeLg689LwTgDMWnWAbYezTa5IRESk7ij8CABXtW3GDV3CKDfgmbkJlJVrKpiIiDRMCj9i89wNHfB1d2HroWw+X5todjkiIiJ1QuFHbIL9PHhicCwAr/+4m7ScQpMrEhERqX0KP1LJyL5RdG3uT25RKZPn7zC7HBERkVqn8COVODtZePnmzjhZ4PvfUli2O93skkRERGrVJYWf5ORkDh06ZHu8bt06JkyYwAcffFBrhYl5OkX4M6Z/NADPfbudwpIykysSERGpPZcUfv7v//6Pn3/+GYDU1FSuu+461q1bxzPPPMPkyZNrtUAxx18GtiXM34OkzAL+/tMes8sRERGpNZcUfrZt20afPn0A+PLLL+nUqROrVq3i888/Z9asWbVZn5jEx92F52/sCMAHK/azJy3X5IpERERqxyWFn5KSEttGoUuWLOGmm24CoF27dmdtRCqOa1DHEOLbB1NSZvDM3G3a+FRERBqESwo/HTt25L333mPlypUsXryYwYMHA3DkyBGCgoJqtUAxj8Vi4YWbOuLp6sy6g5l8tfHQhV8kIiJSz11S+Hn11Vd5//33ufrqqxkxYgRdu3YF4LvvvrMNh0nD0DzAiwnxMQBM/WEnmfnFJlckIiJSMxbjEscyysrKyMnJISAgwHbu4MGDeHl5ERwcXGsFmiEnJwd/f3+ys7Px8/MzuxzTlZSVc+Pff2FXai639WzOG7d3NbskERGRs1T39/cl9fycOHGCoqIiW/BJTEzk7bffZvfu3Q4ffORsrs5OvHxzZywW+HrjIdbszzC7JBERkUt2SeFn2LBhfPrppwBkZWXRt29f3nzzTYYPH86MGTNqtUCpH3pGBTCiTwsAnp23jeLScpMrEhERuTSXFH42bdrEFVdcAcDXX39NSEgIiYmJfPrpp0yfPr1WC5T646lB7Wjq48be9Dw+WLHP7HJEREQuySWFn4KCAnx9fQFYtGgRt9xyC05OTvTr14/ERO0G3lD5e7ny7NAOAPz9p70kZuSbXJGIiMjFu6Tw06ZNG+bNm0dycjILFy5k4MCBAKSnp2uCcAM3rFs4l7dpSlFpOc/O09o/IiLieC4p/Dz33HM8/vjjtGzZkj59+hAXFwdYe4G6d+9eqwVK/WKxWHhpeCfcXJxYuecY83/TopYiIuJYLvlW99TUVFJSUujatStOTtYMtW7dOvz8/GjXrl2tFmlvutX9wt5Zsoe3lvxOM193lvzlKvw9Xc0uSUREGrk6vdUdIDQ0lO7du3PkyBHbDu99+vRx+OAj1fPg1a1o1dSbo7lFvLFwt9nliIiIVNslhZ/y8nImT56Mv78/UVFRREVF0aRJE1566SXKy3ULdGPg7uLMlJs7AfDvtYlsSc4ytyAREZFquqTw88wzz/Duu+8ybdo0Nm/ezObNm3nllVf4+9//zqRJk2q7Rqmn+rduyi3dIzAMePqbBErLFHxFRKT+u6Q5P+Hh4bz33nu23dwrfPvttzz00EMcPny41go0g+b8VN+xvCIGvLmc7BMlPDu0Pfdd0crskkREpJGq0zk/mZmZVc7tadeuHZmZmZfyluKgmvq4M3GI9b+Fvy3+nSNZJ0yuSERE5PwuKfx07dqVd99996zz7777Ll26dKlxUeJY7ugVSa+oAAqKy3jxf9vNLkdEROS8XC7lRa+99hpDhw5lyZIltjV+Vq9eTXJyMj/88EOtFij1n5OThZdv7szQ6StZuD2NJTvSiO8QYnZZIiIiVbqknp+rrrqK33//nZtvvpmsrCyysrK45ZZb2L59O5999llt1ygOIDbU1zbf5/nvtlNQXGpyRSIiIlW75HV+wsPDefnll/nvf//Lf//7X6ZMmcLx48f5+OOPq/0eM2bMoEuXLvj5+eHn50dcXBwLFiw45/WzZs3CYrFUOjw8PCpdYxgGzz33HGFhYXh6ehIfH8+ePXsu9WPKRXh0QAzNAzw5nHWCt5foz1xEROqnSw4/taF58+ZMmzaNjRs3smHDBq699lqGDRvG9u3nnjfi5+dHSkqK7ThzI9XXXnuN6dOn895777F27Vq8vb0ZNGgQhYWFdf1xGj1PN2cmD+sIwMe/HGBnSo7JFYmIiJzN1PBz4403cv311xMTE0Pbtm15+eWX8fHxYc2aNed8jcViITQ01HaEhJyaW2IYBm+//TbPPvssw4YNo0uXLnz66accOXKEefPm2eETybXtQhjSKZSycoOn5yZQXq6NT0VEpH4xNfycrqysjDlz5pCfn2+bRF2VvLw8oqKiiIyMPKuX6MCBA6SmphIfH2875+/vT9++fVm9evU537OoqIicnJxKh1y652/siI+7C5uTsvhifZLZ5YiIiFRyUXd73XLLLed9Pisr66ILSEhIIC4ujsLCQnx8fJg7dy4dOnSo8trY2Fg++eQTunTpQnZ2Nm+88Qb9+/dn+/btNG/enNTUVIBKvUEVjyueq8rUqVN58cUXL7p2qVqovwePDWzLi//bwasLdjGwQyjNfN3NLktERAS4yJ4ff3//8x5RUVGMGjXqogqIjY1ly5YtrF27lnHjxjF69Gh27NhR5bVxcXGMGjWKbt26cdVVV/HNN9/QrFkz3n///Ytq80wTJ04kOzvbdiQnJ9fo/QRGxbWkc4Q/OYWlvPx91X+fIiIiZrionp+ZM2fWegFubm60adMGgJ49e7J+/XreeeedagUaV1dXunfvzt69ewHrTvMAaWlphIWF2a5LS0ujW7du53wfd3d33N3VM1GbnJ0svHxzJ4b/41fmbTnCbT0juTymqdlliYiI1J85PxXKy8spKiqq1rVlZWUkJCTYgk50dDShoaEsXbrUdk1OTg5r16497zwiqRtdmjdhVFxLACZ9u43CkjJzCxIREcHk8DNx4kRWrFjBwYMHSUhIYOLEiSxbtoyRI0cCMGrUKCZOnGi7fvLkySxatIj9+/ezadMm7r77bhITE7nvvvsA651gEyZMYMqUKXz33XckJCQwatQowsPDGT58uBkfsdF7bGBbQvzcOXAsn38u22d2OSIiIpe2vUVtSU9PZ9SoUaSkpODv70+XLl1YuHAh1113HQBJSUk4OZ3KZ8ePH+f+++8nNTWVgIAAevbsyapVqypNkH7yySfJz8/ngQceICsri8svv5wff/zxrMUQxT58PVx5/saOPPT5Jt5bto9h3cJp3czH7LJERKQRsxiGoYVYzpCTk4O/vz/Z2dn4+fmZXY7DMwyDP8xaz7LdR+nfOojP7+uLxWIxuywREWlgqvv7u97N+ZGGx2Kx8NKwTni4OrFqXwZzNx82uyQREWnEFH7ELiIDvfjTgBgAnvt2O5uTjptckYiINFYKP2I391/Rin6tAskrKmXUx+vYmpxldkkiItIIKfyI3bg6O/HJmN70aRlIblEp93y8lm2Hs80uS0REGhmFH7ErLzcXPvlDb3pFBZBTWMrIj9ay/YgCkIiI2I/Cj9idj7sLM//Qm+4tmpB9ooS7P1rLzhRtJisiIvah8COm8PVw5V/39qFrc3+OF5Qw8qO17E7NNbssERFpBBR+xDR+Hq58OrYvnSP8ycwvZuRHa9iTpgAkIiJ1S+FHTOXv6cpnY/vQMdyPY3nFjPhwLXvT88wuS0REGjCFHzFdEy83/j22L+1CfTmWV8T/fbiGA8fyzS5LREQaKIUfqRcCvN34/L6+xIb4kp5bxIgP1nBQAUhEROqAwo/UG0E+7nx+f19ign1IzSlkxIdrSMooMLssERFpYBR+pF5p6uPO7Pv70bqZNynZ1gCUnKkAJCIitUfhR+qdZr7ufHF/P1o19eZw1glGfLiGw1knzC5LREQaCIUfqZeC/TyYfX8/WgZ5cej4CUZ8sIaUbAUgERGpOYUfqbdC/T344oF+tAj0IimzgBEfrCEtp9DsskRExMEp/Ei9FubvyRcP9KN5gCcHM6wBKF0BSEREakDhR+q9iCaefHF/PyKaeLL/WD4jPlzD0dwis8sSEREHpfAjDiEy0Isv7u9HmL8H+47m838fruFYngKQiIhcPIUfcRgtgqwBKMTPnT3pedz90Voy84vNLktERByMwo84lJZNvfni/n4E+7qzKzWXkR+t5bgCkIiIXASFH3E4rZr5MPv+fjT1cWdnSg53f7yW7IISs8sSEREHofAjDqlNsA9f3N+XIG83th/J4Z5P1pJ9QgFIREQuTOFHHFZMiC+z7+9HoLcbvx3KZtQn68gpVAASEZHzU/gRhxYb6svn9/WliZcrW5OzGP3JOnIVgERE5DwUfsThtQ/z499j++Lv6crmpCz+MHM9+UWlZpclIiL1lMKPNAidIvz599i++Hq4sCHxOH+YtZ6CYgUgERE5m8KPNBidm58MQO4urDuQyb2z1nOiuMzsskREpJ5R+JEGpWtkE/41tg8+7i6s2Z/JfZ+up7BEAUhERE5R+JEGp0eLAP51b2+83Zz5dW8G93+6QQFIRERsFH6kQeoZFcjMP/TBy82ZlXuO8cfPNlJUqgAkIiImh58ZM2bQpUsX/Pz88PPzIy4ujgULFpzz+g8//JArrriCgIAAAgICiI+PZ926dZWuGTNmDBaLpdIxePDguv4oUg/1iQ7kkzG98XB1YvnvRxn3700KQCIiYm74ad68OdOmTWPjxo1s2LCBa6+9lmHDhrF9+/Yqr1+2bBkjRozg559/ZvXq1URGRjJw4EAOHz5c6brBgweTkpJiO7744gt7fByph/q1CuKT0b1xd3Hip13pjP98M8Wl5WaXJSIiJrIYhmGYXcTpAgMDef311xk7duwFry0rKyMgIIB3332XUaNGAdaen6ysLObNm3fJNeTk5ODv7092djZ+fn6X/D5Sf6zcc5Sx/9pAcWk513UI4a07u+Hj7mJ2WSIiUouq+/u73sz5KSsrY86cOeTn5xMXF1et1xQUFFBSUkJgYGCl88uWLSM4OJjY2FjGjRtHRkbGed+nqKiInJycSoc0LFfENOPDUb1wc3Zi8Y40bvz7LyQcyja7LBERMYHpPT8JCQnExcVRWFiIj48Ps2fP5vrrr6/Wax966CEWLlzI9u3b8fDwAGDOnDl4eXkRHR3Nvn37ePrpp/Hx8WH16tU4OztX+T4vvPACL7744lnn1fPT8Kw/mMmjX2zmSHYhrs4Wnhrcjnsvi8bJyWJ2aSIiUkPV7fkxPfwUFxeTlJREdnY2X3/9NR999BHLly+nQ4cO533dtGnTeO2111i2bBldunQ553X79++ndevWLFmyhAEDBlR5TVFREUVFRbbHOTk5REZGKvw0UFkFxTz1399YuD0NgKvaNuON27vSzNfd5MpERKQmHCb8nCk+Pp7WrVvz/vvvn/OaN954gylTprBkyRJ69ep1wfds1qwZU6ZM4Y9//GO1atCcn4bPMAw+X5vES/N3UFRaTlMfd966sytXxDQzuzQREblEDjfnp0J5eXmlXpgzvfbaa7z00kv8+OOP1Qo+hw4dIiMjg7CwsNosUxycxWLh7n5RfPfw5bQN8eFYXhH3fLyOqQt26m4wEZEGztTwM3HiRFasWMHBgwdJSEhg4sSJLFu2jJEjRwIwatQoJk6caLv+1VdfZdKkSXzyySe0bNmS1NRUUlNTycvLAyAvL48nnniCNWvWcPDgQZYuXcqwYcNo06YNgwYNMuUzSv0WG+rLdw9fzt39WgDw/vL93P7eKhIz8k2uTERE6oqp4Sc9PZ1Ro0YRGxvLgAEDWL9+PQsXLuS6664DICkpiZSUFNv1M2bMoLi4mNtuu42wsDDb8cYbbwDg7OzMb7/9xk033UTbtm0ZO3YsPXv2ZOXKlbi7az6HVM3D1Zkpwzvz3t098fd0ZeuhbIZO/4V5mw9f+MUiIuJw6t2cn/pAc34ar8NZJ/jznC2sO5gJwC09Ipg8rJPWBBIRcQAOO+dHxEwRTTyZfX9fJsTH4GSBbzYd1ppAIiINjMKPyBlcnJ2YEN+WOQ/EEe7vwYFj+dwy41c+Wrmf8nJ1lIqIODqFH5Fz6BMdyA+PXsGgjiGUlBlM+X4nf5i1nqO5574bUURE6j+FH5HzaOLlxnt392TK8E64u1h3hx/yzkpW7jlqdmkiInKJFH5ELkBrAomINCwKPyLVpDWBREQaBoUfkYugNYFERByfwo/IJRjcKZQfHr2CPi0DySsqZcJ/tvCXL7eQV1RqdmkiInIBCj8il0hrAomIOCaFH5Ea0JpAIiKOR+FHpBZoTSAREceh8CNSS7QmkIiIY1D4EalFWhNIRKT+U/gRqQNaE0hEpP5S+BGpI+daE2ju5kMYhiZDi4iYReFHpI6duSbQn/+zlREfruG3Q1lmlyYi0igp/IjYQcWaQH+Ob4ubixNr9mdy07u/8qcvNpOcWWB2eSIijYrFUP/7WXJycvD39yc7Oxs/Pz+zy5EG5nDWCd5ctJu5mw9jGODqbGFUXEsevqYNAd5uZpcnIuKwqvv7W+GnCgo/Yg/bDmczbcEuftl7DABfDxfGX9OGMf1b4uHqbHJ1IiKOR+GnBhR+xJ5W/H6UV37Yya7UXADC/T14fFAsw7tF4ORkMbk6ERHHofBTAwo/Ym9l5QZzNx/mzUW7SckuBKBDmB8Tr2/HFTHNTK5ORMQxKPzUgMKPmKWwpIyZvx7knz/vJffkDvFXxDRl4pD2dAjXf4siIuej8FMDCj9itsz8Yt79aS+frTlISZmBxQK3dG/OYwPbEt7E0+zyRETqJYWfGlD4kfoiKaOA1xft5n9bjwDg5uLEvZdFM+7q1vh7uppcnYhI/aLwUwMKP1LfbEnO4pUfdrLuQCYAAV6uPHJtDHf3i8LNRct1iYiAwk+NKPxIfWQYBj/tSmfqgl3sTc8DoEWgF08MiuWGLmFYLLozTEQaN4WfGlD4kfqstKycrzYe4m+Lf+dobhEAXZv7M/H69vRrFWRydSIi5lH4qQGFH3EEBcWlfLTyAO8v30d+cRkA8e2DeWpwO2JCfE2uTkTE/hR+akDhRxzJ0dwipi/dw+x1SZSVGzhZ4M7ekfw5vi3Bfh5mlyciYjcKPzWg8COOaN/RPF77cRcLt6cB4OnqzP1XRPPAVa3xcXcxuToRkbqn8FMDCj/iyDYczOSVH3ayKSkLgKY+bjwa35a7ekfi6qw7w0Sk4aru729TfxLOmDGDLl264Ofnh5+fH3FxcSxYsOC8r/nqq69o164dHh4edO7cmR9++KHS84Zh8NxzzxEWFoanpyfx8fHs2bOnLj+GSL3Sq2Ug/x3Xn/fu7kF0U2+O5RUzad42Br21gh+3paJ/74hIY2dq+GnevDnTpk1j48aNbNiwgWuvvZZhw4axffv2Kq9ftWoVI0aMYOzYsWzevJnhw4czfPhwtm3bZrvmtddeY/r06bz33nusXbsWb29vBg0aRGFhob0+lojpLBYLgzuFsejPVzJ5WEeCvN3YfyyfB/+9kdveW83Pu9MpL1cIEpHGqd4NewUGBvL6668zduzYs5678847yc/PZ/78+bZz/fr1o1u3brz33nsYhkF4eDiPPfYYjz/+OADZ2dmEhIQwa9Ys7rrrrirbLCoqoqioyPY4JyeHyMhIDXtJg5FbWMIHK/bz4cr9FJaUAxDd1JvRcVHc1itSc4JEpEFwiGGv05WVlTFnzhzy8/OJi4ur8prVq1cTHx9f6dygQYNYvXo1AAcOHCA1NbXSNf7+/vTt29d2TVWmTp2Kv7+/7YiMjKyFTyRSf/h6uPLYwFiWPX4NYy+PxtfdhQPH8nnhfzvo98pSXvzfdg4eyze7TBERuzA9/CQkJODj44O7uzsPPvggc+fOpUOHDlVem5qaSkhISKVzISEhpKam2p6vOHeua6oyceJEsrOzbUdycnJNPpJIvRXq78GkGzqw5ukBvDSsI62aeZNXVMrMXw9yzZvLuHfWelb8flTzgkSkQTO9rzs2NpYtW7aQnZ3N119/zejRo1m+fPk5A1BdcHd3x93d3W7tiZjN292Fe+JaMrJvFCv3HmPWrwf4efdRftqVzk+70mndzJsx/VtyS4/meGtITEQaGNN7ftzc3GjTpg09e/Zk6tSpdO3alXfeeafKa0NDQ0lLS6t0Li0tjdDQUNvzFefOdY2InOLkZOGqts2Y+Yc+/Pz41Yzp3xIfdxf2Hc1n0rfb6Td1KVPm7yApo8DsUkVEao3p4edM5eXllSYfny4uLo6lS5dWOrd48WLbHKHo6GhCQ0MrXZOTk8PatWvPOY9IRKyim3rzwk0dWT3xWp6/sQMtg7zILSzlo18OcNUbP3Pfvzawau8xDYmJiMMztT974sSJDBkyhBYtWpCbm8vs2bNZtmwZCxcuBGDUqFFEREQwdepUAB599FGuuuoq3nzzTYYOHcqcOXPYsGEDH3zwAWC9vXfChAlMmTKFmJgYoqOjmTRpEuHh4QwfPtysjyniUHw9XPnDZdGMjmvJ8t+PMnPVQVb8fpQlO9NYsjONtiE+jOkfzc3dI/B0cza7XBGRi2Zq+ElPT2fUqFGkpKTg7+9Ply5dWLhwIddddx0ASUlJODmd6pzq378/s2fP5tlnn+Xpp58mJiaGefPm0alTJ9s1Tz75JPn5+TzwwANkZWVx+eWX8+OPP+LhoT2ORC6Gk5OFa9oFc027YPam5/GvVQf576ZD/J6Wx9NzE3j1x13c1TuSe+KiaB7gZXa5IiLVVu/W+akPtL2FSNWyT5Tw1YZk/rX6IMmZJwBwssDADqGMuawlfaMDsVgsJlcpIo2V9vaqAYUfkfMrKzf4eVc6M1cd4Ne9Gbbz7UJ9+cNlLRnWLQIPVw2JiYh9KfzUgMKPSPX9npbLrFUH+WbTIdvq0QFerozo04K7+0UR3sTT5ApFpLFQ+KkBhR+Ri5dVUMx/1ifz6epEDmdZh8ScnSwM7mgdEusVFaAhMRGpUwo/NaDwI3LpSsvKWbIznVmrDrBmf6btfKcIP8b0j+aGLmEaEhOROqHwUwMKPyK1Y2dKDrN+Pci8LYcpKrUOiQV5u3Frz+YM7xZB+zBf9QaJSK1R+KkBhR+R2pWZX8yc9Ul8tjqRlOxC2/nYEF+GdQ9nWLcIIjQ3SERqSOGnBhR+ROpGxZDYvM2H+WlXOsVl5bbn+kYHMrx7BNd3CsPfy9XEKkXEUSn81IDCj0jdyy4oYcG2FOZuPszaA6fmBrk5O3Ftu2CGdw/nmnbBuLtofpCIVI/CTw0o/IjY1+GsE3y35QhzN1tXkK7g5+HC0C5hDO8WQe+WgTg5aX6QiJybwk8NKPyImMMwDHam5PLtlsN8u+UIqTmn5gdFNPHkpm7h3Nw9grYhviZWKSL1lcJPDSj8iJivrNxg7f4M5m05zIKEVHKLSm3PtQ/z4+bu4dzUNYJQf+3bJyJWCj81oPAjUr8UlpSxdGc687YcZtnudErKrD+2LBbo3zqIYd0iGNIpFF8PTZQWacwUfmpA4Uek/jqeX8z3CSl8u+Uw6w8et513d3EivkMIw7tFcFXbZri5OJlYpYiYQeGnBhR+RBxDcmYB3209wjebDrHvaL7tfBMvV244OVG6p7bVEGk0FH5qQOFHxLEYhsH2IznM23yYb7ce4Whuke25yEBPhneLYFi3CNoE+5hYpYjUNYWfGlD4EXFcZeUGq/YdY+7mwyzclkp+cZntuc4R/gzvHsENXcII8dNEaZGGRuGnBhR+RBqGE8VlLN6ZxrzNh1nx+1FKy0/9uOsU4ceAdiEMaB9Mp3B/rSEk0gAo/NSAwo9Iw5ORV8T3CSnM23yYzclZnP6Tr5mvO9fGBjOgfTCXxzTFy83FvEJF5JIp/NSAwo9Iw3Y0t4hlu9NZujOdlXuOVhoac3NxIq5VEAPaB3Ntu2CaB3iZWKmIXAyFnxpQ+BFpPIpKy1h3IJOlO9NZuiuN5MwTlZ6PDfHl2vbBxLcPpltkAM4aHhOptxR+akDhR6RxMgyDvel5LN2Vzk8709mQmMlp04QI8HLlmthgrm0fzJVtm+GnRRVF6hWFnxpQ+BERgKyCYpb/fpQlO9NZvjudnMJTW2y4OFnoEx3Ite2CGdA+hOim3iZWKiKg8FMjCj8icqaSsnI2Jh7np13pLN2ZVmlRRYBWTb1tQahXywBcnbXCtIi9KfzUgMKPiFzIwWP51uGxXWms3Z9Z6TZ6Xw8XrmrbjAHtg7m6bTAB3m4mVirSeCj81IDCj4hcjJzCEn7Zc4wlO9NYtvsomfnFtuecLNCjRQAD2lvXFIoJ9tF2GyJ1ROGnBhR+RORSlZUbbEnO4qddaSzdmc6u1NxKzzcP8OSa2GD6tw6ib6sgAtUrJFJrFH5qQOFHRGrLoeMF/LwrnaW70lm1L4Pi0vJKz7cL9SWudRBxraxhyN9Td5CJXCqFnxpQ+BGRulBQXMove47x695jrNqXwZ70vErPO1mgY7i/LQz1jg7Ex12rTYtUl8JPDSj8iIg9HM0tYs3+DFbvz2DNvgz2H6t8B5mzk4XOEdYw1L91EL2iAvF0czapWpH6T+GnBhR+RMQMqdmFrNmfwap9x1i9P+Os1aZdnS10i2xCXKsg+rUOokeLADxcFYZEKjhE+Jk6dSrffPMNu3btwtPTk/79+/Pqq68SGxt7ztdcffXVLF++/Kzz119/Pd9//z0AY8aM4V//+lel5wcNGsSPP/5YrboUfkSkPjh0vIDV+071DB3JLqz0vJuLEz1aNKF/66bEtQ6ia/MmuLlofSFpvBwi/AwePJi77rqL3r17U1paytNPP822bdvYsWMH3t5Vr5aamZlJcfGp20gzMjLo2rUrH330EWPGjAGs4SctLY2ZM2farnN3dycgIKBadSn8iEh9YxgGSZnWMLTqZCA6mltU6RpPV2d6tQygX6sg4loH0SXCHxcttiiNSHV/f5s6k+7MnphZs2YRHBzMxo0bufLKK6t8TWBgYKXHc+bMwcvLi9tvv73SeXd3d0JDQ2u3YBERk1gsFqKCvIkK8uauPi0wDIN9R/NtvUJr9meQkV/Myj3HWLnnGAA+7i70bhlwcgJ1UzqE+2ljVhFMDj9nys7OBs4OOOfz8ccfc9ddd53VU7Rs2TKCg4MJCAjg2muvZcqUKQQFBVX5HkVFRRQVnfoXVE5OziVULyJiPxaLhTbBPrQJ9uGeflEYhsHvaXmsPjlfaM3+TLJPlPDz7qP8vPsoAH4eLvSJDqJfq0B6RgXQIdwPdxfNGZLGp95MeC4vL+emm24iKyuLX375pVqvWbduHX379mXt2rX06dPHdr6iNyg6Opp9+/bx9NNP4+Pjw+rVq3F2Pvt/9BdeeIEXX3zxrPMa9hIRR1VebrAjJcd6N9m+DNYdyCS3qLTSNW4uTnQK96N7iwC6t2hC9xYBhPt7aAVqcVgOMefndOPGjWPBggX88ssvNG/evFqv+eMf/8jq1av57bffznvd/v37ad26NUuWLGHAgAFnPV9Vz09kZKTCj4g0GKVl5Ww/ksPq/dYgtCU5q9I2HBWCfd1tQah7ZBM6N/fHy61eDRKInJNDzPmp8PDDDzN//nxWrFhR7eCTn5/PnDlzmDx58gWvbdWqFU2bNmXv3r1Vhh93d3fc3d0vum4REUfh4uxE18gmdI1swoNXtbZNoN6UdJzNSVlsTspiZ0oO6blFLNyexsLtaYB1raF2ob70OK13qGWQl3qHxKGZGn4Mw+CRRx5h7ty5LFu2jOjo6Gq/9quvvqKoqIi77777gtceOnSIjIwMwsLCalKuiEiDcfoE6pu7W//ReaK4jG1Hstl8MhBtSjpOWk4R24/ksP1IDp+tSQSgiZcr3SOb2IbLukY2wc9D23KI4zB12Ouhhx5i9uzZfPvtt5XW9vH398fT0xOAUaNGERERwdSpUyu99oorriAiIoI5c+ZUOp+Xl8eLL77IrbfeSmhoKPv27ePJJ58kNzeXhISEavXw6FZ3ERGrlOwTJ3uGrIHot8PZZ+1PZrFAm2Y+dG/R5GQPUQBtgn10Z5nYnUPM+TlXt+nMmTNta/ZcffXVtGzZklmzZtme3717N+3atWPRokVcd911lV574sQJhg8fzubNm8nKyiI8PJyBAwfy0ksvERISUq26FH5ERKpWXFrOzpQcaxhKtg6XJWUWnHWdj7sLXSP96R5p7R3qFtmEIB9NL5C65RDhp75S+BERqb5jeUVsScpic7K1d2hrchb5xWVnXRcV5GUbLusU4Ue7UD+8tXGr1CKFnxpQ+BERuXRl5QZ70nMrDZeduYM9WIfLWgZ50z7Mlw5hfnQI96NDmD8hfu6aUC2XROGnBhR+RERqV/aJEraeHCbbeiiLHUdySM0prPLaAC/Xk0HIGojah/nRupkPrtqqQy5A4acGFH5EROpeRl4RO1Ny2ZmSw46UHHYcyWHv0TzKys/+teTm7ETbUB86hFnDUIcwP9qH++kuM6lE4acGFH5ERMxRWFLGnrS8SoFoZ0rOWatTV4gM9KR9qF+lnqKIJp4aNmukFH5qQOFHRKT+MAyDQ8dPsP1I5UB0OOtEldf7ebhYe4dOC0Qxwb64uWjYrKFT+KkBhR8Rkfovu6DEGoZOC0R70nMpKTv715qLk3Uj2IpAFBvqS9sQX4J9Nbm6IVH4qQGFHxERx1RcWs7e9Dx2pFjD0I6TvUXZJ0qqvN7Pw4WYEF9ign1sX9uG+OqOMwel8FMDCj8iIg2HYRgcyS609Q7tOJLD7+m5JGYUVDm5GsDXw8UaiIJ9iQmxBqO2IT6E+mnX+/pM4acGFH5ERBq+otIyDhzL5/e0PPam5fJ7Wh570nM5eL5Q5O5CmxAfWw9Rm5Nfw/wViuoDhZ8aUPgREWm8ikrLOHisgN/TctmTnseek18PHsun9ByhyMfdhTbBp4WiEOvXcIUiu1L4qQGFHxEROVNxaTkHM/KtoehkL9GetDwOnCcUebs508Y2l+jUMFq4vydO2vi11in81IDCj4iIVFdxaTmJGfm2YbOKYHTgWH6Vd54BeLg60TLIm+im3rRsav0a3dSblkHeNPVxU2/RJaru72/tKCciIlIDbi5O1jvFQnyBMNv5krLTQlFaHr+n57I3LY/9x/IoLClnV2ouu1Jzz3o/X3cXWyCyfvUiuqkP0UHe+HtpRevaoJ6fKqjnR0RE6kpJWTmHjp/g4LF8Dpw8Dmbks/9oPkeyT3C+38oBXq6nQlGQN9HNvG09SN7u6s/QsFcNKPyIiIgZCkvKSM4sYP+xfA6eFooOZuSTllN03tcG+7rTsqk3rU6Go5ZB3rRq5k2LQC88XJ3t9AnMpWEvERERB+Ph6nzaEFpl+UWlHMzI5+CxAg4cy+PAsQIOZlh7jjLzi0nPLSI9t4h1BzIrvc5igXB/z5M9RtYhtJZBXrQI9KJ5gBeebo0jGJ1OPT9VUM+PiIg4kuwTJWcNo1V8n1tY9aawFZr5uhMZ4EmLQC8iA72IDDj5NdCTMH9PnB3orjQNe9WAwo+IiDQEhmGQmV98Vig6eKyA5OMFFwxGrs4Wwpt42nqJWpwMRS1OhqQmXq716s40DXuJiIg0chaLhSAfd4J83OnVMvCs57MLSkjKtAahpMwCkjOtXw8dP8Gh4wWUlBkkZhSQmFFQ5fv7urvQPNCrUs9RRUBqHlB/5xqp56cK6vkREZHGrqzcIC2n0BaKkjMLSD5+whaQ0nPPPwEbrJOwbcNpZ4SkED+PWh9S07BXDSj8iIiInF9hSRmHjheQnHmiUq9RRUDKKzr/kNpfh7Tjwata12pNGvYSERGROuPh6kybYF/aBJ99Z5phGGSdNaR24mTvUQGHj58gMsDLhKqtFH5ERESkVlksFgK83QjwdqNrZJOzni8tK8fMYSeFHxEREbErF2cnU9s3t3URERERO1P4ERERkUZF4UdEREQaFYUfERERaVQUfkRERKRRUfgRERGRRkXhR0RERBoVU8PP1KlT6d27N76+vgQHBzN8+HB279593tfMmjULi8VS6fDw8Kh0jWEYPPfcc4SFheHp6Ul8fDx79uypy48iIiIiDsLU8LN8+XLGjx/PmjVrWLx4MSUlJQwcOJD8/Pzzvs7Pz4+UlBTbkZiYWOn51157jenTp/Pee++xdu1avL29GTRoEIWFhXX5cURERMQBmLrC848//ljp8axZswgODmbjxo1ceeWV53ydxWIhNDS0yucMw+Dtt9/m2WefZdiwYQB8+umnhISEMG/ePO66667a+wAiIiLicOrVnJ/s7GwAAgMDz3tdXl4eUVFRREZGMmzYMLZv32577sCBA6SmphIfH2875+/vT9++fVm9enWV71dUVEROTk6lQ0RERBqmehN+ysvLmTBhApdddhmdOnU653WxsbF88sknfPvtt/z73/+mvLyc/v37c+jQIQBSU1MBCAkJqfS6kJAQ23Nnmjp1Kv7+/rYjMjKylj6ViIiI1Df1JvyMHz+ebdu2MWfOnPNeFxcXx6hRo+jWrRtXXXUV33zzDc2aNeP999+/5LYnTpxIdna27UhOTr7k9xIREZH6rV7s6v7www8zf/58VqxYQfPmzS/qta6urnTv3p29e/cC2OYCpaWlERYWZrsuLS2Nbt26Vfke7u7uuLu72x4bhgGg4S8REREHUvF7u+L3+LmYGn4Mw+CRRx5h7ty5LFu2jOjo6It+j7KyMhISErj++usBiI6OJjQ0lKVLl9rCTk5ODmvXrmXcuHHVes/c3FwADX+JiIg4oNzcXPz9/c/5vKnhZ/z48cyePZtvv/0WX19f25wcf39/PD09ARg1ahQRERFMnToVgMmTJ9OvXz/atGlDVlYWr7/+OomJidx3332A9U6wCRMmMGXKFGJiYoiOjmbSpEmEh4czfPjwatUVHh5OcnIyvr6+WCyWWvu8OTk5REZGkpycjJ+fX629ryPVoPYbd/v1oQa1r/8G1H7Dbd8wDHJzcwkPDz/vdaaGnxkzZgBw9dVXVzo/c+ZMxowZA0BSUhJOTqemJh0/fpz777+f1NRUAgIC6NmzJ6tWraJDhw62a5588kny8/N54IEHyMrK4vLLL+fHH388azHEc3Fycrro4beL4efnZ9oPnfpSg9pv3O3XhxrUvv4bUPsNs/3z9fhUMH3Y60KWLVtW6fFbb73FW2+9dd7XWCwWJk+ezOTJk2tSnoiIiDRA9eZuLxERERF7UPixI3d3d55//vlKd5Y1thrUfuNuvz7UoPb134Dab9ztA1iM6ow9iYiIiDQQ6vkRERGRRkXhR0RERBoVhR8RERFpVBR+REREpFFR+LGTFStWcOONNxIeHo7FYmHevHl2a3vq1Kn07t0bX19fgoODGT58OLt377Zb+2Bd0LJLly62Ra3i4uJYsGCBXWuoMG3aNNtK4PbywgsvYLFYKh3t2rWzW/sAhw8f5u677yYoKAhPT086d+7Mhg0b7NJ2y5Ytz/r8FouF8ePH26X9srIyJk2aRHR0NJ6enrRu3ZqXXnqpWmuN1abc3FwmTJhAVFQUnp6e9O/fn/Xr19dJWxf6mWMYBs899xxhYWF4enoSHx/Pnj177Nb+N998w8CBAwkKCsJisbBly5Zaa/tC7ZeUlPDUU0/RuXNnvL29CQ8PZ9SoURw5csRuNYD150K7du3w9vYmICCA+Ph41q5da7f2T/fggw9isVh4++237db+mDFjzvqZMHjw4Fpr/3wUfuwkPz+frl278o9//MPubS9fvpzx48ezZs0aFi9eTElJCQMHDiQ/P99uNTRv3pxp06axceNGNmzYwLXXXsuwYcPYvn273WoAWL9+Pe+//z5dunSxa7sAHTt2JCUlxXb88ssvdmv7+PHjXHbZZbi6urJgwQJ27NjBm2++SUBAgF3aX79+faXPvnjxYgBuv/12u7T/6quvMmPGDN5991127tzJq6++ymuvvcbf//53u7Rf4b777mPx4sV89tlnJCQkMHDgQOLj4zl8+HCtt3WhnzmvvfYa06dP57333mPt2rV4e3szaNAgCgsL7dJ+fn4+l19+Oa+++mqttHcx7RcUFLBp0yYmTZrEpk2b+Oabb9i9ezc33XST3WoAaNu2Le+++y4JCQn88ssvtGzZkoEDB3L06FG7tF9h7ty5rFmz5oJbQtRF+4MHD670s+GLL76o1RrOyRC7A4y5c+ea1n56eroBGMuXLzetBsMwjICAAOOjjz6yW3u5ublGTEyMsXjxYuOqq64yHn30Ubu1/fzzzxtdu3a1W3tneuqpp4zLL7/ctPbP9OijjxqtW7c2ysvL7dLe0KFDjXvvvbfSuVtuucUYOXKkXdo3DMMoKCgwnJ2djfnz51c636NHD+OZZ56p07bP/JlTXl5uhIaGGq+//rrtXFZWluHu7m588cUXdd7+6Q4cOGAAxubNm2u93eq0X2HdunUGYCQmJppWQ3Z2tgEYS5YssVv7hw4dMiIiIoxt27YZUVFRxltvvVXrbZ+r/dGjRxvDhg2rk/YuRD0/jVB2djYAgYGBprRfVlbGnDlzyM/PJy4uzm7tjh8/nqFDhxIfH2+3Nk+3Z88ewsPDadWqFSNHjiQpKclubX/33Xf06tWL22+/neDgYLp3786HH35ot/ZPV1xczL///W/uvffeWt04+Hz69+/P0qVL+f333wHYunUrv/zyC0OGDLFL+wClpaWUlZWdtcegp6enXXsBAQ4cOEBqamql/xf8/f3p27cvq1evtmst9UV2djYWi4UmTZqY0n5xcTEffPAB/v7+dO3a1S5tlpeXc8899/DEE0/QsWNHu7R5pmXLlhEcHExsbCzjxo0jIyPDLu2aureX2F95eTkTJkzgsssuo1OnTnZtOyEhgbi4OAoLC/Hx8WHu3LmVNqStS3PmzGHTpk11Nr/iQvr27cusWbOIjY0lJSWFF198kSuuuIJt27bh6+tb5+3v37+fGTNm8Je//IWnn36a9evX86c//Qk3NzdGjx5d5+2fbt68eWRlZdk2L7aHv/71r+Tk5NCuXTucnZ0pKyvj5ZdfZuTIkXarwdfXl7i4OF566SXat29PSEgIX3zxBatXr6ZNmzZ2qwMgNTUVgJCQkErnQ0JCbM81JoWFhTz11FOMGDHC7ht9zp8/n7vuuouCggLCwsJYvHgxTZs2tUvbr776Ki4uLvzpT3+yS3tnGjx4MLfccgvR0dHs27ePp59+miFDhrB69WqcnZ3rtG2Fn0Zm/PjxbNu2ze7/0gSIjY1ly5YtZGdn8/XXXzN69GiWL19e5wEoOTmZRx99lMWLF5/1r257Ob2HoUuXLvTt25eoqCi+/PJLxo4dW+ftl5eX06tXL1555RUAunfvzrZt23jvvffsHn4+/vhjhgwZUuvzC87nyy+/5PPPP2f27Nl07NiRLVu2MGHCBMLDw+36+T/77DPuvfdeIiIicHZ2pkePHowYMYKNGzfarQaprKSkhDvuuAPDMJgxY4bd27/mmmvYsmULx44d48MPP+SOO+5g7dq1BAcH12m7Gzdu5J133mHTpk1264E901133WX7vnPnznTp0oXWrVuzbNkyBgwYUKdta9irEXn44YeZP38+P//8M82bN7d7+25ubrRp04aePXsydepUunbtyjvvvFPn7W7cuJH09HR69OiBi4sLLi4uLF++nOnTp+Pi4kJZWVmd13CmJk2a0LZtW/bu3WuX9sLCws4Kme3bt7fr0BtAYmIiS5Ys4b777rNru0888QR//etfueuuu+jcuTP33HMPf/7zn5k6dapd62jdujXLly8nLy+P5ORk1q1bR0lJCa1atbJrHaGhoQCkpaVVOp+WlmZ7rjGoCD6JiYksXrzY7r0+AN7e3rRp04Z+/frx8ccf4+Liwscff1zn7a5cuZL09HRatGhh+7mYmJjIY489RsuWLeu8/aq0atWKpk2b2uXnosJPI2AYBg8//DBz587lp59+Ijo62uySAGtvRFFRUZ23M2DAABISEtiyZYvt6NWrFyNHjmTLli113r1alby8PPbt20dYWJhd2rvsssvOWt7g999/Jyoqyi7tV5g5cybBwcEMHTrUru0WFBTg5FT5x52zszPl5eV2raOCt7c3YWFhHD9+nIULFzJs2DC7th8dHU1oaChLly61ncvJyWHt2rV2nYdnporgs2fPHpYsWUJQUJDZJQH2+7l4zz338Ntvv1X6uRgeHs4TTzzBwoUL67z9qhw6dIiMjAy7/FzUsJed5OXlVUqzBw4cYMuWLQQGBtKiRYs6bXv8+PHMnj2bb7/9Fl9fX9uYvr+/P56ennXadoWJEycyZMgQWrRoQW5uLrNnz2bZsmV2+Z/M19f3rPlN3t7eBAUF2W3e0+OPP86NN95IVFQUR44c4fnnn8fZ2ZkRI0bYpf0///nP9O/fn1deeYU77riDdevW8cEHH/DBBx/YpX2w/lCfOXMmo0ePxsXFvj96brzxRl5++WVatGhBx44d2bx5M3/729+499577VrHwoULMQyD2NhY9u7dyxNPPEG7du34wx/+UOttXehnzoQJE5gyZQoxMTFER0czadIkwsPDGT58uF3az8zMJCkpyba2TkU4Dw0NrZXep/O1HxYWxm233camTZuYP38+ZWVltp+LgYGBuLm51bj9C9UQFBTEyy+/zE033URYWBjHjh3jH//4B4cPH661JSAu9HdwZuBzdXUlNDSU2NjYOm8/MDCQF198kVtvvZXQ0FD27dvHk08+SZs2bRg0aFCttH9eptxj1gj9/PPPBnDWMXr06Dpvu6p2AWPmzJl13naFe++914iKijLc3NyMZs2aGQMGDDAWLVpkt/bPZO9b3e+8804jLCzMcHNzMyIiIow777zT2Lt3r93aNwzD+N///md06tTJcHd3N9q1a2d88MEHdm1/4cKFBmDs3r3bru0ahmHk5OQYjz76qNGiRQvDw8PDaNWqlfHMM88YRUVFdq3jP//5j9GqVSvDzc3NCA0NNcaPH29kZWXVSVsX+plTXl5uTJo0yQgJCTHc3d2NAQMG1OrfzYXanzlzZpXPP//883XefsXt9VUdP//8c620f6EaTpw4Ydx8881GeHi44ebmZoSFhRk33XSTsW7dOru0X5XavtX9fO0XFBQYAwcONJo1a2a4uroaUVFRxv3332+kpqbWWvvnYzEMOy9xKiIiImIizfkRERGRRkXhR0RERBoVhR8RERFpVBR+REREpFFR+BEREZFGReFHREREGhWFHxEREWlUFH5ERESkUVH4ERGpBovFwrx588wuQ0RqgcKPiNR7Y8aMwWKxnHUMHjzY7NJExAFpY1MRcQiDBw9m5syZlc65u7ubVI2IODL1/IiIQ3B3d7ft+F1xBAQEANYhqRkzZjBkyBA8PT1p1aoVX3/9daXXJyQkcO211+Lp6UlQUBAPPPAAeXl5la755JNP6NixI+7u7oSFhfHwww9Xev7YsWPcfPPNeHl5ERMTw3fffVe3H1pE6oTCj4g0CJMmTeLWW29l69atjBw5krvuuoudO3cCkJ+fz6BBgwgICGD9+vV89dVXLFmypFK4mTFjBuPHj+eBBx4gISGB7777jjZt2lRq48UXX+SOO+7gt99+4/rrr2fkyJFkZmba9XOKSC2wy97xIiI1MHr0aMPZ2dnw9vaudLz88suGYRgGYDz44IOVXtO3b19j3LhxhmEYxgcffGAEBAQYeXl5tue///57w8nJyUhNTTUMwzDCw8ONZ5555pw1AMazzz5re5yXl2cAxoIFC2rtc4qIfWjOj4g4hGuuuYYZM2ZUOhcYGGj7Pi4urtJzcXFxbNmyBYCdO3fStWtXvL29bc9fdtlllJeXs3v3biwWC0eOHGHAgAHnraFLly627729vfHz8yM9Pf1SP5KImEThR0Qcgre391nDULXF09OzWte5urpWemyxWCgvL6+LkkSkDmnOj4g0CGvWrDnrcfv27QFo3749W7duJT8/3/b8r7/+ipOTE7Gxsfj6+tKyZUuWLl1q15pFxBzq+RERh1BUVERqamqlcy4uLjRt2hSAr776il69enH55Zfz+eefs27dOj7++GMARo4cyfPPP8/o0aN54YUXOHr0KI888gj33HMPISEhALzwwgs8+OCDBAcHM2TIEHJzc/n111955JFH7PtBRaTOKfyIiEP48ccfCQsLq3QuNjaWXbt2AdY7sebMmcNDDz1EWFgYX3zxBR06dADAy8uLhQsX8uijj9K7d2+8vLy49dZb+dvf/mZ7r9GjR1NYWMhbb73F448/TtOmTbntttvs9wFFxG4shmEYZhchIlITFouFuXPnMnz4cLNLEREHoDk/IiIi0qgo/IiIiEijojk/IuLwNHovIhdDPT8iIiLSqCj8iIiISKOi8CMiIiKNisKPiIiINCoKPyIiItKoKPyIiIhIo6LwIyIiIo2Kwo+IiIg0Kv8PlSxHl0UPb+cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "x = np.arange(1, len(train_losses) + 1)\n",
    "plt.plot(x, train_losses, label='Train loss')\n",
    "plt.plot(x, valid_losses, label='Valid loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11461f",
   "metadata": {
    "id": "ea11461f"
   },
   "source": [
    "### 3.4 Translate French to English (15 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fc85ae8",
   "metadata": {
    "id": "8fc85ae8"
   },
   "outputs": [],
   "source": [
    "sos_token_id = en_tokenizer.token_to_id('<s>')\n",
    "eos_token_id = en_tokenizer.token_to_id('</s>')\n",
    "max_pred_len = 200\n",
    "def translate(encoder: 'Encoder', decoder: 'Decoder', fr_sentences: List[List[int]]):\n",
    "    \"\"\" Translate the src (French) sentences to English sentences.\n",
    "        This is a recursive translation.\n",
    "\n",
    "    Args:\n",
    "        encoder: The encoder part in seq2seq\n",
    "        decoder: The decoder part in seq2seq\n",
    "        fr_sentences: The src token ids of all sentences\n",
    "    Returns:\n",
    "        pred_sentences: The predicted string sentences\n",
    "    \"\"\"\n",
    "    n = len(fr_sentences)\n",
    "    pred_sentences = []\n",
    "    for i, src_ids in enumerate(fr_sentences):\n",
    "        print_line(f'{i + 1} / {n}')\n",
    "        # Shape of src_ids: (1 x seq_len)\n",
    "        src_ids = tf.expand_dims(tf.convert_to_tensor(src_ids, dtype=tf.int64), axis=0)\n",
    "        # pred is the prediction token ids. It starts with <s>\n",
    "        pred = [sos_token_id]\n",
    "        # Start your code here\n",
    "        # Step 1. Calculate the encoder outputs and hidden states (similar to seq2seq2 model)\n",
    "        # Step 2. Run a while loop when the last token in pred is not eos_token_id and the length of pred is less than max_pred_len\n",
    "        # Step 3.     In the while loop, build the input (cur_token) of decoder: the last token of pred. Shape (batch_size, ) -> (1, )\n",
    "        #             For example, if the current pred is [1, 50, 21, 8], the cur_token is [8]\n",
    "        # Step 4.     In the while loop, use decoder.predict to get the decoder output\n",
    "        # Step 5.     In the while loop, find the index with the maximum value. Then you can call tf.squeeze and numpy() to get the index\n",
    "        # Step 6.     In the while loop, append the predicted token to pred\n",
    "        # Step 7. Use en_tokenizer to decode the id to strings: pred_sentence\n",
    "        enc_output, enc_hidden = encoder(src_ids, None)\n",
    "        dec_hidden = enc_hidden\n",
    "        while pred[-1] != eos_token_id and len(pred) < max_pred_len:\n",
    "    #         # Prepare input for decoder\n",
    "            cur_token = np.array([pred[-1]])  # Shape: (1,)\n",
    "    #         # Get decoder output for the current token\n",
    "            dec_output, dec_hidden = decoder.predict(tf.expand_dims(cur_token, axis=0), dec_hidden)\n",
    "    #         # Get index of predicted token with maximum probability\n",
    "            predicted_index = int(tf.argmax(dec_output[0], axis=-1)[-1])\n",
    "    #         # Append predicted token to pred\n",
    "            pred.append(predicted_index)\n",
    "    #     # Decode predicted sentence using English tokenizer\n",
    "        pred_sentence = en_tokenizer.decode(pred)\n",
    "    #     # End\n",
    "        pred_sentences.append(pred_sentence)\n",
    "    print_line('\\n')\n",
    "    return pred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "LTG_p-1040eP",
   "metadata": {
    "id": "LTG_p-1040eP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('my_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C3hxOH6X_D1L",
   "metadata": {
    "id": "C3hxOH6X_D1L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "761c0472",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "761c0472",
    "outputId": "d48f4a87-bf64-4dea-a538-ace398326b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8597 / 8597\n"
     ]
    }
   ],
   "source": [
    "test_pred = translate(model.encoder, model.decoder, fr_sentences=test_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aae118",
   "metadata": {
    "id": "50aae118"
   },
   "source": [
    "### 3.5 Demonstrate 10 translation examples (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c91c9b",
   "metadata": {
    "id": "d7c91c9b"
   },
   "source": [
    "### 3.6 Compute the bleu score (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MEblPzOvdHAc",
   "metadata": {
    "id": "MEblPzOvdHAc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "m7P2_-7gRa_f",
   "metadata": {
    "id": "m7P2_-7gRa_f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8597"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "-Zs-IJrAdWel",
   "metadata": {
    "id": "-Zs-IJrAdWel"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8023 4498 8365 8128 2181 3545 2530 6473 4809 1616]\n",
      "French: les bonnes équipes envoient ces informations de façon à ce que les joueurs puissent s'en servir.\n",
      "Ground Truth English: the good teams stream it in a way that the players can use.\n",
      "Translated English:  the teams that are sending information information to the service of the work they could be.\n",
      "------------------\n",
      "French: merci.\n",
      "Ground Truth English: thank you.\n",
      "Translated English:  thank you.\n",
      "------------------\n",
      "French: il y a eu plusieurs cas où c'était vraiment juste.\n",
      "Ground Truth English: there have been several close calls.\n",
      "Translated English:  there were many cases where it was very good.\n",
      "------------------\n",
      "French: mes prières vous accompagnent dans votre combat.\n",
      "Ground Truth English: my prayers are with you for your fight.\n",
      "Translated English:  your prayers are now beaten up in your head.\n",
      "------------------\n",
      "French: et la question était : comment la technologie pourrait, les nouvelles technologies, y être ajoutée ?\n",
      "Ground Truth English: and the question was: how could technology, new technology,  be added to that?\n",
      "Translated English:  and the question was, how technology could be technology, new technologies?\n",
      "------------------\n",
      "French: combien d'entre vous ont vu l'ordinateur watson d'ibm gagner à jeopardy ?\n",
      "Ground Truth English: i mean, how many of you saw the winning of jeopardy  by ibm's watson?\n",
      "Translated English:  how many of you have seen the computer-wazi ratio of a table?\n",
      "------------------\n",
      "French: j'ai travaillé dans une mine de charbon -- dangereux.\n",
      "Ground Truth English: i worked in a coal mine --  dangerous.\n",
      "Translated English:  i've been working on a coal mine -- a penguin.\n",
      "------------------\n",
      "French: n'importe qui d'autre l'aimerait aussi.\n",
      "Ground Truth English: somebody else would love about this woman.\n",
      "Translated English:  all else would feel like it.\n",
      "------------------\n",
      "French: c'est tragique que les nord-coréens aient à cacher leurs identités et affronter tant de choses seulement pour survivre.\n",
      "Ground Truth English: it's tragic that north koreans have to hide their identities  and struggle so hard just to survive.\n",
      "Translated English:  it's bad for the prisoners of the syrian refugees and the obstacles to survive and to be able to stay in.\n",
      "------------------\n",
      "French: la glace que je photographie dans les icebergs est parfois très jeune -- deux milles ans.\n",
      "Ground Truth English: some of the ice in the icebergs that i photograph is very young --  a couple thousand years old.\n",
      "Translated English:  the ice that i see in the earliest couple of weeks are very young, very, very different from the star.\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(6666)\n",
    "sample_num = 10\n",
    "# Start your code here\n",
    "# Use np.random.choice to sample 10 sentence indices. Remember to set correct replace\n",
    "# Print format:\n",
    "# 1.\n",
    "# French: ...\n",
    "# True English: ...\n",
    "# Translated English: ...\n",
    "# ------------------\n",
    "sampled_indices = np.random.choice(len(test_pred), size=sample_num, replace=False)\n",
    "# Print the format for each sampled sentence\n",
    "print(sampled_indices)\n",
    "for idx in sampled_indices:\n",
    "    print(\"French:\", test_fr_sentences[idx])\n",
    "    print(\"Ground Truth English:\", test_en_sentences[idx])\n",
    "    print(\"Translated English:\", test_pred[idx])\n",
    "    print(\"------------------\")\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb4e81-9837-4938-8eee-46c6f6eb6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_samples = len(train_fr)\n",
    "n_valid_batch = int(np.ceil(len(valid_fr) / batch_size))\n",
    "pad_token_id = fr_tokenizer.token_to_id('<pad>')\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, data_index in enumerate(train_batch_sampler):\n",
    "        src_batch, src_seq_lens = train_fr[data_index], train_seq_lens_fr[data_index]\n",
    "\n",
    "        tgt_batch, tgt_seq_lens = train_en[data_index], train_seq_lens_en[data_index]\n",
    "        real_batch_size = len(src_batch)\n",
    "        (src_batch, src_seq_lens_batch,\n",
    "         tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch) = pad_batch(src_batch, src_seq_lens,\n",
    "                                                                   tgt_batch, tgt_seq_lens,\n",
    "                                                                   pad_val=pad_token_id)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # print(src_batch[0])\n",
    "            # print()\n",
    "            output = model(src_batch, src_seq_lens_batch, tgt_x_batch, tgt_seq_lens_batch)\n",
    "            loss = seq2seq_loss(output, tgt_y_batch, tgt_seq_lens_batch)\n",
    "\n",
    "        print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {len(train_batch_sampler)} - loss: {loss:.4f}')\n",
    "\n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        epoch_loss += loss * real_batch_size\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    for batch_idx in range(n_valid_batch):\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        src_batch, src_seq_lens = valid_fr[start:end], valid_seq_lens_fr[start:end]\n",
    "        tgt_batch, tgt_seq_lens = valid_en[start:end], valid_seq_lens_en[start:end]\n",
    "        real_batch_size = len(src_batch)\n",
    "        (src_batch, src_seq_lens_batch,\n",
    "         tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch) = pad_batch(src_batch, src_seq_lens,\n",
    "                                                                   tgt_batch, tgt_seq_lens,\n",
    "                                                                   pad_val=pad_token_id)\n",
    "        output = model(src_batch, src_seq_lens_batch, tgt_x_batch, tgt_seq_lens_batch, training=False)\n",
    "        loss = seq2seq_loss(output, tgt_y_batch, tgt_seq_lens_batch)\n",
    "\n",
    "        if batch_idx % 1 == 0 or batch_idx == len(valid_en) - 1:\n",
    "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_valid_batch} - loss: {loss:.4f}')\n",
    "\n",
    "        valid_loss += loss * real_batch_size\n",
    "    train_epoch_loss = epoch_loss / n_training_samples\n",
    "    valid_epoch_loss = valid_loss / len(valid_en)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    valid_losses.append(valid_epoch_loss)\n",
    "    print(f'\\rEpoch {epoch + 1} / {num_epoch} - Step {len(train_batch_sampler)} / {len(train_batch_sampler)} - train loss: {train_epoch_loss:.4f} - valid loss: {valid_epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d07c2cf5",
   "metadata": {
    "id": "d07c2cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.14\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "sacrebleu = evaluate.load('sacrebleu', cache_dir=dataset_path)\n",
    "# Start your code here\n",
    "# see https://huggingface.co/spaces/evaluate-metric/sacrebleu\n",
    "# Note: please understand the format and meaning of references.\n",
    "results = sacrebleu.compute(predictions=test_pred, references=test_en_sentences)\n",
    "# End\n",
    "# print(score)\n",
    "score = results['score']\n",
    "print(round(score, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118a02c",
   "metadata": {
    "id": "1118a02c"
   },
   "source": [
    "If you implement everything correctly, the BLEU score will be around 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5471d",
   "metadata": {
    "id": "18e5471d"
   },
   "source": [
    "## Conclusion (5 Points)\n",
    "\n",
    "Including but not limited to: translation example analysis (case study), bleu score analysis, model structure / parameter analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9537daf",
   "metadata": {
    "id": "d9537daf"
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E7zPJPNtGq1e",
   "metadata": {
    "id": "E7zPJPNtGq1e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
