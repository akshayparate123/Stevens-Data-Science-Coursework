{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba6cfda",
   "metadata": {
    "id": "8ba6cfda"
   },
   "source": [
    "# CS 584 Assignment 4 -- Sequence to Sequence Models\n",
    "\n",
    "#### Name: Akshay Parate\n",
    "#### Stevens ID: 20032008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "u-3mMwGSEE7K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-3mMwGSEE7K",
    "outputId": "78275ed1-9d7a-4635-ce27-f2811dc52f69"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZsvuIwI-EFnt",
   "metadata": {
    "id": "ZsvuIwI-EFnt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b106027",
   "metadata": {
    "id": "6b106027"
   },
   "source": [
    "## In this assignment, you are required to follow the steps below:\n",
    "1. Review the lecture slides.\n",
    "2. Implement the seq2seq (translation) model.\n",
    "\n",
    "**Before you start**\n",
    "- Please read the code very carefully.\n",
    "- Install these packages using the following command.\n",
    "```console\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
    "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
    "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951ed7ae-2ed9-44a8-b253-e06d4a567c66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "951ed7ae-2ed9-44a8-b253-e06d4a567c66",
    "outputId": "0a72cad0-af35-478c-8465-fc8282f0d19c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f9a9bad",
   "metadata": {
    "id": "9f9a9bad"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def print_line(*args):\n",
    "    \"\"\" Inline print and go to the begining of line\n",
    "    \"\"\"\n",
    "    args1 = [str(arg) for arg in args]\n",
    "    str_ = ' '.join(args1)\n",
    "    print('\\r' + str_, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffd6958",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ffd6958",
    "outputId": "a5c62f06-2923-4973-87ca-5dc937f62def"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "# If you are going to use GPU, make sure the GPU in in the output\n",
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d226bd",
   "metadata": {
    "id": "63d226bd"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union, Dict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441930a",
   "metadata": {
    "id": "9441930a"
   },
   "source": [
    "## 1. Data preparation (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0745adba",
   "metadata": {
    "id": "0745adba"
   },
   "source": [
    "### 1.1 Load and describe data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb159ef8",
   "metadata": {
    "id": "eb159ef8"
   },
   "source": [
    "Here, we use the [iwslt2017](https://huggingface.co/datasets/iwslt2017) dataset. More specifically, this translation task is from French to English: fr-en."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2326b71",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2326b71",
    "outputId": "01460f68-19d2-4a45-e25a-5439113d228e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\datasets\\load.py:2524: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# The load_dataset function is provided by the huggingface datasets\n",
    "# https://huggingface.co/docs/datasets/index\n",
    "\n",
    "\n",
    "dataset_path = os.path.join('a4-data', 'dataset')\n",
    "dataset = load_dataset('iwslt2017', 'iwslt2017-en-fr', cache_dir=dataset_path, ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da78bb3",
   "metadata": {
    "id": "4da78bb3"
   },
   "source": [
    "Let's first print some basic statistics of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0f96c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0f96c96",
    "outputId": "19a47d2b-6d6e-4303-81c2-c212c7778974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 232825\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 8597\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 890\n",
      "    })\n",
      "})\n",
      "232825 890 8597\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(len(dataset['train']['translation']), len(dataset['validation']['translation']), len(dataset['test']['translation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3r_Y0ruDVUSL",
   "metadata": {
    "id": "3r_Y0ruDVUSL"
   },
   "outputs": [],
   "source": [
    "# dataset['train']['translation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e65f0a1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e65f0a1d",
    "outputId": "109d9bf6-16cf-4441-eb3a-f7d2d1528c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'fr': \"Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['translation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1471b654",
   "metadata": {
    "id": "1471b654"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "# The tokenizer is provided by the huggingface tokenizers\n",
    "# https://huggingface.co/docs/tokenizers/index\n",
    "# Here, I already pretrained a BPE tokenizer and you can simply load the json\n",
    "# The token numbers of both English and French are 10,000\n",
    "# All tokens should be lower-case.\n",
    "# en_tokenizer = Tokenizer.from_file('./drive/MyDrive/a4-data/en_tokenizer.json')\n",
    "# fr_tokenizer = Tokenizer.from_file('./drive/MyDrive/a4-data/fr_tokenizer.json')\n",
    "en_tokenizer = Tokenizer.from_file('./a4-data/en_tokenizer.json')\n",
    "fr_tokenizer = Tokenizer.from_file('./a4-data/fr_tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ade73b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ade73b8",
    "outputId": "41340c6f-cd3d-475b-fb31-6d19fc9416bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 122, 279, 4987, 17, 1]\n",
      "['<s>', 'Ġi', 'Ġlike', 'Ġsports', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "encoding = en_tokenizer.encode(\"i like sports.\")\n",
    "print(encoding.ids)\n",
    "print(encoding.tokens)\n",
    "# >>> [0, 122, 279, 4987, 17, 1]\n",
    "# >>> ['<s>', 'Ġi', 'Ġlike', 'Ġsports', '.', '</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd9b4c",
   "metadata": {
    "id": "80dd9b4c"
   },
   "source": [
    "Extract English and French sentences for training, validation, and test sets.\n",
    "\n",
    "Note: Every sentence is lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1608b13",
   "metadata": {
    "id": "a1608b13"
   },
   "outputs": [],
   "source": [
    "train_en_sentences, train_fr_sentences = zip(*[(pair['en'].lower(), pair['fr'].lower()) for pair in dataset['train']['translation']])\n",
    "valid_en_sentences, valid_fr_sentences = zip(*[(pair['en'].lower(), pair['fr'].lower()) for pair in dataset['validation']['translation']])\n",
    "test_en_sentences, test_fr_sentences = zip(*[(pair['en'].lower(), pair['fr'].lower()) for pair in dataset['test']['translation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Jw5Qk7fcV8_K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Jw5Qk7fcV8_K",
    "outputId": "a320427f-b924-4298-ed90-18aeb7e56b9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'several years ago here at ted, peter skillman  introduced a design challenge  called the marshmallow challenge.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "NxJsfIMjWJM1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "NxJsfIMjWJM1",
    "outputId": "ba454fde-71e4-4ee0-9d0c-26bace2782b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"il y a plusieurs années, ici à ted, peter skillman a présenté une épreuve de conception appelée l'épreuve du marshmallow.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fr_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486e49b",
   "metadata": {
    "id": "4486e49b"
   },
   "source": [
    "### 1.2 Encode data (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ccb3e5d",
   "metadata": {
    "id": "9ccb3e5d"
   },
   "outputs": [],
   "source": [
    "def encode(tokenizer: 'Tokenizer', sentences: List[str]) -> List[List[int]]:\n",
    "    \"\"\" Encode the sentences with the pretrained tokenizer.\n",
    "        You can directly call `tokenizer.encode()` to encode the sentences.\n",
    "        It will automatically add the <s> and </s> token.\n",
    "\n",
    "        Note: Please be carefull with the return value of the encode function.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A pretrained en/fr tokenizer\n",
    "        sentences: A list of strings\n",
    "    Return:\n",
    "        sent_token_ids: A list of token ids\n",
    "    \"\"\"\n",
    "    sent_token_ids = []\n",
    "    n = len(sentences)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i % 100 == 0 or i == n - 1:\n",
    "            print_line('Encoding with Tokenizer:', (i + 1), '/', n)\n",
    "        # Start your code here\n",
    "        sent_token_ids.append(tokenizer.encode(sentence).ids)\n",
    "        # End\n",
    "    print_line('\\n')\n",
    "    return sent_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "771ab620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "771ab620",
    "outputId": "8987015d-70e8-4fdf-9de7-a2ff48228a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "Encoding with Tokenizer: 232825 / 232825\n",
      "Encoding with Tokenizer: 890 / 890\n",
      "Encoding with Tokenizer: 8597 / 8597\n",
      "fr\n",
      "Encoding with Tokenizer: 232825 / 232825\n",
      "Encoding with Tokenizer: 890 / 890\n",
      "Encoding with Tokenizer: 8597 / 8597\n"
     ]
    }
   ],
   "source": [
    "print('en')\n",
    "train_en = encode(en_tokenizer, train_en_sentences)\n",
    "valid_en = encode(en_tokenizer, valid_en_sentences)\n",
    "test_en = encode(en_tokenizer, test_en_sentences)\n",
    "print('fr')\n",
    "train_fr = encode(fr_tokenizer, train_fr_sentences)\n",
    "valid_fr = encode(fr_tokenizer, valid_fr_sentences)\n",
    "test_fr = encode(fr_tokenizer, test_fr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mqNpGXj-RvbX",
   "metadata": {
    "id": "mqNpGXj-RvbX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e910335c",
   "metadata": {
    "id": "e910335c"
   },
   "source": [
    "Check your implementation with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "763287e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "763287e4",
    "outputId": "ac01eea5-e544-4841-f800-526fd96db44c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': \"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\", 'fr': \"Merci beaucoup, Chris. C'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. Je suis très reconnaissant.\"}\n",
      "[0, 658, 162, 188, 494, 15, 2843, 17, 138, 165, 178, 2775, 121, 630, 4502, 140, 222, 124, 1930, 140, 625, 140, 185, 2122, 3446, 30, 122, 400, 2576, 5818, 17, 1] [0, 763, 478, 15, 3016, 17, 145, 10, 178, 487, 169, 8981, 152, 1038, 2055, 266, 323, 2425, 220, 1760, 586, 17, 214, 459, 378, 9952, 17, 1]\n",
      " thank you so much, chris. and it's truly a great honor to have the opportunity to come to this stage twice; i'm extremely grateful.  merci beaucoup, chris. c'est vraiment un honneur de pouvoir venir sur cette scène une deuxième fois. je suis très reconnaissant.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['translation'][0])\n",
    "print(train_en[0], train_fr[0])\n",
    "print(en_tokenizer.decode(train_en[0]), fr_tokenizer.decode(train_fr[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4bd4aa",
   "metadata": {
    "id": "8e4bd4aa"
   },
   "source": [
    "## 2. Sequence to sequence model (40 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9202d2",
   "metadata": {
    "id": "2f9202d2"
   },
   "source": [
    "### 2.1 Encoder (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9db70c22",
   "metadata": {
    "id": "9db70c22"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, GRU, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, units: int):\n",
    "        \"\"\" The encoder model for the src sentences.\n",
    "            It contains an embedding part and a GRU part.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: The src vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here\n",
    "        # Note: Please know what the decoder needs from encoder. This determines the parameters of the GRU layer\n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.gru = GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        # End\n",
    "\n",
    "    def call(self, src_ids, src_mask):\n",
    "        \"\"\" Encoder forward\n",
    "        Args:\n",
    "            src_ids: Tensor, (batch_size x max_len), the token ids of input sentences in a batch\n",
    "            src_mask: Tensor, (batch_size x max_len), the mask of the src input. True value in the mask means this timestep is valid, otherwise this timestep is ignored\n",
    "        Returns:\n",
    "            enc_output: Tensor, (batch_size x max_len x units), the output of GRU for all timesteps\n",
    "            final_state: Tensor, (batch_size x units), the state of the final valid timestep\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1. Retrieve embedding\n",
    "        #      2. GRU\n",
    "        # Please refer to the calling arguments of GRU: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#call-arguments\n",
    "        # print(src_ids)\n",
    "        # print(src_ids[0])\n",
    "        # print()\n",
    "        embedded = self.embedding(src_ids)\n",
    "        # print(embedded)\n",
    "        enc_outputs, final_state = self.gru(embedded, mask=src_mask)\n",
    "        # End\n",
    "        return enc_outputs, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312246f",
   "metadata": {
    "id": "a312246f"
   },
   "source": [
    "### 2.2 Decoder (15 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7b9bc88-a377-46c2-b398-6f50f5bad838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Dense, Embedding, Dropout, Attention\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "class Decoder_Attention(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, units: int, dropout_rate: float):\n",
    "        \"\"\" The decoder model for the tgt sentences with attention.\n",
    "            It contains an embedding part, an attention part, a GRU part, a dropout part, and a classifier part.\n",
    "            \n",
    "        Args:\n",
    "            vocab_size: The tgt vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "            dropout_rate: The classifier has a (units x vocab_size) weight. This is a large weight matrix. We apply a dropout layer to avoid overfitting.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Start your code here\n",
    "        \n",
    "        # Note: 1. Please correctly set the parameter of GRU\n",
    "        #       2. No softmax here because we will need the sequence to sequence loss later\n",
    "        \n",
    "        # Initializing Embedding Layer\n",
    "        self.embedding = Embedding(input_dim = vocab_size, output_dim = embedding_size)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = Attention(score_mode = \"dot\")\n",
    "        \n",
    "        # Initializing GRU Layer\n",
    "        self.gru = GRU(units = units, return_sequences = True, recurrent_initializer = 'glorot_uniform') \n",
    "        \n",
    "        # Initializing Dropout Layer\n",
    "        self.drop = Dropout(rate = dropout_rate)\n",
    "        \n",
    "        # Initializing Output Layer (Classifier)\n",
    "        self.dense = Dense(units = vocab_size)  \n",
    "\n",
    "        # End\n",
    "\n",
    "    def call(self, tgt_ids, initial_state, tgt_mask, encoder_outputs):\n",
    "        \"\"\" Decoder forward.\n",
    "            It is called by decoder(tgt_ids=..., initial_state=..., tgt_mask=...)\n",
    "\n",
    "        Args:\n",
    "            tgt_ids: Tensor, (batch_size x max_len), the token ids of input sentences in a batch\n",
    "            initial_state: Tensor, (batch_size x units), the state of the final valid timestep from the encoder\n",
    "            tgt_mask: Tensor, (batch_size x max_len), the mask of the tgt input. True value in the mask means this timestep is valid, otherwise this timestep is ignored\n",
    "            encoder_outputs: Tensor, (batch_size x max_len x units), the output of GRU for all timesteps\n",
    "        Return:\n",
    "            dec_outputs: Tensor, (batch_size x max_len x vocab_size), the output of GRU for all timesteps\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "                \n",
    "        # Embedding the input (after Embedding: (batch_size x max_len x embedding_size))\n",
    "        tgt_embedding = self.embedding(tgt_ids)\n",
    "\n",
    "        # Applying the GRU layer (after GRU layer: (batch_size x max_len x units))\n",
    "        tgt_hidden = self.gru(inputs = tgt_embedding, mask = tgt_mask, initial_state = initial_state)\n",
    "        \n",
    "        # Applying the Attention layer \n",
    "        context_vector, attention_weights = self.attention([tgt_hidden, encoder_outputs], return_attention_scores = True)\n",
    "        \n",
    "        # Concatenating attention output and the decoder hidden state\n",
    "        attention_op_decode_hidden = tf.concat([context_vector, tgt_hidden], axis=-1)\n",
    "        \n",
    "        # Applying the Dropout layer (after Dropout:\n",
    "        tgt_drop = self.drop(inputs = attention_op_decode_hidden)\n",
    "        \n",
    "        # Applying the Output layer\n",
    "        dec_outputs = self.dense(tgt_drop)\n",
    "\n",
    "        # End\n",
    "        return dec_outputs\n",
    "    \n",
    "    def predict(self, tgt_ids, initial_state, encoder_outputs):\n",
    "        \"\"\" Decoder prediction.\n",
    "            This is a step in recursive prediction. We use the previous prediction and state to predict current token.\n",
    "            Note that we only need to use the gru_cell instead of GRU becasue we only need to calculate one timestep.\n",
    "            \n",
    "        Args:\n",
    "            tgt_ids: Tensor, (batch_size, ) -> (1, ), the token id of the current timestep in the current sentence.\n",
    "            initial_state: Tensor, (batch_size x units) -> (1 x units), the state of the final valid timestep from the encoder or the previous hidden state in prediction.\n",
    "        Return:\n",
    "            dec_outputs: Tensor, (batch_size x vocab_size) -> (1 x vocab_size), the output of GRU for this timestep.\n",
    "            state: Tensor, (batch_size x units) -> (1 x units), the state of this timestep.\n",
    "        \"\"\"\n",
    "        gru_cell = self.gru.cell\n",
    "        \n",
    "        # Start your code here\n",
    "        \n",
    "        # Embedding the input (after embedding: (1 x embedding_size))\n",
    "        tgt_embedding = self.embedding(tgt_ids)\n",
    "        \n",
    "        # Applying the GRU layer (after GRU layer: (1 x units))\n",
    "        tgt_hidden, state = gru_cell(inputs = tgt_embedding, states = initial_state, training = False)\n",
    "\n",
    "        # (1 x 1 x units)\n",
    "        tgt_hidden = tf.expand_dims(tgt_hidden, axis=1)\n",
    "\n",
    "        # Applying the Attention layer \n",
    "        context_vector, attention_weights = self.attention([tgt_hidden, encoder_outputs], training = False, return_attention_scores = True)\n",
    "        context_vector = tf.squeeze(context_vector, axis=1)\n",
    "        tgt_hidden = tf.squeeze(tgt_hidden, axis=1)\n",
    "\n",
    "        # Concatenating attention output and the decoder hidden state\n",
    "        attention_op_decode_hidden = tf.concat([context_vector, tgt_hidden], axis=1)\n",
    "\n",
    "        # Applying the Output layer\n",
    "        dec_outputs = self.dense(attention_op_decode_hidden)\n",
    "\n",
    "        # End\n",
    "        return dec_outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e79c2",
   "metadata": {
    "id": "607e79c2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2570e4c",
   "metadata": {
    "id": "f2570e4c"
   },
   "source": [
    "### 2.3 Seq2seq With Attention(10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5eb3a30-b866-46af-87ec-c1344234f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq_Att(Model):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, embedding_size: int, units: int, dropout_rate: float):\n",
    "        \"\"\" The sequence to sequence model.\n",
    "            It contains an encoder and a decoder.\n",
    "            \n",
    "        Args:\n",
    "            src_vocab_size: The src vocabulary size\n",
    "            tgt_vocab_size: The tgt vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "            dropout_rate: The dropout rate used in the decoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Start your code here\n",
    "        \n",
    "        # Initializing the encoder\n",
    "        self.encoder = Encoder(vocab_size = src_vocab_size, embedding_size = embedding_size, units = units)\n",
    "        \n",
    "        # Initializing the decoder\n",
    "        self.decoder = Decoder_Attention(vocab_size = tgt_vocab_size, embedding_size = embedding_size, units = units, dropout_rate = dropout_rate)        \n",
    "        \n",
    "        # End\n",
    "\n",
    "    def call(self, src_ids, src_seq_lens, tgt_ids, tgt_seq_lens):\n",
    "        \"\"\" Seq2seq forward (for the loss calculation in training/validation only).\n",
    "            It is called by model(src_ids=..., src_seq_lens=..., tgt_ids=..., tgt_seq_lens=)\n",
    "            Note: In prediction, we will also need to set `training=False`.\n",
    "\n",
    "        Args:\n",
    "            src_ids: Tensor, (batch_size x max_len), the token ids of src sentences in a batch\n",
    "            src_seq_lens: Tensor, (batch_size, ), the length of src sentences in a batch\n",
    "            tgt_ids: Tensor, (batch_size x max_len), the token ids of tgt sentences in a batch\n",
    "            tgt_seq_lens: Tensor, (batch_size, ), the length of src sentences in a batch\n",
    "        Returns:\n",
    "            dec_outputs: Tensor, (batch_size x max_len x units), the decoder predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        # Start your code here\n",
    "        \n",
    "        # Building a mask for source sentences\n",
    "        max_src_len = tf.reduce_max(src_seq_lens)\n",
    "        src_mask = tf.sequence_mask(src_seq_lens, max_src_len)\n",
    "        \n",
    "        # Building a mask for target sentences\n",
    "        max_tgt_len = tf.reduce_max(tgt_seq_lens)\n",
    "        tgt_mask = tf.sequence_mask(tgt_seq_lens, max_tgt_len)\n",
    "        \n",
    "        # Encoder forward\n",
    "        enc_op, enc_final_state = self.encoder(src_ids, src_mask)\n",
    "        \n",
    "        # Decoder forward\n",
    "        dec_outputs = self.decoder(tgt_ids, enc_final_state, tgt_mask, enc_op)\n",
    "        \n",
    "        # End\n",
    "        return dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8314e49-8580-4389-85e7-33ef7b050028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0de39f",
   "metadata": {
    "id": "0c0de39f"
   },
   "source": [
    "### 2.4 Seq2seq loss (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56185972",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56185972",
    "outputId": "0cf08112-da88-4b40-a6be-e5b29fea48f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Akshay\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.seq2seq import sequence_loss\n",
    "\n",
    "\n",
    "def seq2seq_loss(logits, target, seq_lens):\n",
    "    \"\"\" Calculate the sequence to sequence loss using the sequence_loss from tensorflow\n",
    "\n",
    "    Args:\n",
    "        logits: Tensor (batch_size x max_seq_len x vocab_size). The output of the RNN model.\n",
    "        target: Tensor (batch_size x max_seq_len). The groud-truth of words.\n",
    "        seq_lens: Tensor (batch_size, ). The real sequence length before padding.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    # Start your code here\n",
    "    # 1. make a sequence mask (batch_size x max_seq_len) using tf.sequence_mask. This is to build a mask with 1 and 0.\n",
    "    mask = tf.sequence_mask(seq_lens, maxlen=tf.shape(target)[1], dtype=tf.float32)\n",
    "    #    Entry with 1 is the valid time step without padding. Entry with 0 is the time step with padding. We need to exclude this time step.\n",
    "    # 2. calculate the loss with sequence_loss. Carefully read the documentation of each parameter\n",
    "    loss = sequence_loss(logits, target,mask,average_across_timesteps = True,average_across_batch = True)\n",
    "    # End\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343fbf8",
   "metadata": {
    "id": "8343fbf8"
   },
   "source": [
    "## 3. Training (50 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce39702",
   "metadata": {
    "id": "bce39702"
   },
   "source": [
    "### 3.1 Pad batch (15 Points)\n",
    "\n",
    "`pad_src_batch`: 5 Points\n",
    "`pad_tgt_batch`: 10 Points\n",
    "\n",
    "Pad the batch to the equal length and make tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be42f754",
   "metadata": {
    "id": "be42f754"
   },
   "outputs": [],
   "source": [
    "def pad_src_batch(src_batch: List[List[int]], src_seq_lens: List[int], pad_val: int):\n",
    "    \"\"\" Pad the batch for src sentences.\n",
    "        Note: Do not use append/extend that can modify the input inplace.\n",
    "\n",
    "    Args:\n",
    "        src_batch: A list of src token ids\n",
    "        src_seq_lens: A list of src lens\n",
    "        pad_val: The padding value\n",
    "\n",
    "    Returns:\n",
    "        src_batch: Tensor, (batch_size x max_len)\n",
    "        src_seq_lens_batch: Tensor, (batch_size, )\n",
    "    \"\"\"\n",
    "    max_src_len = max(src_seq_lens)\n",
    "    # Start your code here\n",
    "    num_sent = len(src_seq_lens)\n",
    "    # Please refer to tf.convert_to_tensor. The dtype should be tf.int64\n",
    "    # Padding\n",
    "    src_batch_padded = [sentence + [pad_val] * (max_src_len - len(sentence)) for sentence in src_batch]\n",
    "    # Convert to tensor\n",
    "    src_batch = tf.convert_to_tensor(src_batch_padded, dtype=tf.int64)\n",
    "    src_seq_lens_batch = tf.convert_to_tensor(src_seq_lens, dtype=tf.int64)\n",
    "    # End\n",
    "    return src_batch, src_seq_lens_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dfa26ac",
   "metadata": {
    "id": "9dfa26ac"
   },
   "outputs": [],
   "source": [
    "def pad_tgt_batch(tgt_batch: List[List[int]], tgt_seq_lens: List[int], pad_val: int):\n",
    "    \"\"\" Pad the batch for tgt sentences.\n",
    "        Note: 1. Do not use append/extend that can modify the input inplace.\n",
    "              2. We need to build the x (feature) and y (label) for tgt sentences.\n",
    "                 Please understand what the feature and label are in translation.\n",
    "\n",
    "    Args:\n",
    "        tgt_batch: A list of src token ids\n",
    "        tgt_seq_lens: A list of src lens\n",
    "        pad_val: The padding value\n",
    "\n",
    "    Returns:\n",
    "        tgt_x_batch: Tensor, (batch_size x max_len)\n",
    "        tgt_y_batch: Tensor, (batch_size x max_len)\n",
    "        src_seq_lens_batch: Tensor, (batch_size, )\n",
    "    \"\"\"\n",
    "    tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch = [], [], []\n",
    "    for sent, seq_len in zip(tgt_batch, tgt_seq_lens):\n",
    "        # Start your code here\n",
    "        # Append x, y, and seq_len\n",
    "        tgt_x_batch.append(sent[:-1])  # Exclude last token for y\n",
    "        tgt_y_batch.append(sent[1:])   # Exclude first token for x\n",
    "        tgt_seq_lens_batch.append(seq_len - 1)  # Reduce sequence length by 1 for y\n",
    "        # End\n",
    "\n",
    "    max_tgt_len = max(tgt_seq_lens_batch)\n",
    "    # Start your code here\n",
    "    # Please refer to tf.convert_to_tensor. The dtype should be tf.int64\n",
    "    # Padding\n",
    "    tgt_x_batch = [sentence + [pad_val] * (max_tgt_len - len(sentence)) for sentence in tgt_x_batch]\n",
    "    tgt_y_batch = [sentence + [pad_val] * (max_tgt_len - len(sentence)) for sentence in tgt_y_batch]\n",
    "\n",
    "    # Convert to tensor\n",
    "    tgt_x_batch = tf.convert_to_tensor(tgt_x_batch, dtype=tf.int64)\n",
    "    tgt_y_batch = tf.convert_to_tensor(tgt_y_batch, dtype=tf.int64)\n",
    "    tgt_seq_lens_batch = tf.convert_to_tensor(tgt_seq_lens_batch, dtype=tf.int64)\n",
    "    # End\n",
    "    return tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bdfff87",
   "metadata": {
    "id": "7bdfff87"
   },
   "outputs": [],
   "source": [
    "def pad_batch(src_batch: List[List[int]], src_seq_lens: List[int], tgt_batch: List[List[int]], tgt_seq_lens: List[int], pad_val: int):\n",
    "    src_batch, src_seq_lens_batch = pad_src_batch(src_batch, src_seq_lens, pad_val)\n",
    "    tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch = pad_tgt_batch(tgt_batch, tgt_seq_lens, pad_val)\n",
    "    return src_batch, src_seq_lens_batch, tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83129bde",
   "metadata": {
    "id": "83129bde"
   },
   "source": [
    "### 3.2 Batch Index Sampler (10 Points)\n",
    "\n",
    "Create a index sampler to sample data index for each batch.\n",
    "\n",
    "This is to make the sentences in each batch have similar lengths to speed up training.\n",
    "\n",
    "Example:\n",
    "```\n",
    "Assume the sentence lengths are: [5, 2, 3, 6, 2, 3, 6] and batch_size is 2.\n",
    "We can make the indices in the batches as follows:\n",
    "[1, 4] of length 2\n",
    "[2, 5] of length 3\n",
    "[0, 3] of lengths 5 and 6\n",
    "[6] of length 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7954420",
   "metadata": {
    "id": "d7954420"
   },
   "outputs": [],
   "source": [
    "class SeqLenBatchSampler:\n",
    "    def __init__(self, seq_lens: List[int], batch_size: int, seed: int = 6666):\n",
    "        \"\"\" The index sampler.\n",
    "            It can be used with iteration:\n",
    "            ```\n",
    "            n_batch = len(sampler)\n",
    "            for indices in sampler:\n",
    "                ...\n",
    "            ```\n",
    "\n",
    "            Args:\n",
    "                seq_lens: A list training sequence lengths (src)\n",
    "                batch_size: .\n",
    "                seed: .\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.seq_lens = seq_lens\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = self._make_batch_index()\n",
    "\n",
    "        self.n_batch = len(self.batches)\n",
    "        self.counter = -1\n",
    "\n",
    "    def _make_batch_index(self) -> List[List[int]]:\n",
    "        \"\"\" Build the indexes in each batch.\n",
    "\n",
    "            Return:\n",
    "                batches: A list of indices batch, e.g., [[0, 2, 8], [3, 6, 4], [5, 1, 7], ...]\n",
    "        \"\"\"\n",
    "        n = len(self.seq_lens)\n",
    "        n_batch = int(np.ceil(n / self.batch_size))\n",
    "        batches = []\n",
    "        # Start your code here\n",
    "        # Step 1. Use np.argsort to get all indices with sorted length\n",
    "        #      2. Split the indices into batches using a for loop: `for i in range(n_batch):`\n",
    "        sorted_indices = np.argsort(self.seq_lens)\n",
    "        batches = [sorted_indices[i::n_batch] for i in range(n_batch)]\n",
    "        # End\n",
    "        return batches\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batch\n",
    "\n",
    "    def __item__(self, index):\n",
    "        return self.batches[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        np.random.shuffle(self.batches)\n",
    "        self.counter = -1\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        self.counter += 1\n",
    "        if self.counter < self.n_batch:\n",
    "            return self.batches[self.counter]\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e8f1e",
   "metadata": {
    "id": "e78e8f1e"
   },
   "source": [
    "### 3.3 Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd73f9b",
   "metadata": {
    "id": "0bd73f9b"
   },
   "source": [
    "Generate the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e8c8ad7",
   "metadata": {
    "id": "7e8c8ad7"
   },
   "outputs": [],
   "source": [
    "np.random.seed(6666)\n",
    "train_seq_lens_en = [len(en_sent) for en_sent in train_en]\n",
    "train_seq_lens_fr = [len(fr_sent) for fr_sent in train_fr]\n",
    "valid_seq_lens_en = [len(en_sent) for en_sent in valid_en]\n",
    "valid_seq_lens_fr = [len(fr_sent) for fr_sent in valid_fr]\n",
    "test_seq_lens_en = [len(en_sent) for en_sent in test_en]\n",
    "test_seq_lens_fr = [len(fr_sent) for fr_sent in test_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038af815",
   "metadata": {
    "id": "038af815"
   },
   "source": [
    "Create np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "affa5645",
   "metadata": {
    "id": "affa5645"
   },
   "outputs": [],
   "source": [
    "train_en = np.array(train_en, dtype=object)\n",
    "train_seq_lens_en = np.array(train_seq_lens_en)\n",
    "train_fr = np.array(train_fr, dtype=object)\n",
    "train_seq_lens_fr = np.array(train_seq_lens_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38e14b",
   "metadata": {
    "id": "bc38e14b"
   },
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5baa89ed",
   "metadata": {
    "id": "5baa89ed"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "seed = 6666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1407ae0",
   "metadata": {
    "id": "d1407ae0"
   },
   "outputs": [],
   "source": [
    "src_vocab_size = len(fr_tokenizer.get_vocab())\n",
    "tgt_vocab_size = len(en_tokenizer.get_vocab())\n",
    "hidden_units = 256\n",
    "embedding_dim = 128\n",
    "dropout_rate = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f48a9b3",
   "metadata": {
    "id": "7f48a9b3"
   },
   "outputs": [],
   "source": [
    "model = Seq2seq_Att(src_vocab_size, tgt_vocab_size, embedding_dim, hidden_units, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb52f7-d0be-42be-b1fd-fda10648180f",
   "metadata": {
    "id": "99bb52f7-d0be-42be-b1fd-fda10648180f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "053d5167",
   "metadata": {
    "id": "053d5167"
   },
   "outputs": [],
   "source": [
    "num_epoch = 15\n",
    "batch_size = 256\n",
    "# num_epoch = 1\n",
    "# batch_size = 32\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f91b82a",
   "metadata": {
    "id": "0f91b82a"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_batch_sampler = SeqLenBatchSampler(train_seq_lens_fr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd8527-290b-4e7a-b0b4-9a3b86e0f02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48cff587",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48cff587",
    "outputId": "32bc0eb4-8599-464b-90c3-bd0fc527e22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 1 - Step 217 / 7276 - loss: 6.0633"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, trainable_vars)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m real_batch_size\n\u001b[0;32m     31\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:730\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m experimental_aggregate_gradients\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m strategy\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m     )\n\u001b[0;32m    723\u001b[0m ):\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_aggregate_gradients=False is not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor ParameterServerStrategy and CentralStorageStrategy. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsed: strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    728\u001b[0m     )\n\u001b[1;32m--> 730\u001b[0m apply_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m    732\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_unaggregated_gradients(\n\u001b[0;32m    733\u001b[0m         grads_and_vars\n\u001b[0;32m    734\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:1097\u001b[0m, in \u001b[0;36mOptimizerV2._prepare\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     apply_state[(var_device, var_dtype)] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(var_device):\n\u001b[1;32m-> 1097\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_state\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:134\u001b[0m, in \u001b[0;36mAdam._prepare_local\u001b[1;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_local\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_device, var_dtype, apply_state):\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     local_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, var_dtype)\n\u001b[0;32m    137\u001b[0m     beta_1_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hyper(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, var_dtype))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:1103\u001b[0m, in \u001b[0;36mOptimizerV2._prepare_local\u001b[1;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_local\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_device, var_dtype, apply_state):\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyper:\n\u001b[1;32m-> 1103\u001b[0m         lr_t \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decayed_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m         apply_state[(var_device, var_dtype)][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_t\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lr_t\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:293\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(identity, \u001b[38;5;28minput\u001b[39m, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    291\u001b[0m   \u001b[38;5;66;03m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[0;32m    292\u001b[0m   \u001b[38;5;66;03m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m   \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m ret \u001b[38;5;241m=\u001b[39m gen_array_ops\u001b[38;5;241m.\u001b[39midentity(\u001b[38;5;28minput\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1629\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1631\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1634\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1635\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m   1637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1638\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1641\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2105\u001b[0m, in \u001b[0;36m_dense_var_to_tensor\u001b[1;34m(var, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dense_var_to_tensor\u001b[39m(var, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2105\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dense_var_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1467\u001b[0m, in \u001b[0;36mBaseResourceVariable._dense_var_to_tensor\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1465\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_value()\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1467\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:582\u001b[0m, in \u001b[0;36mBaseResourceVariable.value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_value\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 582\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:704\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[1;34m(self, no_copy)\u001b[0m\n\u001b[0;32m    702\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    707\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[0;32m    708\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[0;32m    710\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[0;32m    711\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[0;32m    712\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:694\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[1;34m(no_copy)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    693\u001b[0m   gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mdisable_copy_on_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[1;32m--> 694\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:524\u001b[0m, in \u001b[0;36mread_variable_op\u001b[1;34m(resource, dtype, name)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    523\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    527\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_training_samples = len(train_fr)\n",
    "n_valid_batch = int(np.ceil(len(valid_fr) / batch_size))\n",
    "pad_token_id = fr_tokenizer.token_to_id('<pad>')\n",
    "train_losses, valid_losses = [], []\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, data_index in enumerate(train_batch_sampler):\n",
    "        src_batch, src_seq_lens = train_fr[data_index], train_seq_lens_fr[data_index]\n",
    "\n",
    "        tgt_batch, tgt_seq_lens = train_en[data_index], train_seq_lens_en[data_index]\n",
    "        real_batch_size = len(src_batch)\n",
    "        (src_batch, src_seq_lens_batch,\n",
    "         tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch) = pad_batch(src_batch, src_seq_lens,\n",
    "                                                                   tgt_batch, tgt_seq_lens,\n",
    "                                                                   pad_val=pad_token_id)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # print(src_batch[0])\n",
    "            # print()\n",
    "            output = model(src_batch, src_seq_lens_batch, tgt_x_batch, tgt_seq_lens_batch)\n",
    "            loss = seq2seq_loss(output, tgt_y_batch, tgt_seq_lens_batch)\n",
    "\n",
    "        print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {len(train_batch_sampler)} - loss: {loss:.4f}')\n",
    "\n",
    "        trainable_vars = model.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        epoch_loss += loss * real_batch_size\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    for batch_idx in range(n_valid_batch):\n",
    "        start = batch_idx * batch_size\n",
    "        end = start + batch_size\n",
    "        src_batch, src_seq_lens = valid_fr[start:end], valid_seq_lens_fr[start:end]\n",
    "        tgt_batch, tgt_seq_lens = valid_en[start:end], valid_seq_lens_en[start:end]\n",
    "        real_batch_size = len(src_batch)\n",
    "        (src_batch, src_seq_lens_batch,\n",
    "         tgt_x_batch, tgt_y_batch, tgt_seq_lens_batch) = pad_batch(src_batch, src_seq_lens,\n",
    "                                                                   tgt_batch, tgt_seq_lens,\n",
    "                                                                   pad_val=pad_token_id)\n",
    "        output = model(src_batch, src_seq_lens_batch, tgt_x_batch, tgt_seq_lens_batch, training=False)\n",
    "        loss = seq2seq_loss(output, tgt_y_batch, tgt_seq_lens_batch)\n",
    "\n",
    "        if batch_idx % 1 == 0 or batch_idx == len(valid_en) - 1:\n",
    "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_valid_batch} - loss: {loss:.4f}')\n",
    "\n",
    "        valid_loss += loss * real_batch_size\n",
    "    train_epoch_loss = epoch_loss / n_training_samples\n",
    "    valid_epoch_loss = valid_loss / len(valid_en)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    valid_losses.append(valid_epoch_loss)\n",
    "    print(f'\\rEpoch {epoch + 1} / {num_epoch} - Step {len(train_batch_sampler)} / {len(train_batch_sampler)} - train loss: {train_epoch_loss:.4f} - valid loss: {valid_epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf29ac",
   "metadata": {
    "id": "a9cf29ac"
   },
   "source": [
    "If you implement everything correctly, the valid loss will be around 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5605580",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5605580",
    "outputId": "331e0ee2-f632-4417-e87f-d28fa706f7b8"
   },
   "outputs": [],
   "source": [
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3977b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "52a3977b",
    "outputId": "746e6b0f-30b5-47f0-e33c-aae5a2fcacd4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "x = np.arange(1, len(train_losses) + 1)\n",
    "plt.plot(x, train_losses, label='Train loss')\n",
    "plt.plot(x, valid_losses, label='Valid loss')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11461f",
   "metadata": {
    "id": "ea11461f"
   },
   "source": [
    "### 3.4 Translate French to English (15 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc85ae8",
   "metadata": {
    "id": "8fc85ae8"
   },
   "outputs": [],
   "source": [
    "sos_token_id = en_tokenizer.token_to_id('<s>')\n",
    "eos_token_id = en_tokenizer.token_to_id('</s>')\n",
    "max_pred_len = 200\n",
    "def translate(encoder: 'Encoder', decoder: 'Decoder', fr_sentences: List[List[int]]):\n",
    "    \"\"\" Translate the src (French) sentences to English sentences.\n",
    "        This is a recursive translation.\n",
    "\n",
    "    Args:\n",
    "        encoder: The encoder part in seq2seq\n",
    "        decoder: The decoder part in seq2seq\n",
    "        fr_sentences: The src token ids of all sentences\n",
    "    Returns:\n",
    "        pred_sentences: The predicted string sentences\n",
    "    \"\"\"\n",
    "    n = len(fr_sentences)\n",
    "    pred_sentences = []\n",
    "    for i, src_ids in enumerate(fr_sentences):\n",
    "        print_line(f'{i + 1} / {n}')\n",
    "        # Shape of src_ids: (1 x seq_len)\n",
    "        src_ids = tf.expand_dims(tf.convert_to_tensor(src_ids, dtype=tf.int64), axis=0)\n",
    "        # pred is the prediction token ids. It starts with <s>\n",
    "        pred = [sos_token_id]\n",
    "        # Start your code here\n",
    "        # Step 1. Calculate the encoder outputs and hidden states (similar to seq2seq2 model)\n",
    "        # Step 2. Run a while loop when the last token in pred is not eos_token_id and the length of pred is less than max_pred_len\n",
    "        # Step 3.     In the while loop, build the input (cur_token) of decoder: the last token of pred. Shape (batch_size, ) -> (1, )\n",
    "        #             For example, if the current pred is [1, 50, 21, 8], the cur_token is [8]\n",
    "        # Step 4.     In the while loop, use decoder.predict to get the decoder output\n",
    "        # Step 5.     In the while loop, find the index with the maximum value. Then you can call tf.squeeze and numpy() to get the index\n",
    "        # Step 6.     In the while loop, append the predicted token to pred\n",
    "        # Step 7. Use en_tokenizer to decode the id to strings: pred_sentence\n",
    "        enc_output, enc_hidden = encoder(src_ids, None)\n",
    "        dec_hidden = enc_hidden\n",
    "        while pred[-1] != eos_token_id and len(pred) < max_pred_len:\n",
    "    #         # Prepare input for decoder\n",
    "            cur_token = np.array([pred[-1]])  # Shape: (1,)\n",
    "    #         # Get decoder output for the current token\n",
    "            dec_output, dec_hidden = decoder.predict(tf.expand_dims(cur_token, axis=0), dec_hidden)\n",
    "    #         # Get index of predicted token with maximum probability\n",
    "            predicted_index = int(tf.argmax(dec_output[0], axis=-1)[-1])\n",
    "    #         # Append predicted token to pred\n",
    "            pred.append(predicted_index)\n",
    "    #     # Decode predicted sentence using English tokenizer\n",
    "        pred_sentence = en_tokenizer.decode(pred)\n",
    "    #     # End\n",
    "        pred_sentences.append(pred_sentence)\n",
    "    print_line('\\n')\n",
    "    return pred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LTG_p-1040eP",
   "metadata": {
    "id": "LTG_p-1040eP"
   },
   "outputs": [],
   "source": [
    "model.save('my_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C3hxOH6X_D1L",
   "metadata": {
    "id": "C3hxOH6X_D1L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c0472",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "761c0472",
    "outputId": "d48f4a87-bf64-4dea-a538-ace398326b3d"
   },
   "outputs": [],
   "source": [
    "test_pred = translate(model.encoder, model.decoder, fr_sentences=test_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aae118",
   "metadata": {
    "id": "50aae118"
   },
   "source": [
    "### 3.5 Demonstrate 10 translation examples (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c91c9b",
   "metadata": {
    "id": "d7c91c9b"
   },
   "source": [
    "### 3.6 Compute the bleu score (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MEblPzOvdHAc",
   "metadata": {
    "id": "MEblPzOvdHAc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m7P2_-7gRa_f",
   "metadata": {
    "id": "m7P2_-7gRa_f"
   },
   "outputs": [],
   "source": [
    "len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Zs-IJrAdWel",
   "metadata": {
    "id": "-Zs-IJrAdWel"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(6666)\n",
    "sample_num = 10\n",
    "# Start your code here\n",
    "# Use np.random.choice to sample 10 sentence indices. Remember to set correct replace\n",
    "# Print format:\n",
    "# 1.\n",
    "# French: ...\n",
    "# True English: ...\n",
    "# Translated English: ...\n",
    "# ------------------\n",
    "sampled_indices = np.random.choice(len(test_pred), size=sample_num, replace=False)\n",
    "# Print the format for each sampled sentence\n",
    "print(sampled_indices)\n",
    "for idx in sampled_indices:\n",
    "    print(\"French:\", test_fr_sentences[idx])\n",
    "    print(\"Ground Truth English:\", test_en_sentences[idx])\n",
    "    print(\"Translated English Attention:\", test_pred[idx])\n",
    "    print(\"------------------\")\n",
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07c2cf5",
   "metadata": {
    "id": "d07c2cf5"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "sacrebleu = evaluate.load('sacrebleu', cache_dir=dataset_path)\n",
    "# Start your code here\n",
    "# see https://huggingface.co/spaces/evaluate-metric/sacrebleu\n",
    "# Note: please understand the format and meaning of references.\n",
    "results = sacrebleu.compute(predictions=test_pred, references=test_en_sentences)\n",
    "# End\n",
    "# print(score)\n",
    "score = results['score']\n",
    "print(round(score, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118a02c",
   "metadata": {
    "id": "1118a02c"
   },
   "source": [
    "If you implement everything correctly, the BLEU score will be around 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5471d",
   "metadata": {
    "id": "18e5471d"
   },
   "source": [
    "## Conclusion (5 Points)\n",
    "\n",
    "Including but not limited to: translation example analysis (case study), bleu score analysis, model structure / parameter analysis, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9537daf",
   "metadata": {
    "id": "d9537daf"
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E7zPJPNtGq1e",
   "metadata": {
    "id": "E7zPJPNtGq1e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
