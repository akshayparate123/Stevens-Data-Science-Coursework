{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "42e55e46",
      "metadata": {
        "id": "42e55e46"
      },
      "source": [
        "# CS 584 Assignment 3 -- Language Model\n",
        "\n",
        "#### Name: Akshay Parate\n",
        "#### Stevens ID: 20032008"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "hvIUbqBU0_jR",
      "metadata": {
        "id": "hvIUbqBU0_jR"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb5aa24",
      "metadata": {
        "id": "7cb5aa24"
      },
      "source": [
        "## In this assignment, you are required to follow the steps below:\n",
        "1. Review the lecture slides.\n",
        "2. Implement N-gram language modeling.\n",
        "3. Implement RNN language modeling.\n",
        "\n",
        "**Before you start**\n",
        "- Please read the code very carefully.\n",
        "- Install these packages (jupyterlab, matplotlib, nltk, numpy, scikit-learn, tensorflow, tensorflow_addons) using the following command.\n",
        "```console\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "- It's better to train the Tensorflow model with GPU and CUDA. If they are not available on your local machine, please consider Google CoLab. You can check `CoLab.md` in this assignments.\n",
        "- You are **NOT** allowed to use other packages unless otherwise specified.\n",
        "- You are **ONLY** allowed to edit the code between `# Start your code here` and `# End` for each block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "yIuxXG-dsWnQ",
      "metadata": {
        "id": "yIuxXG-dsWnQ"
      },
      "outputs": [],
      "source": [
        "# pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31c08ea0",
      "metadata": {
        "id": "31c08ea0"
      },
      "source": [
        "## Part A: 1. N-Gram (60 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f816da3f",
      "metadata": {
        "id": "f816da3f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "def print_line(*args):\n",
        "    \"\"\" Inline print and go to the begining of line\n",
        "    \"\"\"\n",
        "    args1 = [str(arg) for arg in args]\n",
        "    str_ = ' '.join(args1)\n",
        "    print('\\r' + str_, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "48c21b6c",
      "metadata": {
        "id": "48c21b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d26cd9d-520b-4935-decd-46a1a2f5b64d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# If you are going to use GPU, make sure the GPU is in the output\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ad2ab3e1",
      "metadata": {
        "id": "ad2ab3e1"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Union, Dict\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf5db833",
      "metadata": {
        "id": "bf5db833"
      },
      "source": [
        "### 1.1 Load Data & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6b41ab",
      "metadata": {
        "id": "af6b41ab"
      },
      "source": [
        "You will not need to implement the data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "536ef220",
      "metadata": {
        "id": "536ef220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8147fd9-d719-4cfd-f6cc-7f405de57478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of train sentences: 42068\n",
            "number of valid sentences: 3370\n",
            "number of test sentences: 3165\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "data_path = 'a3-data'\n",
        "\n",
        "train_sentences = open(os.path.join(data_path, 'train.txt')).readlines()\n",
        "valid_sentences = open(os.path.join(data_path, 'valid.txt')).readlines()\n",
        "test_sentences = open(os.path.join(data_path, 'input.txt')).readlines()\n",
        "print('number of train sentences:', len(train_sentences))\n",
        "print('number of valid sentences:', len(valid_sentences))\n",
        "print('number of test sentences:', len(test_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "7d187d2d",
      "metadata": {
        "id": "7d187d2d"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, punctuation=True, url=True, number=True):\n",
        "        self.punctuation = punctuation\n",
        "        self.url = url\n",
        "        self.number = number\n",
        "\n",
        "    def apply(self, sentence: str) -> str:\n",
        "        \"\"\" Apply the preprocessing rules to the sentence\n",
        "        Args:\n",
        "            sentence: raw sentence\n",
        "        Returns:\n",
        "            sentence: clean sentence\n",
        "        \"\"\"\n",
        "        sentence = sentence.lower()\n",
        "        sentence = sentence.replace('<unk>', '')\n",
        "        if self.url:\n",
        "            sentence = Preprocessor.remove_url(sentence)\n",
        "        if self.punctuation:\n",
        "            sentence = Preprocessor.remove_punctuation(sentence)\n",
        "        if self.number:\n",
        "            sentence = Preprocessor.remove_number(sentence)\n",
        "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_punctuation(sentence: str) -> str:\n",
        "        \"\"\" Remove punctuations in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible punctuations\n",
        "        Returns:\n",
        "            sentence: sentence without punctuations\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_url(sentence: str) -> str:\n",
        "        \"\"\" Remove urls in text with re\n",
        "        Args:\n",
        "            sentence: sentence with possible urls\n",
        "        Returns:\n",
        "            sentence: sentence without urls\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'(https|http)?://(\\w|\\.|/|\\?|=|&|%)*\\b', ' ', sentence)\n",
        "        return sentence\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_number(sentence: str) -> str:\n",
        "        \"\"\" Remove numbers in sentence with re\n",
        "        Args:\n",
        "            sentence: sentence with possible numbers\n",
        "        Returns:\n",
        "            sentence: sentence without numbers\n",
        "        \"\"\"\n",
        "        sentence = re.sub(r'\\d+', ' ', sentence)\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "1fdf9342",
      "metadata": {
        "id": "1fdf9342"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, sos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<unk>', mask_token='<mask>'):\n",
        "        # Special tokens.\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.mask_token = mask_token\n",
        "\n",
        "        self.vocab = { sos_token: 0, eos_token: 1, pad_token: 2, unk_token: 3, mask_token: 4 }  # token -> id\n",
        "        self.inverse_vocab = { 0: sos_token, 1: eos_token, 2: pad_token, 3: unk_token, 4: mask_token }  # id -> token\n",
        "        self.token_occurrence = { sos_token: 0, eos_token: 0, pad_token: 0, unk_token: 0, mask_token: 0 }  # token -> occurrence\n",
        "\n",
        "        self.preprocessor = Preprocessor()\n",
        "\n",
        "    @property\n",
        "    def sos_token_id(self):\n",
        "        \"\"\" Create a property method.\n",
        "            You can use self.sos_token_id or tokenizer.sos_token_id to get the id of the sos_token.\n",
        "        \"\"\"\n",
        "        return self.vocab[self.sos_token]\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self):\n",
        "        return self.vocab[self.eos_token]\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self):\n",
        "        return self.vocab[self.pad_token]\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self):\n",
        "        return self.vocab[self.unk_token]\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self):\n",
        "        return self.vocab[self.mask_token]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" A magic method that enable program to know the number of tokens by calling:\n",
        "            ```python\n",
        "            tokenizer = Tokenizer()\n",
        "            num_tokens = len(tokenizer)\n",
        "            ```\n",
        "        \"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def fit(self, sentences: List[str]):\n",
        "        \"\"\" Fit the tokenizer using all sentences.\n",
        "        1. Tokenize the sentence by splitting with spaces.\n",
        "        2. Record the occurrence of all tokens\n",
        "        3. Construct the token to index (self.vocab) map and the inversed map (self.inverse_vocab) based on the occurrence. The token with a higher occurrence has the smaller index\n",
        "\n",
        "        Args:\n",
        "            sentences: All sentences in the dataset.\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Fitting Tokenizer:', (i + 1), '/', n)\n",
        "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
        "            if len(tokens) <= 1:\n",
        "                continue\n",
        "            for token in tokens:\n",
        "                if token == '<unk>':\n",
        "                    continue\n",
        "                self.token_occurrence[token] = self.token_occurrence.get(token, 0) + 1\n",
        "        print_line('\\n')\n",
        "\n",
        "        token_occurrence = sorted(self.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
        "        for token, occurrence in token_occurrence[:-5]:\n",
        "            token_id = len(self.vocab)\n",
        "            self.vocab[token] = token_id\n",
        "            self.inverse_vocab[token_id] = token\n",
        "\n",
        "        print('The number of distinct tokens:', len(self.vocab))\n",
        "\n",
        "    def encode(self, sentences: List[str]) -> List[List[int]]:\n",
        "        \"\"\" Encode the sentences into token ids\n",
        "            Note: 1. if a token in a sentence does not exist in the fit encoder, we ignore it.\n",
        "                  2. If the number of tokens in a sentence is less than two, we ignore this sentence.\n",
        "                  3. Note that, for every sentence, we will add an sos_token, i.e., the id of <s> at the start of the sentence,\n",
        "                     and add an eos_token, i.e., the id of </s> at the end of the sentence.\n",
        "        Args:\n",
        "            sentences: Raw sentences\n",
        "        Returns:\n",
        "            sent_token_ids: A list of id list\n",
        "        \"\"\"\n",
        "        n = len(sentences)\n",
        "        sent_token_ids = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            if i % 100 == 0 or i == n - 1:\n",
        "                print_line('Encoding with Tokenizer:', (i + 1), '/', n)\n",
        "            token_ids = []\n",
        "            tokens = self.preprocessor.apply(sentence.strip()).split()\n",
        "            for token in tokens:\n",
        "                if token == '<unk>':\n",
        "                    continue\n",
        "                if token in self.vocab:\n",
        "                    token_ids.append(self.vocab[token])\n",
        "            if len(token_ids) <= 1:\n",
        "                continue\n",
        "            token_ids = [self.sos_token_id] + token_ids + [self.eos_token_id]\n",
        "            sent_token_ids.append(token_ids)\n",
        "        print_line('\\n')\n",
        "        return sent_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "00ba2b8c",
      "metadata": {
        "id": "00ba2b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e9d22b-7b29-49e5-a3ff-8b0d50c193f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFitting Tokenizer: 1 / 2\rFitting Tokenizer: 2 / 2\r\n",
            "The number of distinct tokens: 44\n",
            "\n",
            "n : 2\n",
            "aer : 1\n",
            "banknote : 1\n",
            "berlitz : 1\n",
            "calloway : 1\n",
            "centrust : 1\n",
            "cluett : 1\n",
            "fromstein : 1\n",
            "gitano : 1\n",
            "guterman : 1\n",
            "\n",
            "\rEncoding with Tokenizer: 1 / 2\rEncoding with Tokenizer: 2 / 2\r\n",
            "\n",
            " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
            " ['<s>', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro', 'quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack', 'food', 'ssangyong', 'swapo', 'wachter', '</s>'] \n",
            "\n",
            " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
            " ['<s>', 'pierre', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov', 'n', '</s>'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(train_sentences[:2])\n",
        "print()\n",
        "\n",
        "token_occurrence = sorted(tokenizer.token_occurrence.items(), key=lambda e: e[1], reverse=True)\n",
        "for token, occurrence in token_occurrence[:10]:\n",
        "    print(token, ':', occurrence)\n",
        "print()\n",
        "sent_token_ids = tokenizer.encode(train_sentences[:2])\n",
        "print()\n",
        "for original_sentence, token_ids in zip(train_sentences[:2], sent_token_ids):\n",
        "    sentence = [tokenizer.inverse_vocab[token] for token in token_ids]\n",
        "    print(original_sentence, sentence, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "aee85a74",
      "metadata": {
        "id": "aee85a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54e3817-380f-4b1e-cb9c-5e2de1d3bfbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting Tokenizer: 42068 / 42068\n",
            "The number of distinct tokens: 9614\n",
            "Encoding with Tokenizer: 42068 / 42068\n",
            "Encoding with Tokenizer: 3370 / 3370\n",
            "Encoding with Tokenizer: 3165 / 3165\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit(train_sentences)\n",
        "train_token_ids = tokenizer.encode(train_sentences)\n",
        "valid_token_ids = tokenizer.encode(valid_sentences)\n",
        "test_token_ids = tokenizer.encode(test_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755ee9c8",
      "metadata": {
        "id": "755ee9c8"
      },
      "source": [
        "### 1.2 Calculate unigram and bigram count (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "25ffeba5",
      "metadata": {
        "id": "25ffeba5"
      },
      "outputs": [],
      "source": [
        "def get_unigram_count(train_token_ids: List[List[int]]) -> Dict:\n",
        "    \"\"\" Calculate the occurrence of each token in the dataset.\n",
        "\n",
        "    Args:\n",
        "        train_token_ids: each element is a list of token ids\n",
        "    Return:\n",
        "        unigram_count: A map from token_id to occurrence\n",
        "    \"\"\"\n",
        "    unigram_count = {}\n",
        "    # Start your code here\n",
        "    for token_id in train_token_ids:\n",
        "        for token in token_id:\n",
        "            if token in unigram_count:\n",
        "                unigram_count[token] = unigram_count[token] + 1\n",
        "            else:\n",
        "                unigram_count[token] = 1\n",
        "    # End\n",
        "    return unigram_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "4bcd1b0a",
      "metadata": {
        "id": "4bcd1b0a"
      },
      "outputs": [],
      "source": [
        "def get_bigram_count(train_token_ids: List[List[int]]) -> Dict[int, Dict]:\n",
        "    \"\"\" Calculate the occurrence of bigrams in the dataset.\n",
        "\n",
        "    Args:\n",
        "        train_token_ids: each element is a list of token ids\n",
        "    Return:\n",
        "        bigram_count: A map from token_id to next token occurrence. Key: token_id, value: Dict[token_id -> occurrence]\n",
        "                      For example, {\n",
        "                          5: { 10: 5, 20: 4 }\n",
        "                      } means (5, 10) occurs 10 times, (5, 20) occurs 4 times.\n",
        "    \"\"\"\n",
        "    bigram_count = {}\n",
        "    # Start your code here\n",
        "    for token_id in train_token_ids:\n",
        "        for i in range(0,len(token_id)-1):\n",
        "\n",
        "            if token_id[i] in bigram_count:\n",
        "                # print(\"If {} in {}\".format(token_id[i],bigram_count))\n",
        "\n",
        "                if token_id[i+1] in bigram_count[token_id[i]]:\n",
        "                    # print(\"If {} in {}\".format(token_id[i+1],bigram_count[token_id[i]]))\n",
        "                    bigram_count[token_id[i]][token_id[i+1]] = bigram_count[token_id[i]][token_id[i+1]] + 1\n",
        "                    # print(\"Increment {}\".format(bigram_count[token_id[i]][token_id[i+1]]))\n",
        "                else:\n",
        "                    # print(\"If {} not in {}\".format(token_id[i+1],bigram_count[token_id[i]]))\n",
        "                    bigram_count[token_id[i]][token_id[i+1]] = 1\n",
        "                    # print(\"Initialize {} as 1\".format(bigram_count[token_id[i]][token_id[i+1]]))\n",
        "            else:\n",
        "                # print(\"If {} not in {}\".format(token_id[i],bigram_count))\n",
        "                bigram_count[token_id[i]] = {token_id[i+1]:1}\n",
        "                # print(\"Initialize {} as 1\".format(bigram_count))\n",
        "    # End\n",
        "    return bigram_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "1990cbaa",
      "metadata": {
        "id": "1990cbaa"
      },
      "outputs": [],
      "source": [
        "unigram_count = get_unigram_count(train_token_ids)\n",
        "bigram_count = get_bigram_count(train_token_ids)\n",
        "# bigram_count = get_bigram_count([[1,2,3,4,5,6,7],[5,6,2,3,9],[3,5,9,1,6,5]])\n",
        "# unigram_count = get_unigram_count([[1,2,3,4,5,6,7],[5,6,2,3,9],[3,5,9,1,6,5]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f59bd27",
      "metadata": {
        "id": "4f59bd27"
      },
      "source": [
        "### 1.3 BiGram (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "1b324fad",
      "metadata": {
        "id": "1b324fad"
      },
      "outputs": [],
      "source": [
        "class BiGram:\n",
        "    def __init__(self, unigram_count, bigram_count):\n",
        "        self.unigram_count = unigram_count\n",
        "        self.bigram_count = bigram_count\n",
        "\n",
        "    def calc_prob(self, w1: int, w2: int) -> float:\n",
        "        \"\"\" Calculate the probability of p(w2 | w1) using the BiGram model.\n",
        "\n",
        "        Args:\n",
        "            w1, w2: current token and next token\n",
        "        Note:\n",
        "            if prob you calculated is 0, you should return 1e-5.\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        count_w1 = 0\n",
        "        count_w1_w2 = 0\n",
        "        prob = 0\n",
        "        if w1 in self.bigram_count:\n",
        "            # print(\"Yes {} in {}\".format(w1,bigram_count))\n",
        "            if w2 in self.bigram_count[w1]:\n",
        "                # print(\"Yes {} in {}\".format(w2,bigram_count[w1]))\n",
        "                count_w1_w2 = self.bigram_count[w1][w2]\n",
        "                for w in list(self.bigram_count[w1].values()):\n",
        "                    count_w1 = count_w1 + w\n",
        "                # print(count_w1_w2,count_w1)\n",
        "                prob = count_w1_w2/count_w1\n",
        "            else:\n",
        "                prob = 1e-5\n",
        "        else:\n",
        "            prob = 1e-5\n",
        "        # End\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "253b166a-2b03-4389-8373-a16e77b7a880",
      "metadata": {
        "id": "253b166a-2b03-4389-8373-a16e77b7a880"
      },
      "outputs": [],
      "source": [
        "# inc = 1\n",
        "# for count in bigram_count:\n",
        "#     # if inc < 0:\n",
        "#     #     inc = inc + 1\n",
        "#     #     continue\n",
        "#     if inc > 5:\n",
        "#         break\n",
        "#     inc = inc + 1\n",
        "#     print(\"{}:{}\".format(count,bigram_count[count]))\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "204e88f8",
      "metadata": {
        "id": "204e88f8"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import curve_fit\n",
        "\n",
        "\n",
        "def power_law(x, a, b):\n",
        "    \"\"\" Power law to fit the number of occurrence\n",
        "    \"\"\"\n",
        "    return a * np.power(x, b)\n",
        "\n",
        "\n",
        "class GoodTuring(BiGram):\n",
        "    def __init__(self, unigram_count, bigram_count, threshold=100):\n",
        "        super().__init__(unigram_count, bigram_count)\n",
        "        self.threshold = threshold\n",
        "        self.bigram_Nc = self.calc_Nc()\n",
        "        self.bi_c_star, self.bi_N = self.smoothing(self.bigram_Nc)\n",
        "\n",
        "    def calc_Nc(self) -> Dict[int, Union[float, int]]:\n",
        "        \"\"\" You need to calculate Nc of bigram\n",
        "\n",
        "        Return:\n",
        "            bigram_Nc: A map from count to the occurrence (count of count)\n",
        "                       For example {\n",
        "                           10: 78\n",
        "                       } means there are 78 bigrams occurs 10 times in the dataset.\n",
        "                       Also, 10 is a small c, for large c, it's occurrence will be replaced with the power law.\n",
        "        \"\"\"\n",
        "        bigram_Nc = {}\n",
        "        # Start your code here\n",
        "        # Count the occurrence of count in self.bigram_count.\n",
        "        for bigram in self.bigram_count:\n",
        "            for count in self.bigram_count[bigram]:\n",
        "                if self.bigram_count[bigram][count] in bigram_Nc:\n",
        "                    bigram_Nc[self.bigram_count[bigram][count]] = bigram_Nc[self.bigram_count[bigram][count]]+1\n",
        "                else:\n",
        "                    bigram_Nc[self.bigram_count[bigram][count]] = 1\n",
        "        # End\n",
        "        # print(bigram_Nc)\n",
        "        self.replace_large_c(bigram_Nc)\n",
        "        return bigram_Nc\n",
        "\n",
        "    def replace_large_c(self, Nc):\n",
        "        \"\"\" Fit with power law\n",
        "        \"\"\"\n",
        "        x, y = zip(*sorted(Nc.items(), reverse=True))\n",
        "        popt, pcov = curve_fit(power_law, x, y, bounds=([0, -np.inf], [np.inf, 0]))\n",
        "        a, b = popt\n",
        "\n",
        "        max_count = max(Nc.keys())\n",
        "        for c in range(self.threshold + 1, max_count + 2):\n",
        "            Nc[c] = power_law(c, a, b)\n",
        "\n",
        "    def smoothing(self, Nc: Dict[int, Union[float, int]]) -> Tuple[Dict[int, float], float]:\n",
        "        \"\"\" Calculate the c_star and N\n",
        "\n",
        "        Args:\n",
        "            self.bigram_Nc\n",
        "        Returns:\n",
        "            c_star: The mapping from bigram count to smoothed count\n",
        "            N: The sum of c multiplied by Nc\n",
        "        \"\"\"\n",
        "        c_star = {}\n",
        "        N = 0\n",
        "        max_count = max(Nc.keys())\n",
        "        # print(max_count)\n",
        "        # Start your code here\n",
        "        for count in range(0,max_count):\n",
        "            if count == 0:\n",
        "                c_star[count] = Nc[1]\n",
        "            else:\n",
        "                c_star[count] = (count + 1) *Nc[count+1]/Nc[count]\n",
        "                N += count * Nc[count]\n",
        "        # End\n",
        "        c_star[max_count] = max_count\n",
        "        return c_star, N\n",
        "\n",
        "    def calc_prob(self, w1, w2):\n",
        "        \"\"\" Calculate the probability of p(w2 | w1) using the Good Turing model.\n",
        "\n",
        "        Args:\n",
        "            w1, w2: current token and next token\n",
        "        Note:\n",
        "            1. The numerator is the smoothed bigram count of (w1, w2)\n",
        "            2. The denominator is the unigram count of w1\n",
        "            3. You should be careful to distinguish when (w1, w2) does not exists in the training data.\n",
        "        \"\"\"\n",
        "        prob = 0\n",
        "        # Start your code here\n",
        "        bigram = (w1, w2)\n",
        "        unigram = w1\n",
        "        # print(self.bi_c_star.keys())\n",
        "        if w1 in self.bigram_count and w2 in self.bigram_count[w1]:\n",
        "                numerator = self.bigram_count[w1][w2]\n",
        "                numerator = self.bi_c_star[numerator]\n",
        "                denominator = self.unigram_count[w1]\n",
        "                # print(numerator,denominator)\n",
        "                prob = numerator / denominator\n",
        "        else:\n",
        "                numerator = self.bigram_Nc[1]/self.bi_N\n",
        "                denominator = self.unigram_count[w1]\n",
        "                prob = numerator / denominator\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "0e1a3705-11db-4cfd-a8b3-46b319869cc3",
      "metadata": {
        "id": "0e1a3705-11db-4cfd-a8b3-46b319869cc3"
      },
      "outputs": [],
      "source": [
        "# gt = GoodTuring(unigram_count, bigram_count, threshold=100)\n",
        "# # # Perplexity\n",
        "# gt_perplexity = perplexity(gt, valid_token_ids)\n",
        "# print(f'The perplexity of Good Turing is: {gt_perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "bad71144-32a5-46ea-8b17-3b8f376a5b74",
      "metadata": {
        "id": "bad71144-32a5-46ea-8b17-3b8f376a5b74"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "495234fa",
      "metadata": {
        "id": "495234fa"
      },
      "source": [
        "### 1.5 Kneser-Ney (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "52ba3ff8",
      "metadata": {
        "id": "52ba3ff8"
      },
      "outputs": [],
      "source": [
        "class KneserNey(BiGram):\n",
        "    def __init__(self, unigram_count, bigram_count, d=0.75):\n",
        "        super().__init__(unigram_count, bigram_count)\n",
        "        self.d = d\n",
        "\n",
        "        self.lambda_ = self.calc_lambda()\n",
        "        self.p_continuation = self.calc_p_continuation()\n",
        "\n",
        "    def calc_lambda(self):\n",
        "        \"\"\" Calculate the λ(w)\n",
        "\n",
        "        Return:\n",
        "            lambda_: A dict from token_id (w) to λ(w).\n",
        "        \"\"\"\n",
        "        lambda_ = {}\n",
        "        # Start your code here\n",
        "        for word in self.unigram_count:\n",
        "            if word in self.bigram_count:\n",
        "                word2 = len(self.bigram_count[word])\n",
        "            else:\n",
        "                word2 = 0\n",
        "            lambda_[word] = (self.d*word2)/self.unigram_count[word]\n",
        "        # End\n",
        "        return lambda_\n",
        "\n",
        "    def calc_p_continuation(self):\n",
        "        \"\"\" Calculate the p_continuation(w)\n",
        "\n",
        "        Return:\n",
        "            lambda_: A dict from token_id (w) to λ(w).\n",
        "        \"\"\"\n",
        "        numerator = {}  # token -> type of previous token\n",
        "        denominator = len(self.bigram_count)  # type of all previous tokens\n",
        "        # Start your code here\n",
        "        for w in self.unigram_count:\n",
        "                c = 0\n",
        "                for w1List in list(self.bigram_count.values()):\n",
        "                    if w in w1List:\n",
        "                        c = c+1\n",
        "                numerator[w] = c\n",
        "        # End\n",
        "        p_continuation = { 0: 0, 2: 0, 3: 0, 4: 0 }\n",
        "        for w, count in numerator.items():\n",
        "            p_continuation[w] = count / denominator\n",
        "        return p_continuation\n",
        "\n",
        "    def calc_prob(self, w1, w2):\n",
        "        \"\"\" Calculate the probability of p(w2 | w1) using the Kneser-Ney model.\n",
        "\n",
        "        Args:\n",
        "            w1, w2: current token and next token\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        c_w1_w2 = self.bigram_count[w1][w2] if w1 in self.bigram_count and w2 in self.bigram_count[w1] else 0\n",
        "        prob = max(c_w1_w2 - self.d, 0) / self.unigram_count[w1] + self.lambda_[w1] * self.p_continuation[w2]\n",
        "        # End\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9738efde",
      "metadata": {
        "id": "9738efde"
      },
      "source": [
        "### Show that perplexity is the exponential of the total loss divided by the number of predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8474370",
      "metadata": {
        "id": "f8474370"
      },
      "source": [
        "### 1.6 Perplexity (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "821225d6",
      "metadata": {
        "id": "821225d6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def perplexity(model, token_ids):\n",
        "    \"\"\" Calculate the perplexity score.\n",
        "\n",
        "    Args:\n",
        "        model: the model you want to evaluate (BiGram, GoodTuring, or KneserNey)\n",
        "        token_ids: a list of validation token_ids\n",
        "    Return:\n",
        "        perplexity: the perplexity of the model on texts\n",
        "    Note:\n",
        "\n",
        "    \"\"\"\n",
        "    log_probs = 0\n",
        "    n = len(token_ids)\n",
        "    n_words = 0\n",
        "    for i, tokens in enumerate(token_ids):\n",
        "        if i % 100 == 0 or i == n - 1:\n",
        "            print_line('Calculating perplexity:', (i + 1), '/', n)\n",
        "        # Start your code here\n",
        "        # Calculate log probability for each bigram in the sequence\n",
        "        for j in range(len(tokens) - 1):\n",
        "            w1 = tokens[j]\n",
        "            w2 = tokens[j + 1]\n",
        "            log_probs += math.log(model.calc_prob(w1, w2) + 1e-200)  # Add a small value to avoid log(0)\n",
        "            n_words += 1\n",
        "\n",
        "        # End\n",
        "\n",
        "    perp = 0\n",
        "    # Start your code here\n",
        "    # Calculate the final perplexity\n",
        "    perp = math.exp(-log_probs / n_words)\n",
        "    # End\n",
        "    print('\\n')\n",
        "\n",
        "    return perp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08663592",
      "metadata": {
        "id": "08663592"
      },
      "source": [
        "If you implement correctly, the perplexity of bigram will be around 320"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "316eea51",
      "metadata": {
        "id": "316eea51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af43ce4-b26f-4719-d49d-fbdea97f6df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Bigram is: 325.8354\n"
          ]
        }
      ],
      "source": [
        "bigram = BiGram(unigram_count, bigram_count)\n",
        "\n",
        "# Perplexity\n",
        "bigram_perplexity = perplexity(bigram, valid_token_ids)\n",
        "print(f'The perplexity of Bigram is: {bigram_perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5217ea8d",
      "metadata": {
        "id": "5217ea8d"
      },
      "source": [
        "If you implement correctly, the perplexity of good turing will be around 130"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "beead231",
      "metadata": {
        "id": "beead231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e557804-b778-41f1-b1ef-3f48c8215012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Good Turing is: 130.5334\n"
          ]
        }
      ],
      "source": [
        "gt = GoodTuring(unigram_count, bigram_count, threshold=100)\n",
        "\n",
        "# Perplexity\n",
        "gt_perplexity = perplexity(gt, valid_token_ids)\n",
        "print(f'The perplexity of Good Turing is: {gt_perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be59fb9",
      "metadata": {
        "id": "1be59fb9"
      },
      "source": [
        "If you implement correctly, the perplexity of good turing will be around 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "62867d18",
      "metadata": {
        "id": "62867d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a525a445-c1e0-4a15-ea12-5f39de31645e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rCalculating perplexity: 1 / 3352\rCalculating perplexity: 101 / 3352\rCalculating perplexity: 201 / 3352\rCalculating perplexity: 301 / 3352\rCalculating perplexity: 401 / 3352\rCalculating perplexity: 501 / 3352\rCalculating perplexity: 601 / 3352\rCalculating perplexity: 701 / 3352\rCalculating perplexity: 801 / 3352\rCalculating perplexity: 901 / 3352\rCalculating perplexity: 1001 / 3352\rCalculating perplexity: 1101 / 3352\rCalculating perplexity: 1201 / 3352\rCalculating perplexity: 1301 / 3352\rCalculating perplexity: 1401 / 3352\rCalculating perplexity: 1501 / 3352\rCalculating perplexity: 1601 / 3352\rCalculating perplexity: 1701 / 3352\rCalculating perplexity: 1801 / 3352\rCalculating perplexity: 1901 / 3352\rCalculating perplexity: 2001 / 3352\rCalculating perplexity: 2101 / 3352\rCalculating perplexity: 2201 / 3352\rCalculating perplexity: 2301 / 3352\rCalculating perplexity: 2401 / 3352\rCalculating perplexity: 2501 / 3352\rCalculating perplexity: 2601 / 3352\rCalculating perplexity: 2701 / 3352\rCalculating perplexity: 2801 / 3352\rCalculating perplexity: 2901 / 3352\rCalculating perplexity: 3001 / 3352\rCalculating perplexity: 3101 / 3352\rCalculating perplexity: 3201 / 3352\rCalculating perplexity: 3301 / 3352\rCalculating perplexity: 3352 / 3352\n",
            "\n",
            "The perplexity of Kneser-Ney is: 62.5908\n"
          ]
        }
      ],
      "source": [
        "kn = KneserNey(unigram_count, bigram_count, d=0.75)\n",
        "\n",
        "# Perplexity\n",
        "kn_perplexity = perplexity(kn, valid_token_ids)\n",
        "print(f'The perplexity of Kneser-Ney is: {kn_perplexity:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95a9fffb",
      "metadata": {
        "id": "95a9fffb"
      },
      "source": [
        "### 1.7 Predict the next word given a previous word (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "9836925c",
      "metadata": {
        "id": "9836925c"
      },
      "outputs": [],
      "source": [
        "def predict(model: 'BiGram', w1: int, vocab_size: int):\n",
        "    \"\"\" Predict the w2 with the hightest probability given w1\n",
        "\n",
        "    Args:\n",
        "        model: A BiGram, GoodTuring, or KneserNey model that has the calc_prob function\n",
        "        w1: current word\n",
        "        vocab_size: the number of tokens in the vocabulary\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    highest_prob = 0\n",
        "    for w2 in range(1, vocab_size):\n",
        "        # Start your code here\n",
        "        prob = model.calc_prob(w1, w2)\n",
        "        if prob > highest_prob:\n",
        "            result = w2\n",
        "            highest_prob = prob\n",
        "        # End\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N-bvcYctMZ9R",
      "metadata": {
        "id": "N-bvcYctMZ9R"
      },
      "source": [
        "Bigram next word prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "j6HG1ObqMXMG",
      "metadata": {
        "id": "j6HG1ObqMXMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb88f53-1ac0-429b-ad83-1171cad7e561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: longer\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: traders\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: services\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict(bigram, token_ids[-1], vocab_size)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9O5eeez1MshG",
      "metadata": {
        "id": "9O5eeez1MshG"
      },
      "source": [
        "Good Turing next word prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "86a1a51f",
      "metadata": {
        "id": "86a1a51f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d618ac12-787b-48e4-f93d-1e3103bbd45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: longer\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: traders\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: officer\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict(gt, token_ids[-1], vocab_size)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uAqMZJAEMwQV",
      "metadata": {
        "id": "uAqMZJAEMwQV"
      },
      "source": [
        "Kneser-Ney next word prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "A6A8Tfb8MPQY",
      "metadata": {
        "id": "A6A8Tfb8MPQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b19a839-86ec-435d-919b-7d851d9d4f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: n\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    pred = predict(kn, token_ids[-1], vocab_size)\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d4c8a3",
      "metadata": {
        "id": "c8d4c8a3"
      },
      "source": [
        "## Part B: 2. RNN (35 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd13ba42",
      "metadata": {
        "id": "fd13ba42"
      },
      "source": [
        "### 2.1 Split feature and label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "6ff349f6-f949-4750-8cc3-581d3755acb2",
      "metadata": {
        "id": "6ff349f6-f949-4750-8cc3-581d3755acb2",
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "1c74d702",
      "metadata": {
        "id": "1c74d702"
      },
      "outputs": [],
      "source": [
        "def get_feature_label(token_ids: List[List[int]], window_size: int=-1):\n",
        "    \"\"\" Split features and labels for the training, validation, and test datasets.\n",
        "\n",
        "    Note:\n",
        "        If window size is -1, for a sentence with n tokens,\n",
        "            it selects the tokens rangeing from [0, n - 1) as the feature,\n",
        "            and selects tokens ranging from [1, n) as the label.\n",
        "        Otherwise, it divides a sentence with multiple windows and do the previous split.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    y = []\n",
        "    seq_lens = []\n",
        "    for sent_token_ids in token_ids:\n",
        "        if window_size == -1:\n",
        "            x.append(sent_token_ids[:-1])\n",
        "            y.append(sent_token_ids[1:])\n",
        "            seq_lens.append(len(sent_token_ids) - 1)\n",
        "        else:\n",
        "            if len(sent_token_ids) > window_size:\n",
        "                sub_sent_size = window_size + 1\n",
        "                n_window = len(sent_token_ids) // (sub_sent_size)\n",
        "                for i in range(n_window):\n",
        "                    start = i * sub_sent_size\n",
        "                    sub_sent = sent_token_ids[start:(start + sub_sent_size)]\n",
        "                    x.append(sub_sent[:-1])\n",
        "                    y.append(sub_sent[1:])\n",
        "                    seq_lens.append(len(sub_sent) - 1)\n",
        "                if len(sent_token_ids) % sub_sent_size > 0:\n",
        "                    sub_sent = sent_token_ids[-sub_sent_size:]\n",
        "                    x.append(sub_sent[:-1])\n",
        "                    y.append(sub_sent[1:])\n",
        "                    seq_lens.append(len(sub_sent) - 1)\n",
        "            else:\n",
        "                x.append(sent_token_ids[:-1])\n",
        "                y.append(sent_token_ids[1:])\n",
        "                seq_lens.append(len(sent_token_ids) - 1)\n",
        "    return x, y, seq_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "f8977eac",
      "metadata": {
        "id": "f8977eac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa50cbcb-76b2-4012-caf1-50a6545d6c97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40 68 68\n"
          ]
        }
      ],
      "source": [
        "window_size = 40\n",
        "x_train, y_train, train_seq_lens = get_feature_label(train_token_ids, window_size)\n",
        "x_valid, y_valid, valid_seq_lens = get_feature_label(valid_token_ids)\n",
        "x_test, y_test, test_seq_lens = get_feature_label(valid_token_ids)\n",
        "print(max(train_seq_lens), max(valid_seq_lens), max(test_seq_lens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "fce50c0e-4ace-4653-94a8-70cba772fbd3",
      "metadata": {
        "id": "fce50c0e-4ace-4653-94a8-70cba772fbd3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "203786d8",
      "metadata": {
        "id": "203786d8"
      },
      "source": [
        "### 2.2 Pad sentences in a batch to equal length (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "3bf82fb2",
      "metadata": {
        "id": "3bf82fb2"
      },
      "outputs": [],
      "source": [
        "def pad_batch(x_batch: List[List[int]], y_batch: List[List[int]], seq_lens_batch: List[int], pad_val: int):\n",
        "    \"\"\" Pad the sentences in a batch with pad_val based on the longest sentence.\n",
        "\n",
        "    Args:\n",
        "        x_batch, y_batch, seq_lens_batch: the input data\n",
        "        pad_val: the padding value you need to fill to pad the sentences to the longest sentence.\n",
        "\n",
        "    Return:\n",
        "        x_batch: Tensor, (batch_size x max_seq_len)\n",
        "        y_batch: Tensor, (batch_size x max_seq_len)\n",
        "        seq_lens_batch: Tensor, (batch_size, )\n",
        "    \"\"\"\n",
        "    max_len = max(seq_lens_batch)\n",
        "    # Start your code here\n",
        "    num_sent = len(seq_lens_batch)\n",
        "\n",
        "    # Padding the sentence to the length of the longest sentence\n",
        "    x_batch = [x_batch[s] + [pad_val]*(max_len - len(x_batch[s])) for s in range(num_sent)]\n",
        "    y_batch = [y_batch[s] + [pad_val]*(max_len - len(y_batch[s])) for s in range(num_sent)]\n",
        "    # End\n",
        "    x_batch, y_batch = tf.convert_to_tensor(x_batch, dtype=tf.int64), tf.convert_to_tensor(y_batch, dtype=tf.int64)\n",
        "    seq_lens_batch = tf.convert_to_tensor(seq_lens_batch, dtype=tf.int64)\n",
        "    return x_batch, y_batch, seq_lens_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "VDuM_7mgROlR",
      "metadata": {
        "id": "VDuM_7mgROlR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\n",
        "\n",
        "class RNN(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_units):\n",
        "        \"\"\" Init of the RNN model\n",
        "\n",
        "        Args:\n",
        "            vocab_size, embedding_dim: used for initialze the embedding layer.\n",
        "            hidden_units: number of hidden units of the RNN layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Start your code here\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.SimpleRNN(hidden_units, return_sequences=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "        # End\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\" Forward of the RNN model\n",
        "\n",
        "        Args:\n",
        "            x: Tensor, (batch_size x max_seq_len). Input tokens. Here, max_seq_len is the longest length of sentences in this batch becasue we did pad_batch.\n",
        "        Return:\n",
        "            outputs: Tensor, (batch_size x max_seq_len x vocab_size). Logits for every time step. !!!NO SOFTMAX HERE!!!\n",
        "        \"\"\"\n",
        "        # Start your code here\n",
        "        embedded = self.embedding(x)  # (batch_size, max_seq_len, embedding_dim)\n",
        "        rnn_output = self.rnn(embedded)  # (batch_size, max_seq_len, hidden_units)\n",
        "        outputs = self.dense(rnn_output)\n",
        "        # End\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7596f0",
      "metadata": {
        "id": "3d7596f0"
      },
      "source": [
        "### 2.3 RNN language model (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "c99cc2b7",
      "metadata": {
        "id": "c99cc2b7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5c9e16d9",
      "metadata": {
        "id": "5c9e16d9"
      },
      "source": [
        "### 2.4 Seq2seq loss (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "ab46f247",
      "metadata": {
        "id": "ab46f247"
      },
      "outputs": [],
      "source": [
        "from tensorflow_addons.seq2seq import sequence_loss\n",
        "\n",
        "\n",
        "def seq2seq_loss(logits, target, seq_lens):\n",
        "    \"\"\" Calculate the sequence to sequence loss using the sequence_loss from tensorflow\n",
        "\n",
        "    Args:\n",
        "        logits: Tensor (batch_size x max_seq_len x vocab_size). The output of the RNN model.\n",
        "        target: Tensor (batch_size x max_seq_len). The groud-truth of words.\n",
        "        seq_lens: Tensor (batch_size, ). The real sequence length before padding.\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "    # Start your code here\n",
        "    # 1. make a sequence mask (batch_size x max_seq_len) using tf.sequence_mask. This is to build a mask with 1 and 0.\n",
        "    mask = tf.sequence_mask(seq_lens, maxlen=tf.shape(target)[1], dtype=tf.float32)\n",
        "    #    Entry with 1 is the valid time step without padding. Entry with 0 is the time step with padding. We need to exclude this time step.\n",
        "    # 2. calculate the loss with sequence_loss. Carefully read the documentation of each parameter\n",
        "    loss = sequence_loss(logits, target,mask,average_across_timesteps = True,average_across_batch = True)\n",
        "    # End\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "28a41c75",
      "metadata": {
        "id": "28a41c75"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer)\n",
        "hidden_units = 128\n",
        "embedding_dim = 64\n",
        "num_epoch = 30\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "883cc6d4",
      "metadata": {
        "id": "883cc6d4"
      },
      "outputs": [],
      "source": [
        "model = RNN(vocab_size, embedding_dim, hidden_units)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e296008d",
      "metadata": {
        "id": "e296008d"
      },
      "source": [
        "### 2.5 Train RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e5efc9",
      "metadata": {
        "id": "04e5efc9"
      },
      "source": [
        "If you implement everything correctly, the finall loss will be around 5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "fcc22c6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcc22c6d",
        "outputId": "30de15c4-2722-449b-b7d9-fa6c52b59e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 30 - Step 170 / 170 - train loss: 6.9871 - valid loss: 6.6481\n",
            "Epoch 2 / 30 - Step 170 / 170 - train loss: 6.6823 - valid loss: 6.6370\n",
            "Epoch 3 / 30 - Step 170 / 170 - train loss: 6.6368 - valid loss: 6.4843\n",
            "Epoch 4 / 30 - Step 170 / 170 - train loss: 6.3856 - valid loss: 6.2237\n",
            "Epoch 5 / 30 - Step 170 / 170 - train loss: 6.1472 - valid loss: 6.0347\n",
            "Epoch 6 / 30 - Step 170 / 170 - train loss: 5.9497 - valid loss: 5.8651\n",
            "Epoch 7 / 30 - Step 170 / 170 - train loss: 5.7855 - valid loss: 5.7475\n",
            "Epoch 8 / 30 - Step 170 / 170 - train loss: 5.6618 - valid loss: 5.6647\n",
            "Epoch 9 / 30 - Step 170 / 170 - train loss: 5.5633 - valid loss: 5.6003\n",
            "Epoch 10 / 30 - Step 170 / 170 - train loss: 5.4787 - valid loss: 5.5414\n",
            "Epoch 11 / 30 - Step 170 / 170 - train loss: 5.4026 - valid loss: 5.4929\n",
            "Epoch 12 / 30 - Step 170 / 170 - train loss: 5.3344 - valid loss: 5.4511\n",
            "Epoch 13 / 30 - Step 170 / 170 - train loss: 5.2728 - valid loss: 5.4181\n",
            "Epoch 14 / 30 - Step 170 / 170 - train loss: 5.2166 - valid loss: 5.3863\n",
            "Epoch 15 / 30 - Step 170 / 170 - train loss: 5.1650 - valid loss: 5.3633\n",
            "Epoch 16 / 30 - Step 170 / 170 - train loss: 5.1180 - valid loss: 5.3489\n",
            "Epoch 17 / 30 - Step 170 / 170 - train loss: 5.0749 - valid loss: 5.3340\n",
            "Epoch 18 / 30 - Step 170 / 170 - train loss: 5.0362 - valid loss: 5.3161\n",
            "Epoch 19 / 30 - Step 170 / 170 - train loss: 4.9979 - valid loss: 5.2981\n",
            "Epoch 20 / 30 - Step 170 / 170 - train loss: 4.9614 - valid loss: 5.2831\n",
            "Epoch 21 / 30 - Step 170 / 170 - train loss: 4.9266 - valid loss: 5.2718\n",
            "Epoch 22 / 30 - Step 170 / 170 - train loss: 4.8944 - valid loss: 5.2631\n",
            "Epoch 23 / 30 - Step 170 / 170 - train loss: 4.8636 - valid loss: 5.2566\n",
            "Epoch 24 / 30 - Step 170 / 170 - train loss: 4.8352 - valid loss: 5.2515\n",
            "Epoch 25 / 30 - Step 170 / 170 - train loss: 4.8084 - valid loss: 5.2487\n",
            "Epoch 26 / 30 - Step 170 / 170 - train loss: 4.7825 - valid loss: 5.2427\n",
            "Epoch 27 / 30 - Step 170 / 170 - train loss: 4.7579 - valid loss: 5.2377\n",
            "Epoch 28 / 30 - Step 170 / 170 - train loss: 4.7344 - valid loss: 5.2365\n",
            "Epoch 29 / 30 - Step 170 / 170 - train loss: 4.7144 - valid loss: 5.2415\n",
            "Epoch 30 / 30 - Step 170 / 170 - train loss: 4.6963 - valid loss: 5.2420\n"
          ]
        }
      ],
      "source": [
        "num_samples = len(x_train)\n",
        "n_batch = int(np.ceil(num_samples / batch_size))\n",
        "n_valid_batch = int(np.ceil(len(x_valid) / batch_size))\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_idx in range(n_batch):\n",
        "        start = batch_idx * batch_size\n",
        "        end = start + batch_size\n",
        "        x_batch, y_batch, seq_lens_batch = x_train[start:end], y_train[start:end], train_seq_lens[start:end]\n",
        "        real_batch_size = len(x_batch)\n",
        "        x_batch, y_batch, seq_lens_batch = pad_batch(x_batch, y_batch, seq_lens_batch, pad_val=tokenizer.pad_token_id)\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(x_batch)\n",
        "            loss = seq2seq_loss(output, y_batch, seq_lens_batch)\n",
        "\n",
        "        if batch_idx % 1 == 0 or batch_idx == num_samples - 1:\n",
        "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_batch} - loss: {loss:.4f}')\n",
        "\n",
        "        trainable_vars = model.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        epoch_loss += loss * real_batch_size\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    for batch_idx in range(n_valid_batch):\n",
        "        start = batch_idx * batch_size\n",
        "        end = start + batch_size\n",
        "        x_batch, y_batch, seq_lens_batch = x_valid[start:end], y_valid[start:end], valid_seq_lens[start:end]\n",
        "        real_batch_size = len(x_batch)\n",
        "        x_batch, y_batch, seq_lens_batch = pad_batch(x_batch, y_batch, seq_lens_batch, pad_val=tokenizer.pad_token_id)\n",
        "        output = model(x_batch)\n",
        "        loss = seq2seq_loss(output, y_batch, seq_lens_batch)\n",
        "\n",
        "        if batch_idx % 1 == 0 or batch_idx == len(x_valid) - 1:\n",
        "            print_line(f'Epoch {epoch + 1} / {num_epoch} - Step {batch_idx + 1} / {n_valid_batch} - loss: {loss:.4f}')\n",
        "\n",
        "        valid_loss += loss * real_batch_size\n",
        "    print(f'\\rEpoch {epoch + 1} / {num_epoch} - Step {n_batch} / {n_batch} - train loss: {epoch_loss / num_samples:.4f} - valid loss: {valid_loss / len(x_valid):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e6815cd",
      "metadata": {
        "id": "2e6815cd"
      },
      "source": [
        "### 2.6 Perplexity of RNN (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a257ec",
      "metadata": {
        "id": "41a257ec"
      },
      "source": [
        "Here,\n",
        "1. you need to calculate the perplexity based on its definition.\n",
        "2. Besides, you need to record the loss for every word prediction and calculate the sum of loss\n",
        "3. Finaly, you will need to compare the perplexity by definition and the perplexity by the loss: `np.exp(total_loss / n_words)`"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bY4_bH-F1rHD"
      },
      "id": "bY4_bH-F1rHD",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "a4dfa582",
      "metadata": {
        "id": "a4dfa582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a37b96-b949-4f7b-ce4d-e27185baf839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating perplexity: 3352 / 3352\n",
            "\n",
            "Perplexity by definition: 188.8956, Perplexity by loss: 188.8954\n"
          ]
        }
      ],
      "source": [
        "n = len(x_valid)\n",
        "log_probs = 0\n",
        "n_words = 0  # number of words to predict in the entire dataset\n",
        "total_loss = 0  # total loss of each word's loss\n",
        "for i in range(n):\n",
        "# for i in range(1):\n",
        "\n",
        "    if i % 1 == 0 or i == n - 1:\n",
        "        print_line('Calculating perplexity:', (i + 1), '/', n)\n",
        "    x_line, y_line, line_seq_lens = x_valid[i:i + 1], y_valid[i: i + 1], valid_seq_lens[i:i + 1]\n",
        "    x_line, y_line, line_seq_lens = pad_batch(x_line, y_line, line_seq_lens, tokenizer.pad_token_id)\n",
        "    output = model(x_line)\n",
        "    pred_probs = tf.nn.softmax(output, axis=-1)\n",
        "\n",
        "    for real_token, probs in zip(y_line[0], pred_probs[0]):\n",
        "        # Start your code here\n",
        "        log_probs += np.log2(probs[real_token])\n",
        "        n_words = n_words+1\n",
        "        # End\n",
        "    loss = 0\n",
        "    # Start your code here\n",
        "    loss = seq2seq_loss(output,y_line,line_seq_lens)\n",
        "    total_loss = total_loss + loss * len(x_line[0])\n",
        "    # End\n",
        "print('\\n')\n",
        "# print(n_words)\n",
        "# print(log_probs)\n",
        "perplexity = 2 ** ((-1 / n_words) * log_probs)\n",
        "print(f'Perplexity by definition: {perplexity:.4f}, Perplexity by loss: {np.exp(total_loss / n_words):.4f}')\n",
        "\n",
        "# If you implement correctly, the two perplexity will be almost the same."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee2a3a7",
      "metadata": {
        "id": "aee2a3a7"
      },
      "source": [
        "### 2.7 Predict the next word given a previous sentence (5 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "c56df665",
      "metadata": {
        "id": "c56df665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9384737-403b-41f7-9bee-82a9f350da4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sharply falling stock prices do reduce consumer wealth damage business ____\n",
            "predicted last token: and\n",
            "---------------------------------------------\n",
            "but robert an official of the association said no ____\n",
            "predicted last token: longer\n",
            "---------------------------------------------\n",
            "it also has interests in military electronics and marine ____\n",
            "predicted last token: </s>\n",
            "---------------------------------------------\n",
            "first chicago since n has reduced its loans to such ____\n",
            "predicted last token: as\n",
            "---------------------------------------------\n",
            "david m jones vice president at g ____\n",
            "predicted last token: s\n",
            "---------------------------------------------\n",
            "the n stock specialist firms on the big board floor ____\n",
            "predicted last token: traders\n",
            "---------------------------------------------\n",
            "at the same time the business was hurt by ____\n",
            "predicted last token: the\n",
            "---------------------------------------------\n",
            "salomon will cover the warrants by buying sufficient shares or ____\n",
            "predicted last token: by\n",
            "---------------------------------------------\n",
            "in july southmark corp the dallas based real estate and financial ____\n",
            "predicted last token: services\n",
            "---------------------------------------------\n",
            "he concluded his remarks by and at some ____\n",
            "predicted last token: of\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(12345)\n",
        "\n",
        "vocab_size = len(tokenizer)\n",
        "indexes = np.random.choice(len(test_token_ids), 10, replace=False)\n",
        "for i in indexes:\n",
        "    token_ids = test_token_ids[i][1:-1]\n",
        "    print(' '.join([tokenizer.inverse_vocab[token_id] for token_id in token_ids]) + ' ____')\n",
        "    x = tf.convert_to_tensor(token_ids, dtype=tf.int64)  # now x is a tensor of (seq_len, )\n",
        "    # Start your code here\n",
        "    x = np.reshape(x,(1,-1))\n",
        "    prob = model.call(x)\n",
        "    output = prob[0]\n",
        "    n_t = output[-1]\n",
        "    pred = np.argmax(n_t)\n",
        "    # End\n",
        "    print(f'predicted last token: {tokenizer.inverse_vocab[pred]}')\n",
        "    print('---------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b1ceb9",
      "metadata": {
        "id": "48b1ceb9"
      },
      "source": [
        "## 3. Conclusion (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb2c1ba1",
      "metadata": {
        "id": "eb2c1ba1"
      },
      "source": [
        "Briefly analyze the result of N-Gram and RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram:\n",
        "\n",
        "1. **Counting Word Frequencies**: Initially, the text is divided into single words (unigrams) and pairs of consecutive words (bigrams). These counts are pivotal for the N-gram model to predict subsequent words based on the frequency of these word pairs.\n",
        "\n",
        "2. **Utilizing Bigram Model**: The model employs bigrams to anticipate the next word by considering the frequency of occurrence of word pairs within the training data.\n",
        "\n",
        "3. **Application of Smoothing Techniques**: Techniques such as Good Turing and Kneser-Ney smoothing are applied to refine the probability estimates of N-gram models, enhancing the accuracy of predictions.\n",
        "\n",
        "4. **Perplexity Evaluation**: Perplexity, which gauges how well a probability distribution anticipates a given sample, is computed for each model. Among these, the Kneser-Ney smoothing model exhibits the lowest perplexity, trailed by Good Turing, and finally, the Bigram model.\n",
        "\n",
        "### RNN:\n",
        "\n",
        "1. **Model Structure**: The architecture comprises an embedding layer to transform words into compact vectors, a hidden SimpleRNN layer for capturing sequential dependencies, and an output layer for making predictions.\n",
        "\n",
        "2. **Addressing Variable Length Sequences**: To facilitate batch processing in the RNN model, padding is applied to ensure that sentences are of uniform length.\n",
        "\n",
        "3. **Loss Metric Calculation**: The sequence-to-sequence loss is determined by averaging the loss across both axes for every sample in the batch.\n",
        "\n",
        "4. **Performance Assessment**: Post-training, the validation loss and training loss are assessed. Additionally, perplexity, calculated using both conventional methods and total loss, is provided to evaluate the RNN model's performance.\n",
        "\n",
        "### Comparison:\n",
        "\n",
        "1. **Complexity and Training Time**: Unlike the RNN model, the N-gram model is less intricate and quicker to train due to its non-iterative nature.\n",
        "\n",
        "2. **Similarity in Predictions**: Despite their structural disparities, both models yield comparable predictions for subsequent words, suggesting they capture analogous patterns in the data.\n",
        "\n",
        "3. **Perplexity Analysis**: While the RNN model boasts a lower perplexity compared to the Bigram model, it falls short of the levels achieved by Good Turing and Kneser-Ney smoothing models. This implies that while RNNs offer potential for capturing nuanced patterns, N-gram models with smoothing techniques provide more accurate predictions.\n",
        "\n",
        "4. **Balancing Complexity and Nuance**: Although the RNN model offers complexity and potential for capturing intricate patterns, it requires greater computational resources and time for training. Hence, the choice between the models hinges on the specific task requirements and the desired level of prediction accuracy."
      ],
      "metadata": {
        "id": "t2cTYoOOcTRm"
      },
      "id": "t2cTYoOOcTRm"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3JxdN-icUSP"
      },
      "id": "f3JxdN-icUSP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}